{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"About","text":"<p>Welcome to the technical documentation website of CIB Mango Tree, a collaborative and open-source project to develop software that tests for coordinated inauthentic behavior (CIB) in datasets of social media activity.</p> <p>This is the technical documentation, for a user-based perspective see the project main home page: cibmangotree.org</p>"},{"location":"license/","title":"License","text":"<pre><code>MIT License\n\nCopyright (c) 2025 CIB Mango Tree\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre>"},{"location":"guides/contributing/analyzers/","title":"Implementing Analyzers","text":""},{"location":"guides/contributing/analyzers/#analyzers-guide","title":"Analyzers Guide","text":"<p>Analyzers are the core data processing components of the platform. They follow a three-tier architecture that separates data processing, analysis, and presentation concerns.</p>"},{"location":"guides/contributing/analyzers/#architecture-overview","title":"Architecture Overview","text":"<p>The analyzer system consists of three types of components:</p> <ol> <li>Primary Analyzer: Performs the core data analysis and outputs structured results</li> <li>Secondary Analyzer: Processes primary analyzer outputs for specific use cases or exports</li> <li>Web Presenter: Creates interactive dashboards and visualizations</li> </ol> <p>This separation allows for:</p> <ul> <li>Reusable analysis logic</li> <li>Multiple presentation formats for the same analysis</li> <li>Collaborative development where different contributors can focus on different layers</li> </ul>"},{"location":"guides/contributing/analyzers/#primary-analyzers","title":"Primary Analyzers","text":"<p>Primary analyzers perform the core data analysis. They read user input data, process it according to their algorithm, and output structured results in parquet format.</p>"},{"location":"guides/contributing/analyzers/#interface-definition","title":"Interface Definition","text":"<p>Every primary analyzer must define an interface that specifies:</p> <ul> <li>Input columns required from the user</li> <li>Parameters the analyzer accepts</li> <li>Output tables the analyzer produces</li> </ul> <pre><code>from analyzer_interface import (\n    AnalyzerInput,\n    AnalyzerInterface, \n    AnalyzerOutput,\n    AnalyzerParam,\n    InputColumn,\n    OutputColumn,\n    IntegerParam\n)\n\ninterface = AnalyzerInterface(\n    id=\"example_analyzer\",  # Must be globally unique\n    version=\"0.1.0\",\n    name=\"Example Analyzer\",\n    short_description=\"Counts characters in messages\",\n    long_description=\"\"\"\nThis analyzer demonstrates the basic structure by counting \ncharacters in each message and marking long messages.\n    \"\"\",\n    input=AnalyzerInput(\n        columns=[\n            InputColumn(\n                name=\"message_id\",\n                human_readable_name=\"Unique Message ID\", \n                data_type=\"identifier\",\n                description=\"The unique identifier of the message\",\n                name_hints=[\"post\", \"message\", \"tweet\", \"id\"]\n            ),\n            InputColumn(\n                name=\"message_text\",\n                human_readable_name=\"Message Text\",\n                data_type=\"text\", \n                description=\"The text content of the message\",\n                name_hints=[\"message\", \"text\", \"content\", \"body\"]\n            )\n        ]\n    ),\n    params=[\n        AnalyzerParam(\n            id=\"fudge_factor\",\n            human_readable_name=\"Character Count Adjustment\",\n            description=\"Adds to the character count for testing purposes\",\n            type=IntegerParam(min=-1000, max=1000),\n            default=0\n        )\n    ],\n    outputs=[\n        AnalyzerOutput(\n            id=\"character_count\",\n            name=\"Character Count Per Message\", \n            internal=True,  # Not shown in export list\n            columns=[\n                OutputColumn(name=\"message_id\", data_type=\"integer\"),\n                OutputColumn(name=\"character_count\", data_type=\"integer\")\n            ]\n        )\n    ]\n)\n</code></pre>"},{"location":"guides/contributing/analyzers/#implementation","title":"Implementation","text":"<p>The main function receives a context object with access to input data and output paths:</p> <pre><code>import polars as pl\nfrom analyzer_interface.context import PrimaryAnalyzerContext\nfrom terminal_tools import ProgressReporter\n\ndef main(context: PrimaryAnalyzerContext):\n    # Read and preprocess input data\n    input_reader = context.input()\n    df_input = input_reader.preprocess(pl.read_parquet(input_reader.parquet_path))\n\n    # Access parameters\n    fudge_factor = context.params.get(\"fudge_factor\")\n    assert isinstance(fudge_factor, int), \"Fudge factor must be an integer\"\n\n    # Perform analysis with progress reporting\n    with ProgressReporter(\"Counting characters\") as progress:\n        df_count = df_input.select(\n            pl.col(\"message_id\"),\n            pl.col(\"message_text\")\n            .str.len_chars()\n            .add(fudge_factor)\n            .alias(\"character_count\")\n        )\n        progress.update(1.0)\n\n    # Write output to specified path\n    df_count.write_parquet(context.output(\"character_count\").parquet_path)\n</code></pre>"},{"location":"guides/contributing/analyzers/#declaration","title":"Declaration","text":"<p>Finally, create the analyzer declaration:</p> <pre><code>from analyzer_interface import AnalyzerDeclaration\nfrom .interface import interface\nfrom .main import main\n\nexample_analyzer = AnalyzerDeclaration(\n    interface=interface,\n    main=main,\n    is_distributed=False  # Set to True for production analyzers\n)\n</code></pre>"},{"location":"guides/contributing/analyzers/#secondary-analyzers","title":"Secondary Analyzers","text":"<p>Secondary analyzers process the output of primary analyzers to create user-friendly exports or perform additional analysis.</p>"},{"location":"guides/contributing/analyzers/#interface-definition_1","title":"Interface Definition","text":"<p>Secondary analyzers specify their base primary analyzer and their own outputs:</p> <pre><code>from analyzer_interface import AnalyzerOutput, OutputColumn, SecondaryAnalyzerInterface\nfrom ..example_base.interface import interface as example_base\n\ninterface = SecondaryAnalyzerInterface(\n    id=\"example_report\",\n    version=\"0.1.0\", \n    name=\"Example Report\",\n    short_description=\"Adds 'is_long' flag to character count analysis\",\n    base_analyzer=example_base,  # Reference to primary analyzer\n    outputs=[\n        AnalyzerOutput(\n            id=\"example_report\",\n            name=\"Example Report\", \n            columns=[\n                OutputColumn(name=\"message_id\", data_type=\"integer\"),\n                OutputColumn(name=\"character_count\", data_type=\"integer\"),\n                OutputColumn(name=\"is_long\", data_type=\"boolean\")  # New column\n            ]\n        )\n    ]\n)\n</code></pre>"},{"location":"guides/contributing/analyzers/#implementation_1","title":"Implementation","text":"<p>Secondary analyzers read primary outputs and create enhanced results:</p> <pre><code>import polars as pl\nfrom analyzer_interface.context import SecondaryAnalyzerContext\n\ndef main(context: SecondaryAnalyzerContext):\n    # Read primary analyzer output\n    df_character_count = pl.read_parquet(\n        context.base.table(\"character_count\").parquet_path\n    )\n\n    # Add derived columns\n    df_export = df_character_count.with_columns(\n        pl.col(\"character_count\").gt(100).alias(\"is_long\")\n    )\n\n    # Access primary analyzer parameters if needed\n    fudge_factor = context.base_params.get(\"fudge_factor\")\n\n    # Write enhanced output\n    df_export.write_parquet(context.output(\"example_report\").parquet_path)\n</code></pre>"},{"location":"guides/contributing/analyzers/#web-presenters","title":"Web Presenters","text":"<p>Web presenters create interactive dashboards using either Dash or Shiny frameworks.</p>"},{"location":"guides/contributing/analyzers/#interface-definition_2","title":"Interface Definition","text":"<pre><code>from analyzer_interface import WebPresenterInterface\nfrom ..example_base import interface as example_base\nfrom ..example_report import interface as example_report\n\ninterface = WebPresenterInterface(\n    id=\"example_web\", \n    version=\"0.1.0\",\n    name=\"Message Length Histogram\",\n    short_description=\"Shows distribution of message lengths\",\n    base_analyzer=example_base,\n    depends_on=[example_report]  # Secondary analyzers used\n)\n</code></pre>"},{"location":"guides/contributing/analyzers/#shiny-implementation","title":"Shiny Implementation","text":"<p>For more interactive dashboards:</p> <pre><code>from shiny import reactive, render, ui\nfrom shinywidgets import output_widget, render_widget\nfrom analyzer_interface.context import WebPresenterContext, FactoryOutputContext, ShinyContext\n\ndef factory(context: WebPresenterContext) -&gt; FactoryOutputContext:\n    # Load data\n    df = pl.read_parquet(context.base.table(\"character_count\").parquet_path)\n\n    # Define UI components\n    analysis_panel = ui.card(\n        ui.card_header(\"Character Count Analysis\"),\n        ui.input_checkbox(\"show_details\", \"Show detailed view\", value=False),\n        output_widget(\"histogram\", height=\"400px\")\n    )\n\n    def server(input, output, session):\n        @render_widget\n        def histogram():\n            # Create interactive plot based on inputs\n            show_details = input.show_details()\n            # ... create plotly figure ...\n            return fig\n\n        @render.text\n        def summary():\n            return f\"Total messages: {len(df)}\"\n\n    return FactoryOutputContext(\n        shiny=ShinyContext(\n            server_handler=server,\n            panel=nav_panel(\"Dashboard\", analysis_panel)\n        )\n    )\n</code></pre>"},{"location":"guides/contributing/analyzers/#api-factory-for-react-dashboards","title":"API Factory for React Dashboards","text":"<p>Web presenters can also implement an <code>api_factory</code> function to provide structured data for React-based frontends through REST API endpoints:</p> <pre><code>from ..utils.pop import pop_unnecessary_fields\n\ndef api_factory(context: WebPresenterContext, options: Optional[dict[str, Any]] = None):\n    \"\"\"\n    Provides structured data for React dashboards via API endpoints.\n\n    Args:\n        context: WebPresenterContext with access to analyzer outputs\n        options: Optional parameters from API requests (filters, etc.)\n\n    Returns:\n        Dict with presenter metadata and processed data arrays\n    \"\"\"\n    # Extract API options/filters\n    filter_value = options.get(\"matcher\", \"\") if options else \"\"\n\n    # Load data\n    data_frame = pl.read_parquet(context.base.table(\"character_count\").parquet_path)\n\n    # Apply filters if provided\n    if filter_value:\n        # Apply filtering logic based on the filter_value\n        data_frame = data_frame.filter(pl.col(\"message_text\").str.contains(filter_value))\n\n    # Build presenter model with metadata\n    presenter_model = context.web_presenter.model_dump()\n\n    # Add visualization configuration\n    presenter_model[\"figure_type\"] = \"histogram\"\n    presenter_model[\"axis\"] = {\n        \"x\": {\"label\": \"Message Character Count\", \"value\": \"message_character_count\"},\n        \"y\": {\"label\": \"Number of Messages\", \"value\": \"number_of_messages\"}\n    }\n\n    # Add data arrays for the frontend\n    presenter_model[\"x\"] = data_frame[\"character_count\"].to_list()\n\n    # Remove internal fields not needed by frontend\n    return FactoryOutputContext(\n        api=pop_unnecessary_fields(presenter_model)\n    )\n</code></pre>"},{"location":"guides/contributing/analyzers/#multi-output-api-factory","title":"Multi-Output API Factory","text":"<p>For analyzers with multiple outputs, return a dictionary with different data views:</p> <pre><code>def api_factory(context: WebPresenterContext, options: Optional[dict[str, Any]] = None):\n    filter_value = options.get(\"matcher\", \"\") if options else \"\"\n\n    # Load different data sources\n    df_stats = pl.read_parquet(\n        context.dependency(ngram_stats).table(OUTPUT_NGRAM_STATS).parquet_path\n    )\n    df_full = pl.read_parquet(\n        context.dependency(ngram_stats).table(OUTPUT_NGRAM_FULL).parquet_path\n    )\n\n    # Apply filtering to both datasets\n    if filter_value:\n        matcher = create_word_matcher(filter_value, pl.col(COL_NGRAM_WORDS))\n        if matcher is not None:\n            df_stats = df_stats.filter(matcher)\n            df_full = df_full.filter(matcher)\n\n    # Create separate presenter models for each output\n    stats_model = context.web_presenter.model_dump()\n    full_model = context.web_presenter.model_dump()\n\n    # Configure stats view\n    stats_model.update({\n        \"figure_type\": \"scatter\",\n        \"explanation\": {\n            \"total_repetition\": \"N-grams to the right are repeated by more users...\",\n            \"amplification_factor\": \"N-grams higher up are repeated more times...\"\n        },\n        \"axis\": {\n            \"x\": {\"label\": \"User Count\", \"value\": \"user_count\"},\n            \"y\": {\"label\": \"Total Repetition\", \"value\": \"total_repetition\"}\n        },\n        \"x\": df_stats[COL_NGRAM_DISTINCT_POSTER_COUNT].to_list(),\n        \"y\": {\n            \"total_repetition\": df_stats[COL_NGRAM_TOTAL_REPS].to_list(),\n            \"amplification_factor\": (\n                df_stats[COL_NGRAM_TOTAL_REPS] / \n                df_stats[COL_NGRAM_DISTINCT_POSTER_COUNT]\n            ).to_list()\n        },\n        \"ngrams\": df_stats[COL_NGRAM_WORDS].to_list()\n    })\n\n    # Configure full data view  \n    full_model.update({\n        \"figure_type\": \"scatter\",\n        \"ids\": df_full[COL_NGRAM_ID].to_list(),\n        \"timestamps\": df_full[COL_MESSAGE_TIMESTAMP].to_list(),\n        \"messages\": df_full[COL_MESSAGE_TEXT].to_list(),\n        \"users\": df_full[COL_AUTHOR_ID].to_list(),\n        # ... additional fields for detailed view\n    })\n\n    return FactoryOutputContext(\n        api={\n            \"default_output\": OUTPUT_NGRAM_STATS,\n            OUTPUT_NGRAM_STATS: pop_unnecessary_fields(stats_model),\n            OUTPUT_NGRAM_FULL: pop_unnecessary_fields(full_model)\n        }\n    )\n</code></pre>"},{"location":"guides/contributing/analyzers/#api-endpoints","title":"API Endpoints","text":"<p>The API factory data is automatically exposed through REST endpoints:</p> <ul> <li><code>GET /api/presenters</code> - List all presenter data</li> <li><code>GET /api/presenters/{id}</code> - Get specific presenter data</li> <li><code>GET /api/presenters/{id}/download/{format}</code> - Download data as CSV/JSON/Excel</li> </ul> <p>Query parameters:</p> <ul> <li><code>output</code> - Specify which output to return (for multi-output presenters)</li> <li><code>filter_field</code> &amp; <code>filter_value</code> - Apply filtering</li> <li><code>matcher</code> - Text matching filter (passed to api_factory options)</li> </ul> <p>Example API usage:</p> <pre><code># Get basic presenter data\ncurl \"/api/presenters/ngram_repetition_by_poster\"\n\n# Get filtered data\ncurl \"/api/presenters/ngram_repetition_by_poster?filter_value=climate&amp;matcher=climate\"\n\n# Get specific output\ncurl \"/api/presenters/ngram_repetition_by_poster?output=ngram_full\"\n\n# Download as CSV\ncurl \"/api/presenters/ngram_repetition_by_poster/download/csv\"\n</code></pre>"},{"location":"guides/contributing/analyzers/#testing-analyzers","title":"Testing Analyzers","text":""},{"location":"guides/contributing/analyzers/#testing-primary-analyzers","title":"Testing Primary Analyzers","text":"<pre><code>from testing import CsvTestData, test_primary_analyzer\nfrom .interface import interface\nfrom .main import main\n\ndef test_example_analyzer():\n    test_primary_analyzer(\n        interface=interface,\n        main=main,\n        input=CsvTestData(\n            \"test_input.csv\",\n            semantics={\"message_id\": identifier}\n        ),\n        params={\"fudge_factor\": 10},\n        outputs={\n            \"character_count\": CsvTestData(\"expected_output.csv\")\n        }\n    )\n</code></pre>"},{"location":"guides/contributing/analyzers/#testing-secondary-analyzers","title":"Testing Secondary Analyzers","text":"<pre><code>from testing import test_secondary_analyzer, ParquetTestData\n\ndef test_example_report():\n    test_secondary_analyzer(\n        interface=interface,\n        main=main,\n        primary_params={\"fudge_factor\": 10},\n        primary_outputs={\n            \"character_count\": ParquetTestData(\"primary_output.parquet\")\n        },\n        expected_outputs={\n            \"example_report\": ParquetTestData(\"expected_report.parquet\")\n        }\n    )\n</code></pre>"},{"location":"guides/contributing/analyzers/#best-practices","title":"Best Practices","text":""},{"location":"guides/contributing/analyzers/#data-processing","title":"Data Processing","text":"<ul> <li>Always call <code>input_reader.preprocess()</code> on input data in primary analyzers</li> <li>Use <code>ProgressReporter</code> for long-running operations</li> <li>Handle missing or null data gracefully</li> <li>Use appropriate data types (avoid defaulting to small integer types)</li> </ul>"},{"location":"guides/contributing/analyzers/#interface-design","title":"Interface Design","text":"<ul> <li>Choose descriptive, globally unique IDs</li> <li>Provide comprehensive <code>name_hints</code> for better column matching</li> <li>Mark internal outputs that users shouldn't see directly</li> <li>Include helpful parameter descriptions and validation</li> </ul>"},{"location":"guides/contributing/analyzers/#performance","title":"Performance","text":"<ul> <li>Use lazy evaluation with Polars when possible</li> <li>Process data in chunks for large datasets</li> <li>Consider memory usage when designing algorithms</li> <li>Use appropriate file formats (parquet for structured data)</li> </ul>"},{"location":"guides/contributing/analyzers/#error-handling","title":"Error Handling","text":"<ul> <li>Validate parameters and input data</li> <li>Provide meaningful error messages</li> <li>Handle edge cases (empty datasets, missing columns)</li> <li>Use assertions for internal consistency checks</li> </ul>"},{"location":"guides/contributing/analyzers/#adding-to-the-suite","title":"Adding to the Suite","text":"<p>Register all analyzers in <code>analyzers/__init__.py</code>:</p> <pre><code>from analyzer_interface import AnalyzerSuite\nfrom .example.example_base import example_base\nfrom .example.example_report import example_report  \nfrom .example.example_web import example_web\n\nsuite = AnalyzerSuite(\n    all_analyzers=[\n        example_base,\n        example_report, \n        example_web,\n        # ... other analyzers\n    ]\n)\n</code></pre> <p>This creates a complete analysis pipeline that users can run through the application interface, from data input through interactive visualization.</p>"},{"location":"guides/contributing/analyzers/#next-steps","title":"Next Steps","text":"<p>Once you finish reading this it would be a good idea to review the sections for each domain. Might also be a good idea to review the sections that discuss implementing  Shiny, and React dashboards.</p> <ul> <li>Core Domain</li> <li>Edge Domain</li> <li>Content Domain</li> <li>Shiny Dashboards</li> <li>React Dashboards</li> </ul>"},{"location":"guides/contributing/contributing/","title":"Contributor Workflow","text":"<p>Before following this workflow please refer to our Getting Started page for instructions on installing dependencies and setting up your development environment.</p>"},{"location":"guides/contributing/contributing/#contributor-workflow","title":"Contributor Workflow","text":""},{"location":"guides/contributing/contributing/#overview","title":"Overview","text":"<p>All changes should be made in a feature branch, merged into <code>develop</code>, and later merged into <code>main</code> for a new release.</p>"},{"location":"guides/contributing/contributing/#contributing-new-changes","title":"Contributing new changes","text":"<ol> <li>Create a Feature Branch</li> <li>Branch from <code>develop</code> using <code>feature/&lt;name&gt;</code> or <code>bugfix/&lt;name&gt;</code>.</li> <li> <p>Example:</p> <pre><code>git checkout develop\ngit pull origin develop\ngit checkout -b feature/new-feature\n</code></pre> </li> <li> <p>Make Changes &amp; Push</p> </li> <li>Commit changes with clear messages.</li> <li> <p>Push the branch.</p> <pre><code>git add .\ngit commit -m \"Description of changes\"\ngit push origin feature/new-feature\n</code></pre> </li> <li> <p>Create a Pull Request</p> </li> <li>Open a PR to merge into <code>develop</code>.</li> <li> <p>Address any review feedback.</p> </li> <li> <p>Merge &amp; Clean Up</p> </li> <li>After approval, merge into <code>develop</code>.</li> <li> <p>Delete the feature branch.</p> </li> <li> <p>Release</p> </li> <li>When develop is clean and ready for a new major release, we will merge <code>develop</code> into <code>main</code>.</li> </ol>"},{"location":"guides/contributing/contributing/#workflow-diagram","title":"Workflow Diagram","text":"<pre><code>graph TD;\n    A[Feature Branch] --&gt;|Commit &amp; Push| B[Pull Request];\n    B --&gt;|Review &amp; Merge| C[Develop Branch];\n    C --&gt;|Release| D[Main Branch];\n</code></pre>"},{"location":"guides/contributing/contributing/#next-steps","title":"Next Steps","text":"<p>Once you finish reading this it's recommended to check out the architecture section.</p>"},{"location":"guides/contributing/logging/","title":"Logging","text":""},{"location":"guides/contributing/logging/#logging","title":"Logging","text":"<p>The application uses a structured JSON logging system that provides consistent logging across all modules. The logging system automatically separates critical alerts from diagnostic information.</p>"},{"location":"guides/contributing/logging/#logging-architecture","title":"Logging Architecture","text":"<ul> <li>Console Output: Only\u00a0<code>ERROR</code>\u00a0and\u00a0<code>CRITICAL</code>\u00a0messages are displayed on stderr</li> <li>File Output: All messages from\u00a0<code>INFO</code>\u00a0level and above are written to log files</li> <li>Log Format: All logs are structured JSON for easy parsing and analysis</li> <li>Log Rotation: Log files automatically rotate at 10MB with 5 backup files retained</li> <li>Log Location:\u00a0<code>~/.local/share/MangoTango/logs/mangotango.log</code>\u00a0(varies by platform)</li> </ul>"},{"location":"guides/contributing/logging/#using-the-logger-in-your-code","title":"Using the Logger in Your Code","text":""},{"location":"guides/contributing/logging/#basic-usage","title":"Basic Usage","text":"<pre><code>from app.logger import get_logger\n\n# Get a logger for your module\nlogger = get_logger(__name__)\n\n# Log at different levels\nlogger.debug(\"Detailed debugging information\")\nlogger.info(\"General information about program execution\")\nlogger.warning(\"Something unexpected happened, but the program continues\")\nlogger.error(\"A serious problem occurred\")\nlogger.critical(\"A very serious error occurred, program may not be able to continue\")\n</code></pre>"},{"location":"guides/contributing/logging/#example-log-output","title":"Example Log Output","text":"<p>Console (stderr) - Only errors:</p> <pre><code>{\"asctime\": \"2025-07-30 16:42:33,914\", \"name\": \"analyzers.hashtags\", \"levelname\": \"ERROR\", \"message\": \"Failed to process hashtags\", \"taskName\": null}\n</code></pre> <p>Log File - All info and above:</p> <pre><code>{\"asctime\": \"2025-07-30 16:42:33,910\", \"name\": \"analyzers.hashtags\", \"levelname\": \"INFO\", \"message\": \"Starting hashtag analysis\", \"taskName\": null}\n{\"asctime\": \"2025-07-30 16:42:33,914\", \"name\": \"analyzers.hashtags\", \"levelname\": \"ERROR\", \"message\": \"Failed to process hashtags\", \"taskName\": null}\n</code></pre>"},{"location":"guides/contributing/logging/#logging-in-analyzers","title":"Logging in Analyzers","text":"<p>When developing analyzers, add logging to help with debugging and monitoring:</p> <pre><code>from app.logger import get_logger\n\ndef main(context):\n    logger = get_logger(__name__)\n\n    logger.info(\"Starting analysis\", extra={\n        \"input_path\": str(context.input_path),\n        \"output_path\": str(context.output_path)\n    })\n\n    try:\n        # Your analysis code here\n        result = perform_analysis(context)\n\n        logger.info(\"Analysis completed successfully\", extra={\n            \"records_processed\": len(result),\n            \"execution_time\": time.time() - start_time\n        })\n\n    except Exception as e:\n        logger.error(\"Analysis failed\", extra={\n            \"error\": str(e),\n            \"error_type\": type(e).__name__\n        }, exc_info=True)\n        raise\n</code></pre>"},{"location":"guides/contributing/logging/#logging-best-practices","title":"Logging Best Practices","text":"<ol> <li> <p>Use Appropriate Log Levels:</p> <ul> <li><code>DEBUG</code>: Detailed diagnostic information, only useful when debugging</li> <li><code>INFO</code>: General information about program execution</li> <li><code>WARNING</code>: Something unexpected happened, but the program continues</li> <li><code>ERROR</code>: A serious problem occurred</li> <li><code>CRITICAL</code>: A very serious error occurred, program may not be able to continue</li> <li>Include Context with\u00a0<code>extra</code>\u00a0Parameter:</li> </ul> <pre><code>logger.info(\"Processing file\", extra={\n    \"filename\": filename,\n    \"file_size\": file_size,\n    \"record_count\": record_count\n})\n</code></pre> </li> <li> <p>Log Exceptions Properly:</p> <pre><code>try:\n    risky_operation()\nexcept Exception as e:\n    logger.error(\"Operation failed\", exc_info=True)  # Includes stack trace\n</code></pre> </li> <li> <p>Avoid Logging Sensitive Information:</p> <ul> <li>Never log passwords, API keys, or personal data</li> <li>Be cautious with user-provided data</li> </ul> </li> </ol>"},{"location":"guides/contributing/logging/#debugging-with-logs","title":"Debugging with Logs","text":"<p>Users can control log verbosity when running the application:</p> <pre><code># Default INFO level\npython -m cibmangotree\n\n# Verbose DEBUG level for troubleshooting\npython -m cibmangotree --log-level DEBUG\n\n# Only show warnings and errors in log file\npython -m cibmangotree --log-level WARNING\n</code></pre>"},{"location":"guides/contributing/logging/#log-file-management","title":"Log File Management","text":"<ul> <li>Log files are automatically rotated when they reach 10MB</li> <li>Up to 5 backup files are kept (<code>mangotango.log.1</code>,\u00a0<code>mangotango.log.2</code>, etc.)</li> <li>Older backup files are automatically deleted</li> <li>Log directory is created automatically if it doesn't exist</li> </ul>"},{"location":"guides/contributing/logging/#testing-with-logs","title":"Testing with Logs","text":"<p>When writing tests that involve logging:</p> <pre><code>import logging\nfrom app.logger import get_logger\n\ndef test_my_function_logs_correctly(caplog):\n    with caplog.at_level(logging.INFO):\n        my_function()\n\n    assert \"Expected log message\" in caplog.text\n</code></pre>"},{"location":"guides/contributing/logging/#next-steps","title":"Next Steps","text":"<p>Once you finish reading this it's recommended to check out the architecture section.</p>"},{"location":"guides/contributing/testing/","title":"Testing","text":""},{"location":"guides/contributing/testing/#testing","title":"Testing","text":"<p>The <code>testing</code> module provides testers for the primary and secondary analyzer modules. See the example for further references.</p>"},{"location":"guides/contributing/dashboards/react/","title":"React (WIP)","text":""},{"location":"guides/contributing/dashboards/react/#react-dashboards-guide","title":"React Dashboards Guide","text":"<p>Web presenters can create modern, client-side dashboards using React for rich interactivity and responsive user experiences. React dashboards consume data through REST APIs and provide smooth, app-like interfaces.</p>"},{"location":"guides/contributing/dashboards/react/#overview","title":"Overview","text":"<p>React dashboards in this platform provide:</p> <ul> <li>Client-side rendering: Fast, responsive user interfaces</li> <li>API-driven: Clean separation between data and presentation</li> <li>Modern UI components: Built with shadcn/ui and Tailwind CSS</li> <li>Rich visualizations: Interactive charts with Deck.gl and Visx</li> <li>Real-time interactions: Immediate feedback and smooth animations</li> </ul>"},{"location":"guides/contributing/dashboards/react/#architecture","title":"Architecture","text":"<p>React dashboards follow a three-tier architecture:</p> <ol> <li>Web Presenter (Python): Implements <code>api_factory</code> to serve structured data via REST API</li> <li>API Layer: Automatically generated endpoints that serve presenter data as JSON</li> <li>React Frontend: TypeScript components that consume the API and render interactive UI</li> </ol> <pre><code>graph TB\n    A[Analyzer Data] --&gt; B[Web Presenter api_factory]\n    B --&gt; C[REST API Endpoints]\n    C --&gt; D[React Components]\n    D --&gt; E[Interactive Dashboard]\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#setting-up-api-factory","title":"Setting Up API Factory","text":"<p>The <code>api_factory</code> function transforms analyzer outputs into structured data for React consumption:</p>"},{"location":"guides/contributing/dashboards/react/#basic-api-factory","title":"Basic API Factory","text":"<pre><code>from typing import Optional, Any\nfrom analyzer_interface.context import WebPresenterContext, FactoryOutputContext\nfrom ..utils.pop import pop_unnecessary_fields\n\ndef api_factory(context: WebPresenterContext, options: Optional[dict[str, Any]] = None):\n    \"\"\"\n    Transform analyzer data for React dashboard consumption.\n\n    Args:\n        context: Access to analyzer outputs and metadata\n        options: Query parameters from API requests (filters, pagination, etc.)\n\n    Returns:\n        FactoryOutputContext with API-formatted data\n    \"\"\"\n    # Extract API options\n    filter_value = options.get(\"filter_value\", \"\") if options else \"\"\n    matcher = options.get(\"matcher\", \"\") if options else \"\"\n\n    # Load analyzer data\n    df = pl.read_parquet(context.base.table(\"main_output\").parquet_path)\n\n    # Apply filtering based on API parameters\n    if filter_value:\n        df = df.filter(pl.col(\"category\") == filter_value)\n\n    if matcher:\n        df = df.filter(pl.col(\"text_field\").str.contains(matcher, literal=False))\n\n    # Build presenter model with metadata\n    presenter_model = context.web_presenter.model_dump()\n\n    # Add visualization configuration\n    presenter_model.update({\n        \"figure_type\": \"scatter\",  # histogram, bar, scatter\n        \"axis\": {\n            \"x\": {\"label\": \"User Count\", \"value\": \"user_count\"},\n            \"y\": {\"label\": \"Message Count\", \"value\": \"message_count\"}\n        },\n        \"explanation\": {\n            \"main_view\": \"This chart shows the relationship between users and messages...\"\n        }\n    })\n\n    # Add data arrays for visualization\n    presenter_model[\"x\"] = df[\"x_column\"].to_list()\n    presenter_model[\"y\"] = df[\"y_column\"].to_list()\n    presenter_model[\"labels\"] = df[\"label_column\"].to_list()\n\n    # Remove internal fields not needed by frontend\n    return FactoryOutputContext(\n        api=pop_unnecessary_fields(presenter_model)\n    )\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#multi-output-api-factory","title":"Multi-Output API Factory","text":"<p>For analyzers with multiple data views:</p> <pre><code>def api_factory(context: WebPresenterContext, options: Optional[dict[str, Any]] = None):\n    # Determine which output to return\n    output_type = options.get(\"output\", \"default\") if options else \"default\"\n\n    # Load different datasets\n    df_summary = pl.read_parquet(\n        context.base.table(\"summary_stats\").parquet_path\n    )\n    df_details = pl.read_parquet(\n        context.dependency(detail_analyzer).table(\"full_details\").parquet_path\n    )\n\n    # Apply common filtering\n    filter_value = options.get(\"filter_value\", \"\") if options else \"\"\n    if filter_value:\n        df_summary = df_summary.filter(pl.col(\"category\") == filter_value)\n        df_details = df_details.filter(pl.col(\"category\") == filter_value)\n\n    # Create different presenter models for each output\n    base_model = context.web_presenter.model_dump()\n\n    summary_model = base_model.copy()\n    summary_model.update({\n        \"figure_type\": \"scatter\",\n        \"axis\": {\n            \"x\": {\"label\": \"Users\", \"value\": \"user_count\"},\n            \"y\": {\"label\": \"Messages\", \"value\": \"message_count\"}\n        },\n        \"x\": df_summary[\"user_count\"].to_list(),\n        \"y\": df_summary[\"message_count\"].to_list(),\n        \"labels\": df_summary[\"category\"].to_list()\n    })\n\n    details_model = base_model.copy()\n    details_model.update({\n        \"figure_type\": \"table\",\n        \"columns\": [\"user_id\", \"message_text\", \"timestamp\", \"category\"],\n        \"data\": df_details.to_dicts()\n    })\n\n    return FactoryOutputContext(\n        api={\n            \"default_output\": \"summary\",\n            \"summary\": pop_unnecessary_fields(summary_model),\n            \"details\": pop_unnecessary_fields(details_model)\n        }\n    )\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#complex-filtering-and-search","title":"Complex Filtering and Search","text":"<pre><code>def api_factory(context: WebPresenterContext, options: Optional[dict[str, Any]] = None):\n    # Load base data\n    df = pl.read_parquet(context.base.table(\"ngram_analysis\").parquet_path)\n\n    # Extract search/filter parameters\n    search_term = options.get(\"matcher\", \"\") if options else \"\"\n    date_range = options.get(\"date_range\") if options else None\n    min_frequency = options.get(\"min_frequency\", 0) if options else 0\n\n    # Apply filters progressively\n    if search_term:\n        # Create word matcher for ngram search\n        search_filter = pl.col(\"ngram_text\").str.contains(\n            search_term.lower(), literal=False\n        )\n        df = df.filter(search_filter)\n\n    if date_range:\n        start_date, end_date = date_range.split(\",\")\n        df = df.filter(\n            pl.col(\"timestamp\").is_between(\n                pl.datetime(start_date), \n                pl.datetime(end_date)\n            )\n        )\n\n    if min_frequency &gt; 0:\n        df = df.filter(pl.col(\"frequency\") &gt;= min_frequency)\n\n    # Sort and limit results for performance\n    df = df.sort(\"frequency\", descending=True).head(1000)\n\n    # Build API response\n    presenter_model = context.web_presenter.model_dump()\n    presenter_model.update({\n        \"figure_type\": \"scatter\",\n        \"axis\": {\n            \"x\": {\"label\": \"User Repetition\", \"value\": \"user_repetition\"},\n            \"y\": {\"label\": \"Total Repetition\", \"value\": \"total_repetition\"}\n        },\n        \"explanation\": {\n            \"total_repetition\": \"N-grams to the right are repeated by more users...\",\n            \"user_repetition\": \"N-grams higher up show higher amplification...\"\n        },\n        # Data for visualization\n        \"x\": df[\"user_count\"].to_list(),\n        \"y\": df[\"total_count\"].to_list(),\n        \"ngrams\": df[\"ngram_text\"].to_list(),\n        \"frequencies\": df[\"frequency\"].to_list(),\n        \"rankings\": list(range(1, len(df) + 1))\n    })\n\n    return FactoryOutputContext(\n        api=pop_unnecessary_fields(presenter_model)\n    )\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#api-endpoints","title":"API Endpoints","text":"<p>The API factory data is automatically exposed through these REST endpoints:</p>"},{"location":"guides/contributing/dashboards/react/#standard-endpoints","title":"Standard Endpoints","text":"<pre><code># List all presenters\nGET /api/presenters\n\n# Get specific presenter data  \nGET /api/presenters/{presenter_id}\n\n# Get specific output (for multi-output presenters)\nGET /api/presenters/{presenter_id}?output=details\n\n# Apply filters\nGET /api/presenters/{presenter_id}?filter_field=category&amp;filter_value=news\n\n# Search/match text\nGET /api/presenters/{presenter_id}?matcher=climate\n\n# Combine parameters\nGET /api/presenters/{presenter_id}?output=summary&amp;matcher=election&amp;min_frequency=5\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#download-endpoints","title":"Download Endpoints","text":"<pre><code># Download as CSV\nGET /api/presenters/{presenter_id}/download/csv\n\n# Download as JSON\nGET /api/presenters/{presenter_id}/download/json\n\n# Download as Excel\nGET /api/presenters/{presenter_id}/download/excel\n\n# Download with filters applied\nGET /api/presenters/{presenter_id}/download/csv?filter_value=news&amp;matcher=climate \n</code></pre>"},{"location":"guides/contributing/dashboards/react/#react-component-architecture","title":"React Component Architecture","text":"<p>The React frontend is organized into reusable components that work together to create cohesive dashboards:</p>"},{"location":"guides/contributing/dashboards/react/#core-component-structure","title":"Core Component Structure","text":"<pre><code>// Main presenter component\nexport default function NgramScatterPlot({ presenter }: ChartContainerProps&lt;NgramPresenterStats&gt;): ReactElement&lt;FC&gt; {\n    const [searchValue, setSearchValue] = useState&lt;string&gt;('');\n    const [selectedItem, setSelectedItem] = useState&lt;string&gt;('');\n    const [currentTab, setCurrentTab] = useState&lt;'total_repetition' | 'amplification_factor'&gt;('total_repetition');\n\n    // Data fetching and state management\n    const { data, isLoading, error } = usePresenterData(presenter.id, {\n        matcher: searchValue,\n        output: currentTab\n    });\n\n    // Event handlers\n    const handleSearch = (value: string) =&gt; setSearchValue(value);\n    const handleItemSelect = (item: DataPoint) =&gt; setSelectedItem(item.id);\n    const handleTabChange = (tab: string) =&gt; setCurrentTab(tab);\n\n    return (\n        &lt;div className=\"space-y-6\"&gt;\n            {/* Controls */}\n            &lt;div className=\"flex justify-between items-center\"&gt;\n                &lt;SearchBar onSubmit={handleSearch} onClear={() =&gt; setSearchValue('')} /&gt;\n                &lt;DownloadButton presenterID={presenter.id} /&gt;\n            &lt;/div&gt;\n\n            {/* Tabs for different views */}\n            &lt;Tabs value={currentTab} onValueChange={handleTabChange}&gt;\n                &lt;TabsList&gt;\n                    &lt;TabsTrigger value=\"total_repetition\"&gt;Total Repetition&lt;/TabsTrigger&gt;\n                    &lt;TabsTrigger value=\"amplification_factor\"&gt;Amplification&lt;/TabsTrigger&gt;\n                &lt;/TabsList&gt;\n\n                &lt;TabsContent value=\"total_repetition\"&gt;\n                    &lt;ScatterPlot \n                        data={data} \n                        onItemClick={handleItemSelect}\n                        tooltip={createTooltipFormatter('total_repetition')} \n                    /&gt;\n                &lt;/TabsContent&gt;\n\n                &lt;TabsContent value=\"amplification_factor\"&gt;\n                    &lt;ScatterPlot \n                        data={data} \n                        onItemClick={handleItemSelect}\n                        tooltip={createTooltipFormatter('amplification_factor')} \n                    /&gt;\n                &lt;/TabsContent&gt;\n            &lt;/Tabs&gt;\n\n            {/* Data table */}\n            &lt;DataTable \n                data={data}\n                columns={tableColumns}\n                onRowSelect={handleItemSelect}\n                selectedRows={selectedItem ? [selectedItem] : []}\n            /&gt;\n        &lt;/div&gt;\n    );\n}\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#data-fetching-hooks","title":"Data Fetching Hooks","text":"<p>Custom hooks manage API communication and state:</p> <pre><code>// hooks/usePresenterData.ts\nimport { useState, useEffect } from 'react';\nimport { fetchPresenter } from '@/lib/data/presenters';\n\ninterface UsePresenterDataOptions {\n    matcher?: string;\n    output?: string;\n    filter_field?: string;\n    filter_value?: string;\n    enabled?: boolean;\n}\n\nexport function usePresenterData&lt;T extends Presenter&gt;(\n    presenterId: string, \n    options: UsePresenterDataOptions = {}\n) {\n    const [data, setData] = useState&lt;T | null&gt;(null);\n    const [isLoading, setIsLoading] = useState(false);\n    const [error, setError] = useState&lt;string | null&gt;(null);\n\n    useEffect(() =&gt; {\n        if (!options.enabled &amp;&amp; options.enabled !== undefined) return;\n\n        const controller = new AbortController();\n\n        const loadData = async () =&gt; {\n            setIsLoading(true);\n            setError(null);\n\n            try {\n                const result = await fetchPresenter(\n                    presenterId, \n                    controller.signal, \n                    options\n                );\n\n                if (result) {\n                    setData(result);\n                } else {\n                    setError('Failed to load data');\n                }\n            } catch (err) {\n                if (!controller.signal.aborted) {\n                    setError(err instanceof Error ? err.message : 'Unknown error');\n                }\n            } finally {\n                setIsLoading(false);\n            }\n        };\n\n        loadData();\n\n        return () =&gt; controller.abort();\n    }, [presenterId, options.matcher, options.output, options.filter_value, options.enabled]);\n\n    return { data, isLoading, error };\n}\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#chart-components","title":"Chart Components","text":"<p>Interactive visualizations using Deck.gl:</p> <pre><code>// components/charts/scatter.tsx\nimport { useMemo, useRef, useState } from 'react';\nimport { ScatterplotLayer } from '@deck.gl/layers';\nimport { COORDINATE_SYSTEM } from '@deck.gl/core';\nimport DeckGL from '@deck.gl/react';\nimport { AxisLeft, AxisBottom } from '@visx/axis';\nimport useChart from '@/lib/hooks/chart';\n\ninterface ScatterPlotProps {\n    data: Array&lt;DataPoint&gt;;\n    onItemClick?: (item: PickingInfo&lt;DataPoint&gt;) =&gt; void;\n    tooltip: TooltipFunction&lt;DataPoint&gt;;\n    darkMode?: boolean;\n    dimensions?: Dimensions;\n}\n\nexport default function ScatterPlot({\n    data,\n    onItemClick,\n    tooltip,\n    darkMode = false,\n    dimensions = { width: 800, height: 600, margins: { top: 20, right: 40, bottom: 40, left: 60 }}\n}: ScatterPlotProps) {\n    const deckRef = useRef&lt;DeckGLRef | null&gt;(null);\n    const [deckInstance, setDeckInstance] = useState&lt;Deck | null&gt;(null);\n\n    // Custom hook handles coordinate transformation and scaling\n    const { data: plotData, deckProps, axis, viewport } = useChart(\n        data,\n        tooltip,\n        deckInstance,\n        true, // resetZoomOnChange\n        { x: { type: 'log', show: true }, y: { type: 'log', show: true } },\n        dimensions\n    );\n\n    // Create Deck.gl layers\n    const layers = useMemo(() =&gt; [\n        new ScatterplotLayer({\n            id: 'scatter-points',\n            data: plotData,\n            pickable: true,\n            opacity: 0.8,\n            stroked: false,\n            filled: true,\n            radiusScale: 6,\n            radiusMinPixels: 2,\n            radiusMaxPixels: 8,\n            coordinateSystem: COORDINATE_SYSTEM.CARTESIAN,\n            getPosition: (d: any) =&gt; d.position,\n            getFillColor: (d: any) =&gt; d.color,\n            updateTriggers: {\n                getFillColor: [darkMode, viewport.viewState.zoom]\n            },\n            transitions: {\n                getPosition: { duration: 300, type: 'spring' },\n                getFillColor: { duration: 200 }\n            }\n        })\n    ], [plotData, darkMode, viewport.viewState.zoom]);\n\n    return (\n        &lt;div className=\"relative\"&gt;\n            {/* Zoom controls */}\n            &lt;div className=\"absolute top-4 right-4 z-10\"&gt;\n                &lt;ToolBox \n                    features={['zoom', 'restore']}\n                    zoomIncrement={viewport.hooks.increment}\n                    zoomDecrement={viewport.hooks.decrement}\n                    zoomReset={viewport.hooks.reset}\n                /&gt;\n            &lt;/div&gt;\n\n            {/* Main chart area */}\n            &lt;div style={{ position: 'relative', width: dimensions.width, height: dimensions.height }}&gt;\n                &lt;DeckGL\n                    ref={deckRef}\n                    {...deckProps}\n                    layers={layers}\n                    onClick={onItemClick}\n                    onAfterRender={() =&gt; {\n                        if (deckRef.current?.deck &amp;&amp; !deckInstance) {\n                            setDeckInstance(deckRef.current.deck);\n                        }\n                    }}\n                /&gt;\n\n                {/* Axes overlay */}\n                &lt;svg width={dimensions.width} height={dimensions.height}&gt;\n                    &lt;AxisBottom\n                        scale={axis.x.scale}\n                        top={dimensions.height - dimensions.margins.bottom}\n                        tickLabelProps={{\n                            fill: darkMode ? '#fff' : '#000',\n                            fontSize: 10,\n                            textAnchor: 'middle'\n                        }}\n                        stroke={darkMode ? '#fff' : '#000'}\n                        tickStroke={darkMode ? '#fff' : '#000'}\n                    /&gt;\n                    &lt;AxisLeft\n                        scale={axis.y.scale}\n                        left={dimensions.margins.left}\n                        tickLabelProps={{\n                            fill: darkMode ? '#fff' : '#000',\n                            fontSize: 10,\n                            textAnchor: 'end'\n                        }}\n                        stroke={darkMode ? '#fff' : '#000'}\n                        tickStroke={darkMode ? '#fff' : '#000'}\n                    /&gt;\n                &lt;/svg&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;\n    );\n}\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#interactive-data-tables","title":"Interactive Data Tables","text":"<p>Rich data tables with selection and filtering:</p> <pre><code>// components/data_table.tsx\nimport { useMemo, useCallback, useState, useEffect } from 'react';\nimport { DataEditor, GridCellKind, CompactSelection } from '@glideapps/glide-data-grid';\n\ninterface DataTableProps&lt;T extends BaseRow&gt; {\n    data: Array&lt;T&gt;;\n    columns: Array&lt;GridColumn&gt;;\n    onRowSelect?: (item: T | null, selection?: GridSelection) =&gt; void;\n    selectedRows?: CompactSelection;\n    darkMode?: boolean;\n    theme?: Partial&lt;Theme&gt;;\n}\n\nexport default function DataTable&lt;T extends BaseRow&gt;({\n    data,\n    columns,\n    onRowSelect,\n    selectedRows,\n    darkMode = false,\n    theme\n}: DataTableProps&lt;T&gt;) {\n    const [gridSelection, setGridSelection] = useState&lt;GridSelection&gt;({\n        columns: CompactSelection.empty(),\n        rows: selectedRows ?? CompactSelection.empty()\n    });\n\n    // Column mapping for data access\n    const columnIds = useMemo(() =&gt; \n        columns.map(col =&gt; col.id).filter(Boolean) as string[]\n    , [columns]);\n\n    // Cell content renderer\n    const getCellContent = useCallback(([col, row]: Item): GridCell =&gt; {\n        const item = data[row];\n        const columnId = columnIds[col];\n        const value = item[columnId];\n\n        // Determine cell type based on value\n        let cellType = GridCellKind.Text;\n        if (typeof value === 'number') cellType = GridCellKind.Number;\n        if (typeof value === 'boolean') cellType = GridCellKind.Boolean;\n\n        return {\n            kind: cellType,\n            allowOverlay: false,\n            displayData: String(value ?? ''),\n            data: value\n        };\n    }, [data, columnIds]);\n\n    // Selection handler\n    const handleSelectionChange = useCallback((selection: GridSelection) =&gt; {\n        setGridSelection(selection);\n\n        const selectedRow = selection.rows.first();\n        const item = selectedRow !== undefined ? data[selectedRow] : null;\n\n        onRowSelect?.(item, selection);\n    }, [data, onRowSelect]);\n\n    // Theme configuration\n    const tableTheme = useMemo(() =&gt; {\n        if (theme) return theme;\n\n        return darkMode ? {\n            accentColor: '#8c96ff',\n            textDark: '#ffffff',\n            textMedium: '#b8b8b8',\n            bgCell: '#16161b',\n            bgHeader: '#212121',\n            borderColor: 'rgba(225,225,225,0.2)',\n            fontFamily: 'Inter, sans-serif'\n        } : {};\n    }, [darkMode, theme]);\n\n    // Sync external selection changes\n    useEffect(() =&gt; {\n        if (selectedRows &amp;&amp; !selectedRows.equals(gridSelection.rows)) {\n            setGridSelection(prev =&gt; ({ ...prev, rows: selectedRows }));\n        }\n    }, [selectedRows]);\n\n    return (\n        &lt;DataEditor\n            width=\"100%\"\n            height=\"50rem\"\n            className=\"rounded-md border shadow-md\"\n            columns={columns}\n            rows={data.length}\n            getCellContent={getCellContent}\n            gridSelection={gridSelection}\n            onGridSelectionChange={handleSelectionChange}\n            theme={tableTheme}\n            rowSelect=\"single\"\n            rowMarkers=\"checkbox-visible\"\n        /&gt;\n    );\n}\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#search-and-filtering-components","title":"Search and Filtering Components","text":"<p>Reusable search components with autocomplete:</p> <pre><code>// components/search.tsx\nimport { useState, useEffect, useMemo } from 'react';\nimport { useVirtualizer } from '@tanstack/react-virtual';\nimport { Input } from '@/components/ui/input';\nimport { Button } from '@/components/ui/button';\nimport { X } from 'lucide-react';\n\ninterface SearchBarProps {\n    searchList: Array&lt;string&gt;;\n    onSubmit: (value: string) =&gt; void;\n    onClear?: () =&gt; void;\n    placeholder?: string;\n    maxSuggestions?: number;\n}\n\nexport default function SearchBar({ \n    searchList, \n    onSubmit, \n    onClear, \n    placeholder = \"Search...\",\n    maxSuggestions = 100\n}: SearchBarProps) {\n    const [value, setValue] = useState('');\n    const [showSuggestions, setShowSuggestions] = useState(false);\n    const containerRef = useRef&lt;HTMLDivElement&gt;(null);\n\n    // Filter suggestions based on input\n    const suggestions = useMemo(() =&gt; {\n        if (!value) return [];\n\n        return searchList\n            .filter(item =&gt; item.toLowerCase().includes(value.toLowerCase()))\n            .slice(0, maxSuggestions);\n    }, [value, searchList, maxSuggestions]);\n\n    // Virtual scrolling for large suggestion lists\n    const virtualizer = useVirtualizer({\n        count: suggestions.length,\n        getScrollElement: () =&gt; containerRef.current,\n        estimateSize: () =&gt; 35\n    });\n\n    const handleSubmit = (e: FormEvent) =&gt; {\n        e.preventDefault();\n        onSubmit(value);\n        setShowSuggestions(false);\n    };\n\n    const handleSuggestionClick = (suggestion: string) =&gt; {\n        setValue(suggestion);\n        onSubmit(suggestion);\n        setShowSuggestions(false);\n    };\n\n    const handleClear = () =&gt; {\n        setValue('');\n        onClear?.();\n    };\n\n    // Show/hide suggestions based on focus and value\n    useEffect(() =&gt; {\n        setShowSuggestions(suggestions.length &gt; 0 &amp;&amp; value.length &gt; 0);\n    }, [suggestions.length, value.length]);\n\n    return (\n        &lt;form onSubmit={handleSubmit} className=\"relative\"&gt;\n            &lt;div className=\"flex items-center space-x-2\"&gt;\n                &lt;Input\n                    type=\"text\"\n                    value={value}\n                    onChange={(e) =&gt; setValue(e.target.value)}\n                    placeholder={placeholder}\n                    className=\"w-80\"\n                    onFocus={() =&gt; suggestions.length &gt; 0 &amp;&amp; setShowSuggestions(true)}\n                    onBlur={() =&gt; setTimeout(() =&gt; setShowSuggestions(false), 150)}\n                /&gt;\n                {value &amp;&amp; (\n                    &lt;Button\n                        type=\"button\"\n                        variant=\"ghost\"\n                        size=\"icon\"\n                        onClick={handleClear}\n                    &gt;\n                        &lt;X className=\"h-4 w-4\" /&gt;\n                    &lt;/Button&gt;\n                )}\n            &lt;/div&gt;\n\n            {/* Suggestions dropdown */}\n            {showSuggestions &amp;&amp; (\n                &lt;div\n                    ref={containerRef}\n                    className=\"absolute z-50 w-80 max-h-96 mt-1 bg-white border rounded-md shadow-lg overflow-auto dark:bg-zinc-950 dark:border-zinc-800\"\n                &gt;\n                    &lt;div\n                        style={{ height: virtualizer.getTotalSize() }}\n                        className=\"relative\"\n                    &gt;\n                        {virtualizer.getVirtualItems().map((item) =&gt; (\n                            &lt;div\n                                key={item.key}\n                                className=\"absolute w-full px-3 py-2 cursor-pointer hover:bg-zinc-100 dark:hover:bg-zinc-800\"\n                                style={{\n                                    height: item.size,\n                                    transform: `translateY(${item.start}px)`\n                                }}\n                                onMouseDown={() =&gt; handleSuggestionClick(suggestions[item.index])}\n                            &gt;\n                                {suggestions[item.index]}\n                            &lt;/div&gt;\n                        ))}\n                    &lt;/div&gt;\n                &lt;/div&gt;\n            )}\n        &lt;/form&gt;\n    );\n}\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#download-components","title":"Download Components","text":"<p>Export functionality for data:</p> <pre><code>// components/download.tsx\nimport { useState } from 'react';\nimport { Button } from '@/components/ui/button';\nimport { DropdownMenu, DropdownMenuContent, DropdownMenuItem, DropdownMenuTrigger } from '@/components/ui/dropdown-menu';\nimport { Sheet, Braces, Table, ChevronDown } from 'lucide-react';\nimport { createDownloadLink } from '@/lib/data/download';\n\ninterface DownloadButtonProps {\n    presenterID: string;\n    queryParams?: PresenterQueryParams;\n    label?: string;\n}\n\nexport function DownloadButton({ \n    presenterID, \n    queryParams, \n    label = \"Export\" \n}: DownloadButtonProps) {\n    const [isOpen, setIsOpen] = useState(false);\n\n    const downloadFormats = [\n        { type: 'excel', icon: Sheet, label: 'Excel', extension: '.xlsx' },\n        { type: 'csv', icon: Table, label: 'CSV', extension: '.csv' },\n        { type: 'json', icon: Braces, label: 'JSON', extension: '.json' }\n    ] as const;\n\n    return (\n        &lt;DropdownMenu open={isOpen} onOpenChange={setIsOpen}&gt;\n            &lt;DropdownMenuTrigger asChild&gt;\n                &lt;Button variant=\"outline\" size=\"sm\"&gt;\n                    {label}\n                    &lt;ChevronDown className={`ml-2 h-4 w-4 transition-transform ${isOpen ? 'rotate-180' : ''}`} /&gt;\n                &lt;/Button&gt;\n            &lt;/DropdownMenuTrigger&gt;\n\n            &lt;DropdownMenuContent align=\"end\"&gt;\n                {downloadFormats.map(({ type, icon: Icon, label: formatLabel }) =&gt; (\n                    &lt;DropdownMenuItem key={type} asChild&gt;\n                        &lt;a\n                            href={createDownloadLink(presenterID, type, queryParams)}\n                            target=\"_blank\"\n                            rel=\"noopener noreferrer\"\n                            className=\"flex items-center w-full\"\n                        &gt;\n                            &lt;Icon className=\"mr-2 h-4 w-4\" /&gt;\n                            {formatLabel}\n                        &lt;/a&gt;\n                    &lt;/DropdownMenuItem&gt;\n                ))}\n            &lt;/DropdownMenuContent&gt;\n        &lt;/DropdownMenu&gt;\n    );\n}\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#state-management","title":"State Management","text":""},{"location":"guides/contributing/dashboards/react/#local-component-state","title":"Local Component State","text":"<p>For simple interactions, use React's built-in state:</p> <pre><code>function NgramDashboard({ presenter }: DashboardProps) {\n    // UI state\n    const [searchTerm, setSearchTerm] = useState('');\n    const [selectedTab, setSelectedTab] = useState&lt;TabType&gt;('overview');\n    const [selectedItems, setSelectedItems] = useState&lt;string[]&gt;([]);\n\n    // Data state with custom hook\n    const { data, isLoading, error } = usePresenterData(presenter.id, {\n        matcher: searchTerm,\n        output: selectedTab\n    });\n\n    // Derived state\n    const filteredData = useMemo(() =&gt; {\n        if (!data || !searchTerm) return data;\n        return data.filter(item =&gt; \n            item.text.toLowerCase().includes(searchTerm.toLowerCase())\n        );\n    }, [data, searchTerm]);\n\n    // Event handlers\n    const handleSearch = useCallback((term: string) =&gt; {\n        setSearchTerm(term);\n        setSelectedItems([]); // Clear selection on new search\n    }, []);\n\n    return (\n        // Component JSX\n    );\n}\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#global-state-with-zustand","title":"Global State with Zustand","text":"<p>For complex dashboards with shared state:</p> <pre><code>// stores/dashboardStore.ts\nimport { create } from 'zustand';\n\ninterface DashboardState {\n    // Data\n    presenters: PresenterCollection;\n    currentPresenter: Presenter | null;\n\n    // UI state\n    sidebarOpen: boolean;\n    theme: 'light' | 'dark' | 'system';\n\n    // Filters\n    globalFilters: {\n        dateRange?: [Date, Date];\n        categories: string[];\n        searchTerm: string;\n    };\n\n    // Actions\n    setCurrentPresenter: (presenter: Presenter) =&gt; void;\n    updateFilters: (filters: Partial&lt;DashboardState['globalFilters']&gt;) =&gt; void;\n    toggleSidebar: () =&gt; void;\n}\n\nexport const useDashboardStore = create&lt;DashboardState&gt;((set, get) =&gt; ({\n    presenters: [],\n    currentPresenter: null,\n    sidebarOpen: true,\n    theme: 'system',\n    globalFilters: {\n        categories: [],\n        searchTerm: ''\n    },\n\n    setCurrentPresenter: (presenter) =&gt; set({ currentPresenter: presenter }),\n\n    updateFilters: (newFilters) =&gt; set(state =&gt; ({\n        globalFilters: { ...state.globalFilters, ...newFilters }\n    })),\n\n    toggleSidebar: () =&gt; set(state =&gt; ({ sidebarOpen: !state.sidebarOpen }))\n}));\n\n// Usage in components\nfunction Dashboard() {\n    const { currentPresenter, globalFilters, updateFilters } = useDashboardStore();\n\n    const handleSearch = (searchTerm: string) =&gt; {\n        updateFilters({ searchTerm });\n    };\n\n    return (\n        // Dashboard JSX\n    );\n}\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#styling-and-theming","title":"Styling and Theming","text":""},{"location":"guides/contributing/dashboards/react/#tailwind-css-classes","title":"Tailwind CSS Classes","text":"<p>The project uses Tailwind CSS for utility-first styling:</p> <pre><code>// Layout utilities\n&lt;div className=\"grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-6\"&gt;\n    &lt;div className=\"col-span-full lg:col-span-2\"&gt;\n        {/* Main content */}\n    &lt;/div&gt;\n    &lt;div className=\"lg:col-span-1\"&gt;\n        {/* Sidebar */}\n    &lt;/div&gt;\n&lt;/div&gt;\n\n// Component styling\n&lt;Card className=\"p-6 shadow-lg border-zinc-200 dark:border-zinc-800\"&gt;\n    &lt;CardHeader className=\"pb-4\"&gt;\n        &lt;CardTitle className=\"text-lg font-semibold text-zinc-900 dark:text-zinc-100\"&gt;\n            Chart Title\n        &lt;/CardTitle&gt;\n    &lt;/CardHeader&gt;\n    &lt;CardContent&gt;\n        {/* Chart content */}\n    &lt;/CardContent&gt;\n&lt;/Card&gt;\n\n// Interactive states\n&lt;Button \n    variant=\"outline\" \n    className=\"hover:bg-zinc-100 dark:hover:bg-zinc-800 transition-colors\"\n    disabled={isLoading}\n&gt;\n    {isLoading ? &lt;Spinner className=\"mr-2\" /&gt; : null}\n    Load Data\n&lt;/Button&gt;\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#dark-mode-support","title":"Dark Mode Support","text":"<p>Dark mode is handled through CSS custom properties and Tailwind's dark variant:</p> <pre><code>// Theme provider context\nexport function ThemeProvider({ children, defaultTheme = \"system\" }) {\n    const [theme, setTheme] = useState&lt;Theme&gt;(defaultTheme);\n\n    useEffect(() =&gt; {\n        const root = window.document.documentElement;\n        root.classList.remove(\"light\", \"dark\");\n\n        if (theme === \"system\") {\n            const systemTheme = window.matchMedia(\"(prefers-color-scheme: dark)\").matches \n                ? \"dark\" : \"light\";\n            root.classList.add(systemTheme);\n        } else {\n            root.classList.add(theme);\n        }\n    }, [theme]);\n\n    return (\n        &lt;ThemeContext.Provider value={{ theme, setTheme }}&gt;\n            {children}\n        &lt;/ThemeContext.Provider&gt;\n    );\n}\n\n// Usage in components\nfunction Chart({ data }: ChartProps) {\n    const { theme } = useTheme();\n    const isDark = theme === 'dark' || \n        (theme === 'system' &amp;&amp; window.matchMedia('(prefers-color-scheme: dark)').matches);\n\n    return (\n        &lt;ScatterPlot \n            data={data}\n            darkMode={isDark}\n            // Colors adapt automatically through Tailwind dark: variants\n        /&gt;\n    );\n}\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#responsive-design","title":"Responsive Design","text":"<p>Components adapt to different screen sizes:</p> <pre><code>&lt;div className=\"space-y-6\"&gt;\n    {/* Mobile-first responsive grid */}\n    &lt;div className=\"grid grid-cols-1 lg:grid-cols-4 gap-4\"&gt;\n        &lt;div className=\"lg:col-span-3\"&gt;\n            {/* Chart takes full width on mobile, 3/4 on desktop */}\n            &lt;ScatterPlot data={data} /&gt;\n        &lt;/div&gt;\n        &lt;div className=\"lg:col-span-1\"&gt;\n            {/* Controls stack below chart on mobile, sidebar on desktop */}\n            &lt;div className=\"space-y-4\"&gt;\n                &lt;SearchBar onSubmit={handleSearch} /&gt;\n                &lt;FilterControls /&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;\n\n    {/* Table with horizontal scroll on mobile */}\n    &lt;div className=\"overflow-x-auto\"&gt;\n        &lt;DataTable \n            data={data}\n            className=\"min-w-[600px]\" \n        /&gt;\n    &lt;/div&gt;\n&lt;/div&gt;\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#performance-optimization","title":"Performance Optimization","text":""},{"location":"guides/contributing/dashboards/react/#memoization-and-optimization","title":"Memoization and Optimization","text":"<pre><code>// Memoize expensive calculations\nconst processedData = useMemo(() =&gt; {\n    if (!rawData) return [];\n\n    return rawData\n        .filter(item =&gt; item.value &gt; threshold)\n        .sort((a, b) =&gt; b.value - a.value)\n        .slice(0, maxItems);\n}, [rawData, threshold, maxItems]);\n\n// Memoize callback functions\nconst handleItemClick = useCallback((item: DataPoint) =&gt; {\n    setSelectedItem(item);\n    onItemSelect?.(item);\n}, [onItemSelect]);\n\n// Memoize complex components\nconst ChartComponent = memo(({ data, options }: ChartProps) =&gt; {\n    return &lt;ExpensiveChart data={data} options={options} /&gt;;\n});\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#virtual-scrolling","title":"Virtual Scrolling","text":"<p>For large datasets, implement virtual scrolling:</p> <pre><code>import { useVirtualizer } from '@tanstack/react-virtual';\n\nfunction VirtualTable({ data }: { data: Array&lt;any&gt; }) {\n    const containerRef = useRef&lt;HTMLDivElement&gt;(null);\n\n    const virtualizer = useVirtualizer({\n        count: data.length,\n        getScrollElement: () =&gt; containerRef.current,\n        estimateSize: () =&gt; 50, // Row height\n        overscan: 10 // Render extra items for smooth scrolling\n    });\n\n    return (\n        &lt;div ref={containerRef} className=\"h-96 overflow-auto\"&gt;\n            &lt;div style={{ height: virtualizer.getTotalSize() }}&gt;\n                {virtualizer.getVirtualItems().map((item) =&gt; (\n                    &lt;div\n                        key={item.key}\n                        style={{\n                            position: 'absolute',\n                            top: 0,\n                            left: 0,\n                            width: '100%',\n                            height: item.size,\n                            transform: `translateY(${item.start}px)`\n                        }}\n                    &gt;\n                        &lt;TableRow data={data[item.index]} /&gt;\n                    &lt;/div&gt;\n                ))}\n            &lt;/div&gt;\n        &lt;/div&gt;\n    );\n}\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#code-splitting","title":"Code Splitting","text":"<p>Split large components with lazy loading:</p> <pre><code>// Lazy load heavy visualization components\nconst AdvancedChart = lazy(() =&gt; import('@/components/charts/advanced-chart'));\nconst ComplexTable = lazy(() =&gt; import('@/components/tables/complex-table'));\n\nfunction Dashboard() {\n    return (\n        &lt;Suspense fallback={&lt;div&gt;Loading chart...&lt;/div&gt;}&gt;\n            &lt;AdvancedChart data={data} /&gt;\n        &lt;/Suspense&gt;\n    );\n}\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#testing-react-dashboards","title":"Testing React Dashboards","text":""},{"location":"guides/contributing/dashboards/react/#component-testing","title":"Component Testing","text":"<pre><code>// __tests__/components/SearchBar.test.tsx\nimport { render, screen, fireEvent, waitFor } from '@testing-library/react';\nimport userEvent from '@testing-library/user-event';\nimport SearchBar from '@/components/search';\n\ndescribe('SearchBar', () =&gt; {\n    const mockSubmit = jest.fn();\n    const mockClear = jest.fn();\n    const searchList = ['apple', 'banana', 'cherry', 'date'];\n\n    beforeEach(() =&gt; {\n        jest.clearAllMocks();\n    });\n\n    test('renders with placeholder', () =&gt; {\n        render(\n            &lt;SearchBar \n                searchList={searchList}\n                onSubmit={mockSubmit}\n                placeholder=\"Search fruits...\"\n            /&gt;\n        );\n\n        expect(screen.getByPlaceholderText('Search fruits...')).toBeInTheDocument();\n    });\n\n    test('shows suggestions when typing', async () =&gt; {\n        const user = userEvent.setup();\n\n        render(\n            &lt;SearchBar searchList={searchList} onSubmit={mockSubmit} /&gt;\n        );\n\n        const input = screen.getByRole('textbox');\n        await user.type(input, 'a');\n\n        await waitFor(() =&gt; {\n            expect(screen.getByText('apple')).toBeInTheDocument();\n            expect(screen.getByText('banana')).toBeInTheDocument();\n        });\n    });\n\n    test('calls onSubmit when form submitted', async () =&gt; {\n        const user = userEvent.setup();\n\n        render(\n            &lt;SearchBar searchList={searchList} onSubmit={mockSubmit} /&gt;\n        );\n\n        const input = screen.getByRole('textbox');\n        await user.type(input, 'apple');\n        await user.keyboard('{Enter}');\n\n        expect(mockSubmit).toHaveBeenCalledWith('apple');\n    });\n});\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#integration-testing","title":"Integration Testing","text":"<pre><code>// __tests__/integration/Dashboard.test.tsx\nimport { render, screen, waitFor } from '@testing-library/react';\nimport { rest } from 'msw';\nimport { setupServer } from 'msw/node';\nimport Dashboard from '@/components/dashboard';\n\n// Mock API server\nconst server = setupServer(\n    rest.get('/api/presenters/:id', (req, res, ctx) =&gt; {\n        return res(ctx.json({\n            id: 'test-presenter',\n            name: 'Test Presenter',\n            data: [\n                { x: 1, y: 2, label: 'Point 1' },\n                { x: 3, y: 4, label: 'Point 2' }\n            ]\n        }));\n    })\n);\n\nbeforeAll(() =&gt; server.listen());\nafterEach(() =&gt; server.resetHandlers());\nafterAll(() =&gt; server.close());\n\ntest('loads and displays data', async () =&gt; {\n    render(&lt;Dashboard presenterId=\"test-presenter\" /&gt;);\n\n    // Initially shows loading\n    expect(screen.getByText(/loading/i)).toBeInTheDocument();\n\n    // After API call, shows data\n    await waitFor(() =&gt; {\n        expect(screen.getByText('Test Presenter')).toBeInTheDocument();\n    });\n\n    // Chart renders with data points\n    expect(screen.getByText('Point 1')).toBeInTheDocument();\n    expect(screen.getByText('Point 2')).toBeInTheDocument();\n});\n\ntest('handles API errors gracefully', async () =&gt; {\n    server.use(\n        rest.get('/api/presenters/:id', (req, res, ctx) =&gt; {\n            return res(ctx.status(500), ctx.json({ error: 'Server error' }));\n        })\n    );\n\n    render(&lt;Dashboard presenterId=\"test-presenter\" /&gt;);\n\n    await waitFor(() =&gt; {\n        expect(screen.getByText(/error loading data/i)).toBeInTheDocument();\n    });\n});\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#e2e-testing-with-cypress","title":"E2E Testing with Cypress","text":"<pre><code>// cypress/integration/dashboard.spec.ts\ndescribe('Dashboard Interaction', () =&gt; {\n    beforeEach(() =&gt; {\n        cy.intercept('GET', '/api/presenters/ngram-analysis', { \n            fixture: 'ngram-data.json' \n        }).as('getNgramData');\n\n        cy.visit('/dashboard/ngram-analysis');\n        cy.wait('@getNgramData');\n    });\n\n    it('allows searching and filtering data', () =&gt; {\n        // Search for specific terms\n        cy.get('[data-testid=\"search-input\"]').type('climate');\n        cy.get('[data-testid=\"search-submit\"]').click();\n\n        // Verify results update\n        cy.get('[data-testid=\"chart-points\"]').should('have.length.lessThan', 100);\n        cy.get('[data-testid=\"data-table\"]').should('contain', 'climate');\n\n        // Clear search\n        cy.get('[data-testid=\"search-clear\"]').click();\n        cy.get('[data-testid=\"chart-points\"]').should('have.length.greaterThan', 100);\n    });\n\n    it('supports chart interactions', () =&gt; {\n        // Click on chart point\n        cy.get('[data-testid=\"chart-container\"]').click(300, 200);\n\n        // Verify tooltip appears\n        cy.get('[data-testid=\"tooltip\"]').should('be.visible');\n        cy.get('[data-testid=\"tooltip\"]').should('contain', 'Ranking:');\n\n        // Verify data table selection updates\n        cy.get('[data-testid=\"data-table\"] .selected-row').should('exist');\n    });\n\n    it('downloads data in different formats', () =&gt; {\n        // Open download menu\n        cy.get('[data-testid=\"download-button\"]').click();\n\n        // Download CSV\n        cy.get('[data-testid=\"download-csv\"]').click();\n        cy.readFile('cypress/downloads/data.csv').should('exist');\n\n        // Download JSON\n        cy.get('[data-testid=\"download-button\"]').click();\n        cy.get('[data-testid=\"download-json\"]').click();\n        cy.readFile('cypress/downloads/data.json').should('exist');\n    });\n});\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#deployment-and-build-process","title":"Deployment and Build Process","text":""},{"location":"guides/contributing/dashboards/react/#production-build","title":"Production Build","text":"<p>The React dashboard builds as static assets:</p> <pre><code># Build for production\nnpm run build\n\n# Outputs to app/web_templates/build/\n# - bundled/ (JS/CSS assets)\n# - manifest.json (asset mapping)\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#integration-with-backend","title":"Integration with Backend","text":"<p>The Python backend serves the React app:</p> <pre><code># Backend integration\nfrom pathlib import Path\nimport json\n\n# Load build manifest\nmanifest_path = Path(\"web_templates/build/manifest.json\")\nwith open(manifest_path) as f:\n    manifest = json.load(f)\n\n# Serve React app\n@app.route(\"/\")\ndef dashboard():\n    return render_template(\n        \"index.html\",\n        js_files=get_js_files(manifest),\n        css_files=get_css_files(manifest),\n        project_name=config.PROJECT_NAME\n    )\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#environment-configuration","title":"Environment Configuration","text":"<pre><code>// Environment variables for different deployments\nconst config = {\n    API_BASE_URL: process.env.REACT_APP_API_URL || 'http://localhost:8050',\n    ENABLE_DEV_TOOLS: process.env.NODE_ENV === 'development',\n    VERSION: process.env.REACT_APP_VERSION || '1.0.0'\n};\n\n// API client configuration\nconst apiClient = axios.create({\n    baseURL: config.API_BASE_URL,\n    timeout: 30000,\n    headers: {\n        'Content-Type': 'application/json'\n    }\n});\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#best-practices","title":"Best Practices","text":""},{"location":"guides/contributing/dashboards/react/#component-design","title":"Component Design","text":"<pre><code>// 1. Keep components focused and single-purpose\nfunction ChartControls({ onFilterChange, onExport }: ChartControlsProps): ReactElement&lt;FC&gt; {\n    // Only handle UI controls, delegate data management\n    return (\n        &lt;div className=\"flex gap-4\"&gt;\n            &lt;SearchBar onSubmit={(term) =&gt; onFilterChange({ search: term })} /&gt;\n            &lt;ExportButton onExport={onExport} /&gt;\n        &lt;/div&gt;\n    );\n}\n\n// 2. Use composition over inheritance\nfunction Dashboard({ children }: PropsWithChildren): ReactElement&lt;FC&gt; {\n    return (\n        &lt;div className=\"dashboard-layout\"&gt;\n            &lt;Sidebar /&gt;\n            &lt;main className=\"main-content\"&gt;\n                {children}\n            &lt;/main&gt;\n        &lt;/div&gt;\n    );\n}\n\n// Usage\n&lt;Dashboard&gt;\n    &lt;ChartContainer&gt;\n        &lt;ScatterPlot data={data} /&gt;\n        &lt;DataTable data={data} /&gt;\n    &lt;/ChartContainer&gt;\n&lt;/Dashboard&gt;\n\n// 3. Extract custom hooks for reusable logic\nfunction useChartData(presenterId: string, filters: Filters) {\n    const [data, setData] = useState(null);\n    const [isLoading, setIsLoading] = useState(false);\n\n    useEffect(() =&gt; {\n        // Data fetching logic\n    }, [presenterId, filters]);\n\n    return { data, isLoading, refetch: () =&gt; setData(null) };\n}\n</code></pre>"},{"location":"guides/contributing/dashboards/react/#complete-example-ngram-analysis-dashboard","title":"Complete Example: Ngram Analysis Dashboard","text":"<p>Here's a complete example showing all concepts together:</p> <pre><code>// components/ngram-dashboard.tsx\nimport { useState, useEffect, useMemo, useCallback } from 'react';\nimport { useTheme } from '@/components/theme-provider';\nimport { Tabs, TabsContent, TabsList, TabsTrigger } from '@/components/ui/tabs';\nimport { Card, CardContent, CardHeader, CardTitle } from '@/components/ui/card';\nimport { Info } from 'lucide-react';\nimport { Tooltip, TooltipContent, TooltipTrigger } from '@/components/ui/tooltip';\n\nimport ScatterPlot from '@/components/charts/scatter';\nimport DataTable from '@/components/data-table';\nimport SearchBar from '@/components/search';\nimport { DownloadButton } from '@/components/download';\nimport { usePresenterData } from '@/hooks/usePresenterData';\n\ninterface NgramDashboardProps {\n    presenter: NgramPresenter;\n}\n\nexport default function NgramDashboard({ presenter }: NgramDashboardProps) {\n    // State management\n    const [searchTerm, setSearchTerm] = useState('');\n    const [selectedNgram, setSelectedNgram] = useState('');\n    const [currentTab, setCurrentTab] = useState&lt;'total_repetition' | 'amplification_factor'&gt;('total_repetition');\n    const [selectedRows, setSelectedRows] = useState&lt;CompactSelection&gt;(CompactSelection.empty());\n\n    // Theme\n    const { theme } = useTheme();\n    const isDark = theme === 'dark' || \n        (theme === 'system' &amp;&amp; window.matchMedia('(prefers-color-scheme: dark)').matches);\n\n    // Data fetching\n    const { data: summaryData, isLoading } = usePresenterData(presenter.id, {\n        output: 'summary',\n        matcher: searchTerm\n    });\n\n    const { data: detailData } = usePresenterData(presenter.id, {\n        output: 'details',\n        filter_field: 'ngram',\n        filter_value: selectedNgram\n    }, { enabled: !!selectedNgram });\n\n    // Computed values\n    const currentData = useMemo(() =&gt; {\n        if (!summaryData) return [];\n\n        return summaryData.map((item, index) =&gt; ({\n            ...item,\n            ranking: index + 1,\n            y: currentTab === 'total_repetition' \n                ? item.total_repetition \n                : item.amplification_factor\n        }));\n    }, [summaryData, currentTab]);\n\n    const tableColumns = useMemo(() =&gt; {\n        if (selectedNgram &amp;&amp; detailData) {\n            return [\n                { id: 'ngram', title: 'N-gram', width: 200 },\n                { id: 'user', title: 'User', width: 150 },\n                { id: 'userReps', title: 'User Reps', width: 100 },\n                { id: 'message', title: 'Message', width: 400 },\n                { id: 'timestamp', title: 'Timestamp', width: 200 }\n            ];\n        }\n\n        return [\n            { id: 'ranking', title: 'Rank', width: 80 },\n            { id: 'ngram', title: 'N-gram', width: 300 },\n            { id: 'x', title: 'User Count', width: 120 },\n            { \n                id: 'y', \n                title: currentTab === 'total_repetition' ? 'Total Reps' : 'Amplification',\n                width: 120 \n            }\n        ];\n    }, [selectedNgram, detailData, currentTab]);\n\n    // Event handlers\n    const handleSearch = useCallback((term: string) =&gt; {\n        setSearchTerm(term);\n        setSelectedNgram(''); // Clear selection when searching\n    }, []);\n\n    const handleSearchClear = useCallback(() =&gt; {\n        setSearchTerm('');\n        setSelectedNgram('');\n    }, []);\n\n    const handleChartClick = useCallback((info: PickingInfo&lt;DataPoint&gt;) =&gt; {\n        if (info.object) {\n            setSelectedNgram(info.object.ngram);\n        }\n    }, []);\n\n    const handleTableSelect = useCallback((item: DataPoint | null, selection?: GridSelection) =&gt; {\n        if (item) {\n            setSelectedNgram(item.ngram);\n        }\n        if (selection) {\n            setSelectedRows(selection.rows);\n        }\n    }, []);\n\n    const handleTabChange = useCallback((tab: string) =&gt; {\n        setCurrentTab(tab as typeof currentTab);\n    }, []);\n\n    // Tooltip formatters\n    const createTooltipFormatter = useCallback((type: string) =&gt; (params: DataPoint) =&gt; `\n        &lt;div class=\"space-y-2\"&gt;\n            &lt;div class=\"font-bold\"&gt;${params.ngram}&lt;/div&gt;\n            &lt;div&gt;Ranking: ${params.ranking}&lt;/div&gt;\n            &lt;div&gt;User Count: ${params.x}&lt;/div&gt;\n            &lt;div&gt;${type === 'total_repetition' ? 'Total Reps' : 'Amplification'}: ${params.y}&lt;/div&gt;\n        &lt;/div&gt;\n    `, []);\n\n    // Loading state\n    if (isLoading) {\n        return (\n            &lt;Card&gt;\n                &lt;CardContent className=\"flex items-center justify-center h-96\"&gt;\n                    &lt;div className=\"text-center\"&gt;\n                        &lt;div className=\"animate-spin rounded-full h-12 w-12 border-b-2 border-blue-600 mx-auto mb-4\"&gt;&lt;/div&gt;\n                        &lt;p&gt;Loading dashboard...&lt;/p&gt;\n                    &lt;/div&gt;\n                &lt;/CardContent&gt;\n            &lt;/Card&gt;\n        );\n    }\n\n    return (\n        &lt;Card&gt;\n            &lt;CardContent className=\"space-y-6\"&gt;\n                &lt;Tabs value={currentTab} onValueChange={handleTabChange}&gt;\n                    &lt;div className=\"flex items-center justify-between\"&gt;\n                        &lt;TabsList&gt;\n                            &lt;TabsTrigger value=\"total_repetition\"&gt;Total Repetition&lt;/TabsTrigger&gt;\n                            &lt;TabsTrigger value=\"amplification_factor\"&gt;Amplification Factor&lt;/TabsTrigger&gt;\n                        &lt;/TabsList&gt;\n\n                        &lt;Tooltip&gt;\n                            &lt;TooltipTrigger&gt;\n                                &lt;Info className=\"h-5 w-5 text-zinc-500\" /&gt;\n                            &lt;/TooltipTrigger&gt;\n                            &lt;TooltipContent&gt;\n                                &lt;p className=\"max-w-xs\"&gt;\n                                    {presenter.explanation[currentTab]}\n                                &lt;/p&gt;\n                            &lt;/TooltipContent&gt;\n                        &lt;/Tooltip&gt;\n                    &lt;/div&gt;\n\n                    &lt;TabsContent value=\"total_repetition\" className=\"space-y-6\"&gt;\n                        &lt;div className=\"flex items-center justify-between\"&gt;\n                            &lt;SearchBar\n                                searchList={presenter.ngrams}\n                                onSubmit={handleSearch}\n                                onClear={handleSearchClear}\n                                placeholder=\"Search n-grams...\"\n                            /&gt;\n                            &lt;DownloadButton \n                                presenterID={presenter.id}\n                                queryParams={{ \n                                    output: 'summary',\n                                    matcher: searchTerm || undefined \n                                }}\n                            /&gt;\n                        &lt;/div&gt;\n\n                        &lt;ScatterPlot\n                            data={currentData}\n                            darkMode={isDark}\n                            onClick={handleChartClick}\n                            tooltip={createTooltipFormatter('total_repetition')}\n                            axis={{\n                                x: { type: 'log', show: true },\n                                y: { type: 'log', show: true }\n                            }}\n                        /&gt;\n\n                        &lt;DataTable\n                            data={selectedNgram &amp;&amp; detailData ? detailData : currentData}\n                            columns={tableColumns}\n                            onRowSelect={handleTableSelect}\n                            selectedRows={selectedRows}\n                            darkMode={isDark}\n                        /&gt;\n                    &lt;/TabsContent&gt;\n\n                    &lt;TabsContent value=\"amplification_factor\" className=\"space-y-6\"&gt;\n                        &lt;div className=\"flex items-center justify-between\"&gt;\n                            &lt;SearchBar\n                                searchList={presenter.ngrams}\n                                onSubmit={handleSearch}\n                                onClear={handleSearchClear}\n                                placeholder=\"Search n-grams...\"\n                            /&gt;\n                            &lt;DownloadButton \n                                presenterID={presenter.id}\n                                queryParams={{ \n                                    output: 'summary',\n                                    matcher: searchTerm || undefined \n                                }}\n                            /&gt;\n                        &lt;/div&gt;\n\n                        &lt;ScatterPlot\n                            data={currentData}\n                            darkMode={isDark}\n                            onClick={handleChartClick}\n                            tooltip={createTooltipFormatter('amplification_factor')}\n                            axis={{\n                                x: { type: 'log', show: true },\n                                y: { type: 'log', show: true }\n                            }}\n                        /&gt;\n\n                        &lt;DataTable\n                            data={selectedNgram &amp;&amp; detailData ? detailData : currentData}\n                            columns={tableColumns}\n                            onRowSelect={handleTableSelect}\n                            selectedRows={selectedRows}\n                            darkMode={isDark}\n                        /&gt;\n                    &lt;/TabsContent&gt;\n                &lt;/Tabs&gt;\n            &lt;/CardContent&gt;\n        &lt;/Card&gt;\n    );\n}\n</code></pre> <p>This comprehensive guide covers all aspects of building React dashboards for the analyzer platform. The combination of TypeScript, modern React patterns, rich UI components, and seamless API integration creates powerful, user-friendly data analysis interfaces that complement the Python-based analyzer pipeline.</p>"},{"location":"guides/contributing/dashboards/react/#next-steps","title":"Next Steps","text":"<p>After this section it would be a good idea to review the sections that discuss implementing  Shiny dashboards. Although once you finish reading this it would also be a good idea to review the sections for each domain.</p> <ul> <li>Core Domain</li> <li>Edge Domain</li> <li>Content Domain</li> <li>Shiny Dashboards</li> </ul>"},{"location":"guides/contributing/dashboards/shiny/","title":"Shiny","text":""},{"location":"guides/contributing/dashboards/shiny/#shiny-dashboards-guide","title":"Shiny Dashboards Guide","text":"<p>Web presenters can create interactive dashboards using Python Shiny for rich server-side interactivity. Shiny dashboards provide immediate reactivity, complex data processing capabilities, and seamless integration with the analyzer pipeline. Which in turn allows developers and data scientists the ability to quickly prototype new analyses.</p>"},{"location":"guides/contributing/dashboards/shiny/#overview","title":"Overview","text":"<p>Shiny dashboards are server-rendered applications that provide:</p> <ul> <li>Real-time interactivity: Components update automatically when inputs change</li> <li>Server-side processing: Complex calculations run on the server with full Python ecosystem access</li> <li>Widgets: Built-in components for inputs, outputs, and visualizations</li> <li>Session management: Automatic handling of user sessions and state</li> </ul>"},{"location":"guides/contributing/dashboards/shiny/#basic-structure","title":"Basic Structure","text":"<p>Every Shiny web presenter follows this pattern:</p> <pre><code>from shiny import reactive, render, ui\nfrom shinywidgets import output_widget, render_widget\nfrom analyzer_interface.context import WebPresenterContext, FactoryOutputContext, ShinyContext\nimport polars as pl\nimport plotly.express as px\n\ndef factory(context: WebPresenterContext) -&gt; FactoryOutputContext:\n    # Load analyzer data\n    df = pl.read_parquet(context.base.table(\"your_output\").parquet_path)\n\n    # Define UI layout\n    dashboard_ui = ui.card(\n        ui.card_header(\"Your Dashboard Title\"),\n        ui.row(\n            ui.column(4, \n                # Input controls\n                ui.input_selectize(\"category\", \"Select Category\", \n                                 choices=df[\"category\"].unique().to_list()),\n                ui.input_slider(\"threshold\", \"Threshold\", 0, 100, 50)\n            ),\n            ui.column(8,\n                # Output displays\n                output_widget(\"main_plot\", height=\"400px\"),\n                ui.output_text(\"summary_stats\")\n            )\n        )\n    )\n\n    def server(input, output, session):\n        @reactive.Calc\n        def filtered_data():\n            # Reactive data filtering\n            return df.filter(\n                (pl.col(\"category\") == input.category()) &amp;\n                (pl.col(\"value\") &gt;= input.threshold())\n            )\n\n        @render_widget\n        def main_plot():\n            # Create interactive plot\n            plot_df = filtered_data().to_pandas()\n            fig = px.scatter(plot_df, x=\"x\", y=\"y\", color=\"category\")\n            return fig\n\n        @render.text\n        def summary_stats():\n            data = filtered_data()\n            return f\"Showing {len(data)} items, avg value: {data['value'].mean():.2f}\"\n\n    return FactoryOutputContext(\n        shiny=ShinyContext(\n            server_handler=server,\n            panel=nav_panel(\"Dashboard\", dashboard_ui)\n        )\n    )\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#user-interface-components","title":"User Interface Components","text":""},{"location":"guides/contributing/dashboards/shiny/#layout-components","title":"Layout Components","text":"<p>Organize your dashboard with these layout elements:</p> <pre><code># Cards for grouped content\nui.card(\n    ui.card_header(\"Section Title\"),\n    ui.card_body(\"Content goes here\")\n)\n\n# Grid layouts\nui.row(\n    ui.column(6, \"Left column\"),\n    ui.column(6, \"Right column\")\n)\n\n# Navigation\nui.navset_tab(\n    ui.nav_panel(\"Tab 1\", \"Content 1\"),\n    ui.nav_panel(\"Tab 2\", \"Content 2\")\n)\n\n# Sidebars\nui.sidebar(\n    \"Sidebar content\",\n    open=\"open\"  # or \"closed\"\n)\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#input-controls","title":"Input Controls","text":"<p>Collect user input with various widgets:</p> <pre><code># Text inputs\nui.input_text(\"text_id\", \"Label\", value=\"default\")\nui.input_text_area(\"textarea_id\", \"Description\", rows=3)\n\n# Numeric inputs\nui.input_numeric(\"number_id\", \"Number\", value=10, min=0, max=100)\nui.input_slider(\"slider_id\", \"Range\", 0, 100, value=[20, 80])\n\n# Selection inputs\nui.input_select(\"select_id\", \"Choose one\", choices=[\"A\", \"B\", \"C\"])\nui.input_selectize(\"selectize_id\", \"Type to search\", \n                   choices=data[\"column\"].unique().to_list(),\n                   multiple=True)\n\n# Boolean inputs\nui.input_checkbox(\"check_id\", \"Enable feature\", value=True)\nui.input_switch(\"switch_id\", \"Toggle mode\")\n\n# File uploads\nui.input_file(\"file_id\", \"Upload CSV\", accept=\".csv\")\n\n# Date/time inputs\nui.input_date(\"date_id\", \"Select date\")\nui.input_date_range(\"daterange_id\", \"Date range\")\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#output-components","title":"Output Components","text":"<p>Display results with these output components:</p> <pre><code># Text outputs\nui.output_text(\"text_id\")        # Plain text\nui.output_text_verbatim(\"code_id\")  # Monospace text\nui.output_ui(\"dynamic_ui\")       # Dynamic UI elements\n\n# Tables\nui.output_table(\"table_id\")      # Basic table\nui.output_data_frame(\"df_id\")    # Interactive data frame\n\n# Plots\noutput_widget(\"plot_id\")         # For plotly/bokeh widgets\nui.output_plot(\"matplotlib_id\")  # For matplotlib plots\n\n# Downloads\nui.download_button(\"download_id\", \"Download Data\")\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#reactive-programming","title":"Reactive Programming","text":"<p>Shiny's reactive system automatically updates outputs when inputs change:</p>"},{"location":"guides/contributing/dashboards/shiny/#reactive-calculations","title":"Reactive Calculations","text":"<p>Use <code>@reactive.Calc</code> for expensive computations that multiple outputs depend on:</p> <pre><code>@reactive.Calc\ndef processed_data():\n    # This only runs when dependencies change\n    raw_data = load_data()\n    return raw_data.filter(pl.col(\"active\") == input.show_active())\n\n@render_widget\ndef plot1():\n    data = processed_data()  # Uses cached result\n    return create_plot(data)\n\n@render.text  \ndef summary():\n    data = processed_data()  # Uses same cached result\n    return f\"Records: {len(data)}\"\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#reactive-effects","title":"Reactive Effects","text":"<p>Use <code>@reactive.Effect</code> for side effects like updating other inputs:</p> <pre><code>@reactive.Effect\ndef update_choices():\n    # Update selectize choices when category changes\n    category = input.category()\n    new_choices = df.filter(pl.col(\"category\") == category)[\"subcategory\"].unique()\n    ui.update_selectize(\"subcategory\", choices=new_choices.to_list())\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#event-handling","title":"Event Handling","text":"<p>Respond to button clicks and other events:</p> <pre><code>@reactive.Effect\n@reactive.event(input.reset_button)\ndef reset_filters():\n    ui.update_slider(\"threshold\", value=50)\n    ui.update_select(\"category\", selected=\"All\")\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#data-visualization","title":"Data Visualization","text":""},{"location":"guides/contributing/dashboards/shiny/#plotly-integration","title":"Plotly Integration","text":"<p>Create interactive plots with plotly:</p> <pre><code>from shinywidgets import output_widget, render_widget\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n@render_widget\ndef scatter_plot():\n    df_plot = filtered_data().to_pandas()\n\n    fig = px.scatter(\n        df_plot, \n        x=\"x_value\", \n        y=\"y_value\",\n        color=\"category\",\n        size=\"size_value\",\n        hover_data=[\"additional_info\"],\n        title=\"Interactive Scatter Plot\"\n    )\n\n    # Customize layout\n    fig.update_layout(\n        height=500,\n        showlegend=True,\n        hovermode=\"closest\"\n    )\n\n    return fig\n\n@render_widget  \ndef time_series():\n    df_ts = time_series_data().to_pandas()\n\n    fig = go.Figure()\n\n    for category in df_ts[\"category\"].unique():\n        category_data = df_ts[df_ts[\"category\"] == category]\n        fig.add_trace(go.Scatter(\n            x=category_data[\"date\"],\n            y=category_data[\"value\"],\n            name=category,\n            mode=\"lines+markers\"\n        ))\n\n    fig.update_layout(\n        title=\"Time Series Analysis\",\n        xaxis_title=\"Date\",\n        yaxis_title=\"Value\"\n    )\n\n    return fig\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#custom-plots","title":"Custom Plots","text":"<p>Create custom visualizations with matplotlib or other libraries:</p> <pre><code>from shiny import render\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n@render.plot\ndef correlation_heatmap():\n    df_corr = correlation_data().to_pandas()\n\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(\n        df_corr.corr(),\n        annot=True,\n        cmap=\"coolwarm\",\n        center=0,\n        square=True\n    )\n    plt.title(\"Correlation Matrix\")\n    plt.tight_layout()\n    return plt.gcf()\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#data-tables","title":"Data Tables","text":"<p>Display and interact with tabular data:</p>"},{"location":"guides/contributing/dashboards/shiny/#basic-tables","title":"Basic Tables","text":"<pre><code>@render.table\ndef simple_table():\n    return filtered_data().to_pandas()\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#interactive-data-frames","title":"Interactive Data Frames","text":"<pre><code>from shiny.render import DataGrid, DataTable\n\n@render.data_frame\ndef interactive_grid():\n    df_display = filtered_data().to_pandas()\n\n    return DataGrid(\n        df_display,\n        selection_mode=\"rows\",  # or \"none\", \"row\", \"rows\", \"col\", \"cols\"\n        filters=True,\n        width=\"100%\",\n        height=\"400px\"\n    )\n\n# Access selected rows\n@reactive.Effect\ndef handle_selection():\n    selected = interactive_grid.data_view(selected=True)\n    if len(selected) &gt; 0:\n        # Process selected data\n        pass\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#custom-table-styling","title":"Custom Table Styling","text":"<pre><code>@render.table\ndef styled_table():\n    df = summary_stats().to_pandas()\n\n    # Format numeric columns\n    df[\"percentage\"] = df[\"percentage\"].map(\"{:.1%}\".format)\n    df[\"amount\"] = df[\"amount\"].map(\"${:,.0f}\".format)\n\n    return df\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#advanced-features","title":"Advanced Features","text":""},{"location":"guides/contributing/dashboards/shiny/#dynamic-ui","title":"Dynamic UI","text":"<p>Create UI elements that change based on user input:</p> <pre><code>@render.ui\ndef dynamic_controls():\n    analysis_type = input.analysis_type()\n\n    if analysis_type == \"correlation\":\n        return ui.div(\n            ui.input_selectize(\"x_var\", \"X Variable\", choices=numeric_columns),\n            ui.input_selectize(\"y_var\", \"Y Variable\", choices=numeric_columns)\n        )\n\n    if analysis_type == \"distribution\":\n        return ui.div(\n            ui.input_select(\"dist_var\", \"Variable\", choices=all_columns),\n            ui.input_numeric(\"bins\", \"Number of bins\", value=30)\n        )\n\n    eturn ui.div(\"Select an analysis type\")\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#progress-indicators","title":"Progress Indicators","text":"<p>Show progress for long-running operations:</p> <pre><code>from shiny import ui\n\n@reactive.Effect\n@reactive.event(input.run_analysis)\ndef run_long_analysis():\n    with ui.Progress(min=0, max=100) as progress:\n        progress.set(message=\"Loading data\", value=0)\n        data = load_large_dataset()\n\n        progress.set(message=\"Processing\", value=50)\n        results = process_data(data)\n\n        progress.set(message=\"Finalizing\", value=90)\n        save_results(results)\n\n        progress.set(value=100)\n\n    ui.notification_show(\"Analysis complete!\", type=\"success\")\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#integration-with-analyzers","title":"Integration with Analyzers","text":""},{"location":"guides/contributing/dashboards/shiny/#accessing-analyzer-data","title":"Accessing Analyzer Data","text":"<pre><code>def factory(context: WebPresenterContext) -&gt; FactoryOutputContext:\n    # Access primary analyzer outputs\n    main_data = pl.read_parquet(\n        context.base.table(\"main_analysis\").parquet_path\n    )\n\n    # Access secondary analyzer outputs\n    summary_data = pl.read_parquet(\n        context.dependency(summary_analyzer).table(\"summary\").parquet_path\n    )\n\n    # Access parameters used in analysis\n    threshold = context.base_params.get(\"threshold\", 0.5)\n\n    # Build dashboard with this data\n    # ...\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#parameter-integration","title":"Parameter Integration","text":"<p>Use analyzer parameters in your dashboard:</p> <pre><code>def server(input, output, session):\n    # Get analyzer parameters\n    analyzer_threshold = context.base_params.get(\"threshold\", 0.5)\n\n    @render.text\n    def analysis_info():\n        return f\"Analysis run with threshold: {analyzer_threshold}\"\n\n    @render_widget\n    def threshold_comparison():\n        # Compare user input with analyzer parameter\n        user_threshold = input.user_threshold()\n        df_comparison = main_data.with_columns([\n            (pl.col(\"value\") &gt; analyzer_threshold).alias(\"analyzer_flag\"),\n            (pl.col(\"value\") &gt; user_threshold).alias(\"user_flag\")\n        ])\n\n        return create_comparison_plot(df_comparison)\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#performance-optimization","title":"Performance Optimization","text":""},{"location":"guides/contributing/dashboards/shiny/#efficient-data-processing","title":"Efficient Data Processing","text":"<pre><code>@reactive.Calc\ndef base_data():\n    # Load once and cache\n    return pl.read_parquet(data_path)\n\n@reactive.Calc  \ndef filtered_data():\n    # Efficient filtering with Polars\n    filters = []\n\n    if input.category() != \"All\":\n        filters.append(pl.col(\"category\") == input.category())\n\n    if input.date_range() is not None:\n        start, end = input.date_range()\n        filters.append(pl.col(\"date\").is_between(start, end))\n\n    if filters:\n        return base_data().filter(pl.all_horizontal(filters))\n\n    else:\n        return base_data()\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#lazy-evaluation","title":"Lazy Evaluation","text":"<pre><code>@reactive.Calc\ndef expensive_calculation():\n    # Only runs when dependencies change\n    data = filtered_data()\n\n    # Use lazy evaluation\n    result = (\n        data\n        .group_by(\"category\")\n        .agg([\n            pl.col(\"value\").mean().alias(\"avg_value\"),\n            pl.col(\"value\").std().alias(\"std_value\"),\n            pl.col(\"value\").count().alias(\"count\")\n        ])\n        .sort(\"avg_value\", descending=True)\n    )\n\n    return result\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#testing-shiny-dashboards","title":"Testing Shiny Dashboards","text":""},{"location":"guides/contributing/dashboards/shiny/#unit-testing-components","title":"Unit Testing Components","text":"<pre><code>import pytest\nfrom shiny.testing import ShinyAppProc\nfrom your_presenter import factory\n\ndef test_dashboard_loads():\n    \"\"\"Test that dashboard loads without errors\"\"\"\n    app = factory(mock_context)\n\n    # Test UI renders\n    assert app.shiny.panel is not None\n\n    # Test server function exists\n    assert callable(app.shiny.server_handler)\n\ndef test_data_filtering():\n    \"\"\"Test reactive data filtering\"\"\"\n    with ShinyAppProc(factory(mock_context)) as proc:\n        # Set input values\n        proc.set_inputs(category=\"TypeA\", threshold=50)\n\n        # Check outputs update correctly\n        output = proc.get_output(\"summary_stats\")\n        assert \"TypeA\" in output\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#integration-testing","title":"Integration Testing","text":"<pre><code>def test_with_real_data():\n    \"\"\"Test dashboard with actual analyzer output\"\"\"\n    # Run analyzer to generate test data\n    context = create_test_context(test_data_path)\n\n    # Test dashboard with real data\n    app = factory(context)\n\n    # Verify data loads correctly\n    assert app.shiny.panel is not None\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#deployment-considerations","title":"Deployment Considerations","text":""},{"location":"guides/contributing/dashboards/shiny/#resource-management","title":"Resource Management","text":"<ul> <li>Use <code>@reactive.Calc</code> for expensive operations to enable caching</li> <li>Implement pagination for large datasets</li> <li>Consider data sampling for very large visualizations</li> <li>Use lazy loading for secondary data</li> </ul>"},{"location":"guides/contributing/dashboards/shiny/#error-handling","title":"Error Handling","text":"<pre><code>@render_widget\ndef safe_plot():\n    try:\n        data = filtered_data()\n        if len(data) == 0:\n            return empty_plot_message()\n\n        return create_plot(data)\n\n    except Exception as e:\n        ui.notification_show(f\"Plot error: {str(e)}\", type=\"error\")\n        return error_plot()\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#session-management","title":"Session Management","text":"<pre><code>def server(input, output, session):\n    # Clean up resources when session ends\n    @reactive.Effect\n    def cleanup():\n        session.on_ended(lambda: cleanup_user_data())\n</code></pre>"},{"location":"guides/contributing/dashboards/shiny/#example-complete-dashboard","title":"Example: Complete Dashboard","text":"<p>Here's a complete example of a Shiny dashboard for analyzing message sentiment:</p> <pre><code>from shiny import reactive, render, ui\nfrom shinywidgets import output_widget, render_widget\nimport plotly.express as px\nimport polars as pl\n\ndef factory(context: WebPresenterContext) -&gt; FactoryOutputContext:\n    # Load data\n    df_sentiment = pl.read_parquet(\n        context.base.table(\"sentiment_analysis\").parquet_path\n    )\n\n    # Get unique values for inputs\n    date_range = (df_sentiment[\"date\"].min(), df_sentiment[\"date\"].max())\n    categories = [\"All\"] + df_sentiment[\"category\"].unique().to_list()\n\n    # UI Layout\n    dashboard = ui.page_sidebar(\n        ui.sidebar(\n            ui.h3(\"Analysis Controls\"),\n            ui.input_date_range(\n                \"date_filter\", \n                \"Date Range\",\n                start=date_range[0],\n                end=date_range[1]\n            ),\n            ui.input_selectize(\n                \"category_filter\",\n                \"Categories\", \n                choices=categories,\n                selected=\"All\",\n                multiple=True\n            ),\n            ui.input_slider(\n                \"sentiment_threshold\",\n                \"Sentiment Threshold\",\n                -1, 1, 0, step=0.1\n            ),\n            ui.hr(),\n            ui.input_action_button(\"reset\", \"Reset Filters\"),\n            ui.download_button(\"download\", \"Download Data\")\n        ),\n\n        ui.div(\n            ui.h2(\"Sentiment Analysis Dashboard\"),\n\n            ui.row(\n                ui.column(6, ui.value_box(\n                    title=\"Total Messages\",\n                    value=ui.output_text(\"total_count\"),\n                    theme=\"primary\"\n                )),\n                ui.column(6, ui.value_box(\n                    title=\"Avg Sentiment\", \n                    value=ui.output_text(\"avg_sentiment\"),\n                    theme=\"success\"\n                ))\n            ),\n\n            ui.navset_tab(\n                ui.nav_panel(\n                    \"Time Series\",\n                    output_widget(\"timeseries_plot\", height=\"500px\")\n                ),\n                ui.nav_panel(\n                    \"Distribution\", \n                    output_widget(\"distribution_plot\", height=\"500px\")\n                ),\n                ui.nav_panel(\n                    \"Data Table\",\n                    ui.output_data_frame(\"data_table\")\n                )\n            )\n        )\n    )\n\n    def server(input, output, session):\n        @reactive.Calc\n        def filtered_data():\n            data = df_sentiment\n\n            # Date filtering\n            if input.date_filter() is not None:\n                start, end = input.date_filter()\n                data = data.filter(pl.col(\"date\").is_between(start, end))\n\n            # Category filtering\n            if \"All\" not in input.category_filter():\n                data = data.filter(pl.col(\"category\").is_in(input.category_filter()))\n\n            # Sentiment filtering\n            data = data.filter(pl.col(\"sentiment\") &gt;= input.sentiment_threshold())\n\n            return data\n\n        @render.text\n        def total_count():\n            return f\"{len(filtered_data()):,}\"\n\n        @render.text\n        def avg_sentiment():\n            avg = filtered_data()[\"sentiment\"].mean()\n            return f\"{avg:.3f}\"\n\n        @render_widget\n        def timeseries_plot():\n            df_plot = (\n                filtered_data()\n                .group_by(\"date\")\n                .agg(pl.col(\"sentiment\").mean().alias(\"avg_sentiment\"))\n                .sort(\"date\")\n                .to_pandas()\n            )\n\n            fig = px.line(\n                df_plot, \n                x=\"date\", \n                y=\"avg_sentiment\",\n                title=\"Sentiment Over Time\"\n            )\n            fig.add_hline(y=0, line_dash=\"dash\", line_color=\"gray\")\n            return fig\n\n        @render_widget\n        def distribution_plot():\n            df_plot = filtered_data().to_pandas()\n\n            fig = px.histogram(\n                df_plot,\n                x=\"sentiment\", \n                color=\"category\",\n                title=\"Sentiment Distribution\",\n                nbins=50\n            )\n            return fig\n\n        @render.data_frame\n        def data_table():\n            return filtered_data().to_pandas()\n\n        @reactive.Effect\n        @reactive.event(input.reset)\n        def reset_filters():\n            ui.update_date_range(\"date_filter\", start=date_range[0], end=date_range[1])\n            ui.update_selectize(\"category_filter\", selected=\"All\")\n            ui.update_slider(\"sentiment_threshold\", value=0)\n\n        @render.download(filename=\"sentiment_data.csv\")\n        def download():\n            return filtered_data().write_csv()\n\n    return FactoryOutputContext(\n        shiny=ShinyContext(\n            server_handler=server,\n            panel=nav_panel(\"Sentiment Analysis\", dashboard)\n        )\n    )\n</code></pre> <p>This comprehensive guide covers all aspects of building Shiny dashboards for your analyzer platform. The reactive programming model, rich widget ecosystem, and seamless Python integration make Shiny an excellent choice for creating sophisticated data analysis interfaces.</p>"},{"location":"guides/contributing/dashboards/shiny/#next-steps","title":"Next Steps","text":"<p>Once you finish reading section be a good idea to review the section that discuss implementing React dashboards. Might also be a good idea to review the sections for each domain. </p> <ul> <li>Core Domain</li> <li>Edge Domain</li> <li>Content Domain</li> <li>React Dashboards</li> </ul>"},{"location":"guides/design-philosophy/architecture/","title":"Architecture","text":"<p>Before contributing please refer to our Contributor Workflow</p>"},{"location":"guides/design-philosophy/architecture/#application-design-overview","title":"Application Design Overview","text":"<p>The CIB \ud83e\udd6d application is a terminal-based tool for performing data analysis and visualization. It is designed to be modular and extensible, allowing developers to contribute new analysis modules and visualization components while providing a consistent user experience around data import, preprocessing, and output generation.</p> <p>This design is motivated by a common pain point when moving from a data analysis script for private use to a tool that can be shared with others: A script for private consumption carries assumptions about the desired input and output data format and structure that are convenient to its author. When such a script is made available to others, debates on these aspects often arise. For a suite of analyses that this project aims to offer, if left decentralized, this debate can lead to inconsistent UX offerings across analyses, code duplication, and even bugs.</p> <p>The architecture of the CIB \ud83e\udd6d application is designed to address this problem by providing a clear separation between the core application logic and the analysis modules, such that the analysis module does not need to be concerned with the input and output data format and structure; such responsibilities are handled by the core application, where we aim to provide a rich, consistent, and intuitive user experience.</p>"},{"location":"guides/design-philosophy/architecture/#architecture-overview","title":"Architecture Overview","text":"<p>The application has three \"domains\": - The Core domain is responsible for workspace management, user flow, and integration of analysis runs and data import/export in a generic sense. It has three parts that correspond loosely to the MVC paradigm.   - The Application defines the workspace logic and exposes generic capabilities for importing and exporting data as well as analyses and dashboards. This is the \"controller\" part.   - The Terminal Components render the terminal interface and handle user input. This is the \"view\" part.   - The Storage IO persists the workspace data and is responsible for reading and writing data. This is the \"model\" part.</p> <p>The core application provides the context necessary for the other domains to function in a way that allows them to be agnostic about the specifics of the workspace and user flow.</p> <ul> <li>The Edge domain is responsible for data import and export while being agnostic about the specific analysis being run. Currently, this consists of the Importers and the Semantic Preprocessor.</li> </ul> <p>Note that the Storage IO is currently responsible for data export, but we should consider moving this to the Edge domain to allow for more extensibility and looser coupling.</p> <ul> <li>The Content domain is responsible for the actual data analysis and visualization and is agnostic about data import/export or workspace specifics. This consists of the Analyzers (both Primary and Secondary) as well as the Web Presenters.</li> </ul> <pre><code>flowchart TD\n    terminal[\"Terminal (core)\"]\n    application[\"Application (core)\"]\n    storage[\"Storage (core)\"]\n\n    importers[\"Importers (edge)\"]\n    semantic[\"Semantic Preprocessor (edge)\"]\n\n    content[\"Analyzers/Web Presenters (content)\"]\n\n    terminal --&gt; application\n    application --&gt; storage\n\n    application --&gt; importers\n    application --&gt; semantic\n\n    application --&gt; content\n</code></pre>"},{"location":"guides/design-philosophy/architecture/#questions-comments-and-feedback","title":"Questions, Comments, and Feedback","text":"<p>Talk to us on the Civic Tech DC Slack workspace!</p>"},{"location":"guides/design-philosophy/architecture/#next-steps","title":"Next Steps","text":"<p>It would be recommended to review the sections for each domain, and the section for implementing analyzers. Might also be a good idea to review the sections that discuss implementing  Shiny, and React dashboards.</p> <ul> <li>Core Domain</li> <li>Edge Domain</li> <li>Content Domain</li> <li>Implementing Analyzers</li> <li>Shiny Dashboards</li> <li>React Dashboards</li> </ul>"},{"location":"guides/design-philosophy/content-domain/","title":"Content Domain","text":""},{"location":"guides/design-philosophy/content-domain/#content-domain","title":"Content Domain","text":"<p>The Content domain is where the analysis and visualization happen.</p> <p>An analysis is added to the application by defining a Primary Analyzer, which comes with an interface declaration and an implementation. The interface declaration defines the input data structure and the output tables, which the application depends on for user guidance. The implementation is made workspace-agnostic by means of the \"context\" object.</p> <p>The goal of the Primary Analyzer is to produce a set of output tables that can be used by other analyzers, including Secondary Analyzers and Web Presenters. Primary Analyzer outputs are ideally normalized, non-duplicated, and non-redundant. As such, they are not always suitable for direct user consumption. It is the job of the Secondary Analyzers to produce user-friendly outputs and the job of Web Presenters to produce interactive visualizations.</p> <p>Both Secondary Analyzers and Web Presenters are also defined using interface objects. Secondary Analyzers will depend on the output of Primary Analyzers, and Web Presenters will depend on the output of both Primary and Secondary Analyzers.</p>"},{"location":"guides/design-philosophy/content-domain/#next-steps","title":"Next Steps","text":"<p>Once you finish reading this section it would be a good idea to review the other domain sections. Might also be a good idea to review the sections that discuss implementing  Shiny, and React dashboards.</p> <ul> <li>Core Domain</li> <li>Edge Domain</li> <li>Shiny Dashboards</li> <li>React Dashboards</li> </ul>"},{"location":"guides/design-philosophy/core-domain/","title":"Core Domain","text":""},{"location":"guides/design-philosophy/core-domain/#core-domain","title":"Core Domain","text":""},{"location":"guides/design-philosophy/core-domain/#application","title":"Application","text":"<p>The Application lives inside the <code>app</code> directory in the project root. This is responsible for defining and executing all capabilities of the application's workspace. Any extension or modification of the application's workspace capabilities should be done here.</p> <p>The application code should be free of specific storage implementation and be agnostic about the specifics of the terminal interface and the available analyzers.</p> <p>Here's what the entrypoint for the application module looks like</p> <p>./app/init.py:</p> <pre><code>from .analysis_context import AnalysisContext\nfrom .analysis_output_context import AnalysisOutputContext\nfrom .analysis_webserver_context import AnalysisWebServerContext\nfrom .app import App\nfrom .app_context import AppContext\nfrom .project_context import ProjectContext\nfrom .settings_context import SettingsContext\n</code></pre>"},{"location":"guides/design-philosophy/core-domain/#terminal-components","title":"Terminal Components","text":"<p>The Terminal Components live inside the <code>terminal_tools</code> inside the project root. Their main responsibility is user flow, rendering the terminal interface, and handling user input.</p> <p>The user flow understandably depends on the set of capabilities offered by the Application, so an adjustment there may require an adjustment here.</p> <p>Here's what the entrypoint for the termnal module looks like</p> <p>./terminal_tools/init.py</p> <pre><code>from .progress import ProgressReporter\nfrom .utils import (\n    clear_printed_lines,\n    clear_terminal,\n    draw_box,\n    enable_windows_ansi_support,\n    open_directory_explorer,\n    print_ascii_table,\n    wait_for_key,\n)\n</code></pre>"},{"location":"guides/design-philosophy/core-domain/#storage-io","title":"Storage IO","text":"<p>The Storage IO lives Inside the <code>storage</code> directory inside the project root. It is responsible for interacting directly with the file system where the workspace data and data files are stored. It makes decisions on paths, intermediate file formats, and database schema and implementation. It should know as little as possible about how the data is used and should be agnostic about the specifics of the terminal interface and the available analyzers.</p> <p>Here's what the entrypoint for the storage module looks like</p> <p>./storage/init.py:</p> <pre><code>import math\nimport os\nimport re\nimport shutil\nfrom datetime import datetime\nfrom typing import Callable, Iterable, Literal, Optional\n\nimport platformdirs\nimport polars as pl\nimport pyarrow.parquet as pq\nfrom filelock import FileLock\nfrom pydantic import BaseModel\nfrom tinydb import Query, TinyDB\nfrom xlsxwriter import Workbook\n\nfrom analyzer_interface.interface import AnalyzerOutput\nfrom analyzer_interface.params import ParamValue\n\nfrom .file_selector import FileSelectorStateManager\n\n\nclass ProjectModel(BaseModel):\n    class_: Literal[\"project\"] = \"project\"\n    id: str\n    display_name: str\n\n\nclass SettingsModel(BaseModel):\n    class_: Literal[\"settings\"] = \"settings\"\n    export_chunk_size: Optional[int | Literal[False]] = None\n\n\nclass FileSelectionState(BaseModel):\n    class_: Literal[\"file_selector_state\"] = \"file_selector_state\"\n    last_path: Optional[str] = None\n\n\nclass AnalysisModel(BaseModel):\n    class_: Literal[\"analysis\"] = \"analysis\"\n    analysis_id: str\n    project_id: str\n    display_name: str\n    primary_analyzer_id: str\n    path: str\n    column_mapping: Optional[dict[str, str]] = None\n    create_timestamp: Optional[float] = None\n    param_values: dict[str, ParamValue] = dict()\n    is_draft: bool = False\n\n    def create_time(self):\n        return (\n            datetime.fromtimestamp(self.create_timestamp)\n            if self.create_timestamp\n            else None\n        )\n\n\nSupportedOutputExtension = Literal[\"parquet\", \"csv\", \"xlsx\", \"json\"]\n\n\nclass Storage:\n    def __init__(self, *, app_name: str, app_author: str):\n        self.user_data_dir = platformdirs.user_data_dir(\n            appname=app_name, appauthor=app_author, ensure_exists=True\n        )\n        self.temp_dir = platformdirs.user_cache_dir(\n            appname=app_name, appauthor=app_author, ensure_exists=True\n        )\n        self.db = TinyDB(self._get_db_path())\n        with self._lock_database():\n            self._bootstrap_analyses_v1()\n\n        self.file_selector_state = AppFileSelectorStateManager(self)\n\n    def init_project(self, *, display_name: str, input_temp_file: str):\n        with self._lock_database():\n            project_id = self._find_unique_project_id(display_name)\n            project = ProjectModel(id=project_id, display_name=display_name)\n            self.db.insert(project.model_dump())\n\n        project_dir = self._get_project_path(project_id)\n        os.makedirs(project_dir, exist_ok=True)\n\n        shutil.move(input_temp_file, self._get_project_input_path(project_id))\n        return project\n\n    def list_projects(self):\n        q = Query()\n        projects = self.db.search(q[\"class_\"] == \"project\")\n        return sorted(\n            (ProjectModel(**project) for project in projects),\n            key=lambda project: project.display_name,\n        )\n\n    def get_project(self, project_id: str):\n        q = Query()\n        project = self.db.search((q[\"class_\"] == \"project\") &amp; (q[\"id\"] == project_id))\n        if project:\n            return ProjectModel(**project[0])\n        return None\n\n    def delete_project(self, project_id: str):\n        with self._lock_database():\n            q = Query()\n            self.db.remove((q[\"id\"] == project_id) &amp; (q[\"class_\"] == \"project\"))\n        project_path = self._get_project_path(project_id)\n        shutil.rmtree(project_path, ignore_errors=True)\n\n    def rename_project(self, project_id: str, name: str):\n        with self._lock_database():\n            q = Query()\n            self.db.update(\n                {\"display_name\": name},\n                (q[\"id\"] == project_id) &amp; (q[\"class_\"] == \"project\"),\n            )\n\n    def load_project_input(self, project_id: str, *, n_records: Optional[int] = None):\n        input_path = self._get_project_input_path(project_id)\n        return pl.read_parquet(input_path, n_rows=n_records)\n\n    def get_project_input_stats(self, project_id: str):\n        input_path = self._get_project_input_path(project_id)\n        num_rows = pl.scan_parquet(input_path).select(pl.count()).collect().item()\n        return TableStats(num_rows=num_rows)\n\n    def save_project_primary_outputs(\n        self, analysis: AnalysisModel, outputs: dict[str, pl.DataFrame]\n    ):\n        for output_id, output_df in outputs.items():\n            self._save_output(\n                os.path.join(\n                    self._get_project_primary_output_root_path(analysis),\n                    output_id,\n                ),\n                output_df,\n                \"parquet\",\n            )\n\n    def save_project_secondary_outputs(\n        self,\n        analysis: AnalysisModel,\n        secondary_id: str,\n        outputs: dict[str, pl.DataFrame],\n    ):\n        for output_id, output_df in outputs.items():\n            self._save_output(\n                os.path.join(\n                    self._get_project_secondary_output_root_path(\n                        analysis, secondary_id\n                    ),\n                    output_id,\n                ),\n                output_df,\n                \"parquet\",\n            )\n\n    def save_project_secondary_output(\n        self,\n        analysis: AnalysisModel,\n        secondary_id: str,\n        output_id: str,\n        output_df: pl.DataFrame,\n        extension: SupportedOutputExtension,\n    ):\n        root_path = self._get_project_secondary_output_root_path(analysis, secondary_id)\n        self._save_output(\n            os.path.join(root_path, output_id),\n            output_df,\n            extension,\n        )\n\n    def _save_output(\n        self,\n        output_path_without_extension,\n        output_df: pl.DataFrame | pl.LazyFrame,\n        extension: SupportedOutputExtension,\n    ):\n        output_df = output_df.lazy()\n        os.makedirs(os.path.dirname(output_path_without_extension), exist_ok=True)\n        output_path = f\"{output_path_without_extension}.{extension}\"\n        if extension == \"parquet\":\n            output_df.sink_parquet(output_path)\n        elif extension == \"csv\":\n            output_df.sink_csv(output_path)\n        elif extension == \"xlsx\":\n            # See https://xlsxwriter.readthedocs.io/working_with_dates_and_time.html#timezone-handling\n            with Workbook(output_path, {\"remove_timezone\": True}) as workbook:\n                output_df.collect().write_excel(workbook)\n        elif extension == \"json\":\n            output_df.collect().write_json(output_path)\n        else:\n            raise ValueError(f\"Unsupported format: {extension}\")\n        return output_path\n\n    def load_project_primary_output(self, analysis: AnalysisModel, output_id: str):\n        output_path = self.get_primary_output_parquet_path(analysis, output_id)\n        return pl.read_parquet(output_path)\n\n    def get_primary_output_parquet_path(self, analysis: AnalysisModel, output_id: str):\n        return os.path.join(\n            self._get_project_primary_output_root_path(analysis),\n            f\"{output_id}.parquet\",\n        )\n\n    def load_project_secondary_output(\n        self, analysis: AnalysisModel, secondary_id: str, output_id: str\n    ):\n        output_path = self.get_secondary_output_parquet_path(\n            analysis, secondary_id, output_id\n        )\n        return pl.read_parquet(output_path)\n\n    def get_secondary_output_parquet_path(\n        self, analysis: AnalysisModel, secondary_id: str, output_id: str\n    ):\n        return os.path.join(\n            self._get_project_secondary_output_root_path(analysis, secondary_id),\n            f\"{output_id}.parquet\",\n        )\n\n    def export_project_primary_output(\n        self,\n        analysis: AnalysisModel,\n        output_id: str,\n        *,\n        extension: SupportedOutputExtension,\n        spec: AnalyzerOutput,\n        export_chunk_size: Optional[int] = None,\n    ):\n        return self._export_output(\n            self.get_primary_output_parquet_path(analysis, output_id),\n            os.path.join(self._get_project_exports_root_path(analysis), output_id),\n            extension=extension,\n            spec=spec,\n            export_chunk_size=export_chunk_size,\n        )\n\n    def export_project_secondary_output(\n        self,\n        analysis: AnalysisModel,\n        secondary_id: str,\n        output_id: str,\n        *,\n        extension: SupportedOutputExtension,\n        spec: AnalyzerOutput,\n        export_chunk_size: Optional[int] = None,\n    ):\n        exported_path = os.path.join(\n            self._get_project_exports_root_path(analysis),\n            (\n                secondary_id\n                if secondary_id == output_id\n                else f\"{secondary_id}__{output_id}\"\n            ),\n        )\n        return self._export_output(\n            self.get_secondary_output_parquet_path(analysis, secondary_id, output_id),\n            exported_path,\n            extension=extension,\n            spec=spec,\n            export_chunk_size=export_chunk_size,\n        )\n\n    def _export_output(\n        self,\n        input_path: str,\n        output_path: str,\n        *,\n        extension: SupportedOutputExtension,\n        spec: AnalyzerOutput,\n        export_chunk_size: Optional[int] = None,\n    ):\n        with pq.ParquetFile(input_path) as reader:\n            num_chunks = (\n                math.ceil(reader.metadata.num_rows / export_chunk_size)\n                if export_chunk_size\n                else 1\n            )\n\n        if num_chunks == 1:\n            df = pl.scan_parquet(input_path)\n            self._save_output(output_path, spec.transform_output(df), extension)\n            return f\"{output_path}.{extension}\"\n\n        with pq.ParquetFile(input_path) as reader:\n            get_batches = (\n                df\n                for batch in reader.iter_batches()\n                if (df := pl.from_arrow(batch)) is not None\n            )\n            for chunk_id, chunk in enumerate(\n                collect_dataframe_chunks(get_batches, export_chunk_size)\n            ):\n                chunk = spec.transform_output(chunk)\n                self._save_output(f\"{output_path}_{chunk_id}\", chunk, extension)\n                yield chunk_id / num_chunks\n            return f\"{output_path}_[*].{extension}\"\n\n    def list_project_analyses(self, project_id: str):\n        with self._lock_database():\n            q = Query()\n            analysis_models = self.db.search(\n                (q[\"class_\"] == \"analysis\") &amp; (q[\"project_id\"] == project_id)\n            )\n        return [AnalysisModel(**analysis) for analysis in analysis_models]\n\n    def init_analysis(\n        self,\n        project_id: str,\n        display_name: str,\n        primary_analyzer_id: str,\n        column_mapping: dict[str, str],\n        param_values: dict[str, ParamValue],\n    ) -&gt; AnalysisModel:\n        with self._lock_database():\n            analysis_id = self._find_unique_analysis_id(project_id, display_name)\n            analysis = AnalysisModel(\n                analysis_id=analysis_id,\n                project_id=project_id,\n                display_name=display_name,\n                primary_analyzer_id=primary_analyzer_id,\n                path=os.path.join(\"analysis\", analysis_id),\n                column_mapping=column_mapping,\n                create_timestamp=datetime.now().timestamp(),\n                param_values=param_values,\n                is_draft=True,\n            )\n            self.db.insert(analysis.model_dump())\n        return analysis\n\n    def save_analysis(self, analysis: AnalysisModel):\n        with self._lock_database():\n            q = Query()\n            self.db.update(\n                analysis.model_dump(),\n                (q[\"class_\"] == \"analysis\")\n                &amp; (q[\"project_id\"] == analysis.project_id)\n                &amp; (q[\"analysis_id\"] == analysis.analysis_id),\n            )\n\n    def delete_analysis(self, analysis: AnalysisModel):\n        with self._lock_database():\n            q = Query()\n            self.db.remove(\n                (q[\"class_\"] == \"analysis\")\n                &amp; (q[\"project_id\"] == analysis.project_id)\n                &amp; (q[\"analysis_id\"] == analysis.analysis_id)\n            )\n            analysis_path = os.path.join(\n                self._get_project_path(analysis.project_id), analysis.path\n            )\n            shutil.rmtree(analysis_path, ignore_errors=True)\n\n    def _find_unique_analysis_id(self, project_id: str, display_name: str):\n        return self._get_unique_name(\n            self._slugify_name(display_name),\n            lambda analysis_id: self._is_analysis_id_unique(project_id, analysis_id),\n        )\n\n    def _is_analysis_id_unique(self, project_id: str, analysis_id: str):\n        q = Query()\n        id_unique = not self.db.search(\n            (q[\"class_\"] == \"analysis\")\n            &amp; (q[\"project_id\"] == project_id)\n            &amp; (q[\"analysis_id\"] == analysis_id)\n        )\n        dir_unique = not os.path.exists(\n            os.path.join(self._get_project_path(project_id), \"analysis\", analysis_id)\n        )\n        return id_unique and dir_unique\n\n    def _bootstrap_analyses_v1(self):\n        legacy_v1_analysis_dirname = \"analyzers\"\n        projects = self.list_projects()\n        for project in projects:\n            project_id = project.id\n            project_path = self._get_project_path(project_id)\n            try:\n                v1_analyses = os.listdir(\n                    os.path.join(project_path, legacy_v1_analysis_dirname)\n                )\n            except FileNotFoundError:\n                continue\n            for analyzer_id in v1_analyses:\n                db_analyzer_id = f\"__v1__{analyzer_id}\"\n                modified_time = os.path.getmtime(\n                    os.path.join(project_path, legacy_v1_analysis_dirname, analyzer_id)\n                )\n                self.db.upsert(\n                    AnalysisModel(\n                        analysis_id=db_analyzer_id,\n                        project_id=project_id,\n                        display_name=analyzer_id,\n                        primary_analyzer_id=analyzer_id,\n                        path=os.path.join(legacy_v1_analysis_dirname, analyzer_id),\n                        create_timestamp=modified_time,\n                    ).model_dump(),\n                    (Query()[\"class_\"] == \"analysis\")\n                    &amp; (Query()[\"project_id\"] == project_id)\n                    &amp; (Query()[\"analysis_id\"] == db_analyzer_id),\n                )\n\n    def list_secondary_analyses(self, analysis: AnalysisModel) -&gt; list[str]:\n        try:\n            analyzers = os.listdir(\n                os.path.join(\n                    self._get_project_path(analysis.project_id),\n                    analysis.path,\n                    \"secondary_outputs\",\n                ),\n            )\n            return analyzers\n        except FileNotFoundError:\n            return []\n\n    def _find_unique_project_id(self, display_name: str):\n        \"\"\"Turn the display name into a unique project ID\"\"\"\n        return self._get_unique_name(\n            self._slugify_name(display_name), self._is_project_id_unique\n        )\n\n    def _is_project_id_unique(self, project_id: str):\n        \"\"\"Check the database if the project ID is unique\"\"\"\n        q = Query()\n        id_unique = not self.db.search(\n            q[\"class_\"] == \"project\" and q[\"id\"] == project_id\n        )\n        dir_unique = not os.path.exists(self._get_project_path(project_id))\n        return id_unique and dir_unique\n\n    def _get_db_path(self):\n        return os.path.join(self.user_data_dir, \"db.json\")\n\n    def _get_project_path(self, project_id: str):\n        return os.path.join(self.user_data_dir, \"projects\", project_id)\n\n    def _get_project_input_path(self, project_id: str):\n        return os.path.join(self._get_project_path(project_id), \"input.parquet\")\n\n    def _get_project_primary_output_root_path(self, analysis: AnalysisModel):\n        return os.path.join(\n            self._get_project_path(analysis.project_id),\n            analysis.path,\n            \"primary_outputs\",\n        )\n\n    def _get_project_secondary_output_root_path(\n        self, analysis: AnalysisModel, secondary_id: str\n    ):\n        return os.path.join(\n            self._get_project_path(analysis.project_id),\n            analysis.path,\n            \"secondary_outputs\",\n            secondary_id,\n        )\n\n    def _get_project_exports_root_path(self, analysis: AnalysisModel):\n        return os.path.join(\n            self._get_project_path(analysis.project_id), analysis.path, \"exports\"\n        )\n\n    def _get_web_presenter_state_path(self, analysis: AnalysisModel, presenter_id: str):\n        return os.path.join(\n            self._get_project_path(analysis.project_id),\n            analysis.path,\n            \"web_presenters\",\n            presenter_id,\n            \"state\",\n        )\n\n    def _lock_database(self):\n        \"\"\"\n        Locks the database to prevent concurrent access, in case multiple instances\n        of the application are running.\n        \"\"\"\n        lock_path = os.path.join(self.temp_dir, \"db.lock\")\n        return FileLock(lock_path)\n\n    def get_settings(self):\n        with self._lock_database():\n            return self._get_settings()\n\n    def _get_settings(self):\n        q = Query()\n        settings = self.db.search(q[\"class_\"] == \"settings\")\n        if settings:\n            return SettingsModel(**settings[0])\n        return SettingsModel()\n\n    def save_settings(self, **kwargs):\n        with self._lock_database():\n            q = Query()\n            settings = self._get_settings()\n            new_settings = SettingsModel(\n                **{\n                    **settings.model_dump(),\n                    **{\n                        key: value for key, value in kwargs.items() if value is not None\n                    },\n                }\n            )\n            self.db.upsert(new_settings.model_dump(), q[\"class_\"] == \"settings\")\n\n    @staticmethod\n    def _slugify_name(name: str):\n        return re.sub(r\"\\W+\", \"_\", name.lower()).strip(\"_\")\n\n    @staticmethod\n    def _get_unique_name(base_name: str, validator: Callable[[str], bool]):\n        if validator(base_name):\n            return base_name\n        i = 1\n        while True:\n            candidate = f\"{base_name}_{i}\"\n            if validator(candidate):\n                return candidate\n            i += 1\n\n\nclass TableStats(BaseModel):\n    num_rows: int\n\n\ndef collect_dataframe_chunks(\n    input: Iterable[pl.DataFrame], size_threshold: int\n) -&gt; Iterable[pl.DataFrame]:\n    output_buffer = []\n    size = 0\n    for df in input:\n        while True:\n            available_space = size_threshold - size\n            slice = df.head(available_space)\n            output_buffer.append(slice)\n            size = size + slice.height\n            remaining_space = available_space - slice.height\n\n            if remaining_space == 0:\n                yield pl.concat(output_buffer)\n                output_buffer = []\n                size = 0\n\n            if slice.height == df.height:\n                break\n            else:\n                df = df.tail(-available_space)\n\n    if output_buffer:\n        yield pl.concat(output_buffer)\n\n\nclass AppFileSelectorStateManager(FileSelectorStateManager):\n    def __init__(self, storage: \"Storage\"):\n        self.storage = storage\n\n    def get_current_path(self):\n        return self._load_state().last_path\n\n    def set_current_path(self, path: str):\n        self._save_state(path)\n\n    def _load_state(self):\n        q = Query()\n        state = self.storage.db.search(q[\"class_\"] == \"file_selector_state\")\n        if state:\n            return FileSelectionState(**state[0])\n        return FileSelectionState()\n\n    def _save_state(self, last_path: str):\n        self.storage.db.upsert(\n            FileSelectionState(last_path=last_path).model_dump(),\n            Query()[\"class_\"] == \"file_selector_state\",\n        )\n</code></pre>"},{"location":"guides/design-philosophy/core-domain/#next-steps","title":"Next Steps","text":"<p>Once you finish reading this section it would be a good idea to review the other domain sections. Might also be a good idea to review the sections that discuss implementing  Shiny, and React dashboards.</p> <ul> <li>Edge Domain</li> <li>Content Domain</li> <li>Shiny Dashboards</li> <li>React Dashboards</li> </ul>"},{"location":"guides/design-philosophy/edge-domain/","title":"Edge Domain","text":""},{"location":"guides/design-philosophy/edge-domain/#edge-domain","title":"Edge Domain","text":"<p>The Edge domain governs data import and export.</p>"},{"location":"guides/design-philosophy/edge-domain/#importers","title":"Importers","text":"<p>The Importers live inside the <code>importing</code> directory inside the project root. Each importer offers a new way to import data into the workspace. The importers should be agnostic about the available analyzers. However, the Importers currently provide a terminal user flow so that their options can be customized by the user\u2014a necessity since each importer may expose different sets of options and may have different UX approaches for their configuration.</p> <p>The importers eventually write data to a parquet file, whose path is provisioned by the application.</p> <p>Here's what the entrypoint for the importer module looks like</p> <p>./importing/init.py:</p> <pre><code>from .csv import CSVImporter\nfrom .excel import ExcelImporter\nfrom .importer import Importer, ImporterSession\n\nimporters: list[Importer[ImporterSession]] = [CSVImporter(), ExcelImporter()]\n</code></pre>"},{"location":"guides/design-philosophy/edge-domain/#semantic-preprocessor","title":"Semantic Preprocessor","text":"<p>The Semantic Preprocessor lives inside the <code>preprocessing</code> directory inside the project root. It defines all the column data semantics\u2014a kind of type system that is used to guide the user in selecting the right columns for the right analysis. It is agnostic about the specific analyzers but does depend on them in a generic way\u2014the available semantics exist to support the needs of analyzers and will be extended as necessary.</p> <p>Here's what the entrypoint for the preprocessing module looks like</p> <p>./preprocessing/series_semantic.py:</p> <pre><code>from datetime import datetime\nfrom typing import Callable, Type, Union\n\nimport polars as pl\nfrom pydantic import BaseModel\n\nfrom analyzer_interface import DataType\n\n\nclass SeriesSemantic(BaseModel):\n    semantic_name: str\n    column_type: Union[Type[pl.DataType], Callable[[pl.DataType], bool]]\n    prevalidate: Callable[[pl.Series], bool] = lambda s: True\n    try_convert: Callable[[pl.Series], pl.Series]\n    validate_result: Callable[[pl.Series], pl.Series] = lambda s: s.is_not_null()\n    data_type: DataType\n\n    def check(self, series: pl.Series, threshold: float = 0.8, sample_size: int = 100):\n        if not self.check_type(series):\n            return False\n\n        sample = sample_series(series, sample_size)\n        try:\n            if not self.prevalidate(sample):\n                return False\n            result = self.try_convert(sample)\n        except Exception:\n            return False\n        return self.validate_result(result).sum() / sample.len() &gt; threshold\n\n    def check_type(self, series: pl.Series):\n        if isinstance(self.column_type, type):\n            return isinstance(series.dtype, self.column_type)\n        return self.column_type(series.dtype)\n\n\ndatetime_string = SeriesSemantic(\n    semantic_name=\"datetime\",\n    column_type=pl.String,\n    try_convert=lambda s: s.str.strptime(pl.Datetime, strict=False),\n    data_type=\"datetime\",\n)\n\n\ntimestamp_seconds = SeriesSemantic(\n    semantic_name=\"timestamp_seconds\",\n    column_type=lambda dt: dt.is_numeric(),\n    try_convert=lambda s: (s * 1_000).cast(pl.Datetime(time_unit=\"ms\")),\n    validate_result=lambda s: ((s &gt; datetime(2000, 1, 1)) &amp; (s &lt; datetime(2100, 1, 1))),\n    data_type=\"datetime\",\n)\n\ntimestamp_milliseconds = SeriesSemantic(\n    semantic_name=\"timestamp_milliseconds\",\n    column_type=lambda dt: dt.is_numeric(),\n    try_convert=lambda s: s.cast(pl.Datetime(time_unit=\"ms\")),\n    validate_result=lambda s: ((s &gt; datetime(2000, 1, 1)) &amp; (s &lt; datetime(2100, 1, 1))),\n    data_type=\"datetime\",\n)\n\nurl = SeriesSemantic(\n    semantic_name=\"url\",\n    column_type=pl.String,\n    try_convert=lambda s: s.str.strip_chars(),\n    validate_result=lambda s: s.str.count_matches(\"^https?://\").gt(0),\n    data_type=\"url\",\n)\n\nidentifier = SeriesSemantic(\n    semantic_name=\"identifier\",\n    column_type=pl.String,\n    try_convert=lambda s: s.str.strip_chars(),\n    validate_result=lambda s: s.str.count_matches(r\"^@?[A-Za-z0-9_.:-]+$\").eq(1),\n    data_type=\"identifier\",\n)\n\ntext_catch_all = SeriesSemantic(\n    semantic_name=\"free_text\",\n    column_type=pl.String,\n    try_convert=lambda s: s,\n    validate_result=lambda s: constant_series(s, True),\n    data_type=\"text\",\n)\n\ninteger_catch_all = SeriesSemantic(\n    semantic_name=\"integer\",\n    column_type=lambda dt: dt.is_integer(),\n    try_convert=lambda s: s,\n    validate_result=lambda s: constant_series(s, True),\n    data_type=\"integer\",\n)\n\nfloat_catch_all = SeriesSemantic(\n    semantic_name=\"float\",\n    column_type=lambda dt: dt.is_float(),\n    try_convert=lambda s: s,\n    validate_result=lambda s: constant_series(s, True),\n    data_type=\"float\",\n)\n\nboolean_catch_all = SeriesSemantic(\n    semantic_name=\"boolean\",\n    column_type=pl.Boolean,\n    try_convert=lambda s: s,\n    validate_result=lambda s: constant_series(s, True),\n    data_type=\"boolean\",\n)\n\nall_semantics = [\n    datetime_string,\n    timestamp_seconds,\n    timestamp_milliseconds,\n    url,\n    identifier,\n    text_catch_all,\n    integer_catch_all,\n    float_catch_all,\n    boolean_catch_all,\n]\n\n\ndef infer_series_semantic(\n    series: pl.Series, *, threshold: float = 0.8, sample_size=100\n):\n    for semantic in all_semantics:\n        if semantic.check(series, threshold=threshold, sample_size=sample_size):\n            return semantic\n    return None\n\n\ndef sample_series(series: pl.Series, n: int = 100):\n    if series.len() &lt; n:\n        return series\n    return series.sample(n, seed=0)\n\n\ndef constant_series(series: pl.Series, constant) -&gt; pl.Series:\n    \"\"\"Create a series with a constant value for each row of `series`.\"\"\"\n    return pl.Series([constant] * series.len(), dtype=pl.Boolean)\n</code></pre>"},{"location":"guides/design-philosophy/edge-domain/#next-steps","title":"Next Steps","text":"<p>Once you finish reading this section it would be a good idea to review the other domain sections. Might also be a good idea to review the sections that discuss implementing  Shiny, and React dashboards.</p> <ul> <li>Core Domain</li> <li>Content Domain</li> <li>Shiny Dashboards</li> <li>React Dashboards</li> </ul>"},{"location":"guides/get-started/installation/","title":"Installation","text":""},{"location":"guides/get-started/installation/#prerequisites","title":"Prerequisites","text":""},{"location":"guides/get-started/installation/#required-software","title":"Required Software","text":"<ul> <li>Python 3.12 - Required for all features to work correctly</li> <li>Node.JS (20.0.0 or above) - Required for the React dashboards   to work correctly</li> <li>Git - For version control and contributing</li> <li>Terminal/Command Line - Application runs in terminal interface</li> </ul>"},{"location":"guides/get-started/installation/#system-requirements","title":"System Requirements","text":"<ul> <li>Operating System: Windows (PowerShell), macOS, Linux</li> <li>Memory: 4GB+ RAM (for processing large datasets)</li> <li>Storage: 1GB+ free space (for project data and virtual environment)</li> </ul>"},{"location":"guides/get-started/installation/#resources","title":"Resources","text":"<p>If you haven't installed git, node.js, and/or python yet refer to the following links for instructions on downloading and installing said packages:</p> <ul> <li>https://codefinity.com/blog/A-step-by-step-guide-to-Git-installation</li> <li>https://nodejs.org/en/download</li> <li>https://realpython.com/installing-python/</li> </ul>"},{"location":"guides/get-started/installation/#checking-dependencies","title":"Checking Dependencies","text":"<p>If you're not sure which packages you already have installed on your system, the following commands can be used to figure what packages you already installed:</p>"},{"location":"guides/get-started/installation/#linux-mac-os","title":"Linux &amp; Mac OS","text":"<pre><code>which &lt;program_name_here (node|python|git)&gt;\n</code></pre>"},{"location":"guides/get-started/installation/#windows","title":"Windows","text":"<pre><code>where.exe &lt;program_name_here (node|python|git)&gt; \n</code></pre>"},{"location":"guides/get-started/installation/#installation","title":"Installation","text":""},{"location":"guides/get-started/installation/#1-clone-repository","title":"1. Clone Repository","text":"<pre><code>git clone https://github.com/CIB-Mango-Tree/mango-tango-cli.git\ncd mango-tango-cli\n</code></pre>"},{"location":"guides/get-started/installation/#2-create-virtual-environment","title":"2. Create Virtual Environment","text":"<pre><code>python -m venv venv\n</code></pre> <p>Verify Python version:</p> <pre><code>python --version  # Should show Python 3.12.x\n</code></pre>"},{"location":"guides/get-started/installation/#3-bootstrap-development-environment","title":"3. Bootstrap Development Environment","text":"<p>Mac OS/Linux (Bash):</p> <pre><code>./bootstrap.sh\n</code></pre> <p>Windows (PowerShell):</p> <pre><code>./bootstrap.ps1\n</code></pre> <p>The bootstrap script will:</p> <ul> <li>Activate the virtual environment</li> <li>Install all dependencies from <code>requirements-dev.txt</code></li> <li>Set up pre-commit hooks for code formatting</li> </ul>"},{"location":"guides/get-started/installation/#4-verify-installation","title":"4. Verify Installation","text":"<pre><code>python -m cibmangotree --noop\n</code></pre> <p>Should output: \"No-op flag detected. Exiting successfully.\"</p>"},{"location":"guides/get-started/installation/#activating-virtual-environment","title":"Activating Virtual Environment","text":"<p>After Completing the Installation the following commands can be used to activate the virtual environment in order to work with the project.</p> <p>Mac OS/Linux (Bash):</p> <pre><code>source ./venv/bin/activate\n</code></pre> <p>PowerShell (Windows):</p> <pre><code>./env/bin/Activate.ps1\n</code></pre>"},{"location":"guides/get-started/installation/#development-environment-setup","title":"Development Environment Setup","text":""},{"location":"guides/get-started/installation/#dependencies-overview","title":"Dependencies Overview","text":"<p>Production Dependencies (<code>requirements.txt</code>):</p> <ul> <li><code>polars==1.9.0</code> - Primary data processing</li> <li><code>pydantic==2.9.1</code> - Data validation and models</li> <li><code>inquirer==3.4.0</code> - Interactive terminal prompts</li> <li><code>tinydb==4.8.0</code> - Lightweight JSON database</li> <li><code>dash==2.18.1</code> - Web dashboard framework</li> <li><code>shiny==1.4.0</code> - Modern web UI framework</li> <li><code>plotly==5.24.1</code> - Data visualization</li> <li><code>XlsxWriter==3.2.0</code> - Excel export functionality</li> </ul> <p>Development Dependencies (<code>requirements-dev.txt</code>):</p> <ul> <li><code>black==24.10.0</code> - Code formatter</li> <li><code>isort==5.13.2</code> - Import organizer</li> <li><code>pytest==8.3.4</code> - Testing framework</li> <li><code>pyinstaller==6.14.1</code> - Executable building</li> </ul> <p>React Dashboard Dependencies (app/web_templates/package.json):</p> <ul> <li>typescript: 5.7.3</li> <li>vite: 6.3.5</li> <li>react: 19.0.0</li> <li>@deck.gl: 9.1.11</li> <li>@visx: 3.12.0</li> <li>@glideapps/glide-data-grid: 6.0.3</li> <li>@radix-ui: (Varies based on component being used)</li> <li>zustand: 5.0.3</li> <li>tailwindcss: 4.0.6</li> <li>lucide-react: 0.475.0</li> </ul>"},{"location":"guides/get-started/installation/#code-formatting-setup","title":"Code Formatting Setup","text":"<p>The project uses automatic code formatting:</p> <ul> <li>Black: Code style and formatting</li> <li>isort: Import organization</li> <li>Pre-commit hooks: Automatic formatting on commit</li> </ul> <p>Manual formatting:</p> <pre><code>isort .\nblack .\n</code></pre>"},{"location":"guides/get-started/installation/#project-structure-setup","title":"Project Structure Setup","text":"<p>After installation, your project structure should be:</p> <pre><code>mango-tango-cli/\n\u251c\u2500\u2500 venv/                    # Virtual environment\n\u251c\u2500\u2500 .serena/                 # Serena semantic analysis\n\u2502   \u2514\u2500\u2500 memories/           # Project knowledge base\n\u251c\u2500\u2500 docs/                    # Documentation\n\u2502   \u251c\u2500\u2500 ai-context/         # AI assistant context\n\u2502   \u2514\u2500\u2500 dev-guide.md        # Development guide\n\u251c\u2500\u2500 app/                     # Application layer\n\u251c\u2500\u2500 analyzers/              # Analysis modules\n\u251c\u2500\u2500 components/             # Terminal UI components\n\u251c\u2500\u2500 storage/                # Data persistence\n\u251c\u2500\u2500 importing/              # Data import modules\n\u251c\u2500\u2500 requirements*.txt       # Dependencies\n\u2514\u2500\u2500 cibmangotree.py         # Main entry point\n</code></pre>"},{"location":"guides/get-started/installation/#database-and-storage-setup","title":"Database and Storage Setup","text":""},{"location":"guides/get-started/installation/#application-data-directory","title":"Application Data Directory","text":"<p>The application automatically creates data directories:</p> <ul> <li>macOS: <code>~/Library/Application Support/MangoTango/</code></li> <li>Windows: <code>%APPDATA%/Civic Tech DC/MangoTango/</code></li> <li>Linux: <code>~/.local/share/MangoTango/</code></li> </ul>"},{"location":"guides/get-started/installation/#database-initialization","title":"Database Initialization","text":"<ul> <li>TinyDB: Automatically initialized on first run</li> <li>Project Files: Created in user data directory</li> <li>Parquet Files: Used for all analysis data storage</li> </ul> <p>No manual database setup required.</p>"},{"location":"guides/get-started/installation/#running-the-application","title":"Running the Application","text":""},{"location":"guides/get-started/installation/#basic-usage","title":"Basic Usage","text":"<pre><code># Start the application\npython -m cibmangotree\n</code></pre>"},{"location":"guides/get-started/installation/#development-mode","title":"Development Mode","text":"<pre><code># Run with debugging/development flags\npython -m cibmangotree --noop  # Test mode, exits immediately\n</code></pre>"},{"location":"guides/get-started/installation/#development-mode-for-the-react-dashboards","title":"Development Mode for The React Dashboards","text":"<p>The following commands can be used to start the development vite server for the react dashboards that are currently in development.</p> <p>npm:</p> <pre><code>cd ./app/web_templates\nnpm run dev\n</code></pre> <p>pnpm:</p> <pre><code>cd ./app/web_templates\npnpm dev\n</code></pre>"},{"location":"guides/get-started/installation/#testing-setup","title":"Testing Setup","text":""},{"location":"guides/get-started/installation/#run-tests","title":"Run Tests","text":"<pre><code># Run all tests\npytest\n\n# Run specific test file\npytest analyzers/hashtags/test_hashtags_analyzer.py\n\n# Run with verbose output\npytest -v\n\n# Run specific test function\npytest analyzers/hashtags/test_hashtags_analyzer.py::test_gini\n</code></pre>"},{"location":"guides/get-started/installation/#test-data","title":"Test Data","text":"<ul> <li>Test data is co-located with analyzers in <code>test_data/</code> directories</li> <li>Each analyzer should include its own test files</li> <li>Tests use sample data to verify functionality</li> </ul>"},{"location":"guides/get-started/installation/#build-setup-optional","title":"Build Setup (Optional)","text":""},{"location":"guides/get-started/installation/#executable-building","title":"Executable Building","text":"<pre><code># Build standalone executable\npyinstaller pyinstaller.spec\n\n# Output will be in dist/ directory\n</code></pre>"},{"location":"guides/get-started/installation/#bundle-building-for-react-dashboard","title":"Bundle Building for React Dashboard","text":"<p>npm:</p> <pre><code>npm run build\n</code></pre> <p>pnpm:</p> <pre><code>pnpm build\n</code></pre>"},{"location":"guides/get-started/installation/#build-requirements","title":"Build Requirements","text":"<ul> <li>Included in <code>requirements-dev.txt</code></li> <li>Used primarily for release distribution</li> <li>Not required for development</li> </ul>"},{"location":"guides/get-started/installation/#ide-integration","title":"IDE Integration","text":""},{"location":"guides/get-started/installation/#recommended-ide-settings","title":"Recommended IDE Settings","text":"<p>VS Code (<code>.vscode/</code> configuration):</p> <ul> <li>Python interpreter: <code>./venv/bin/python</code></li> <li>Black formatter integration</li> <li>isort integration</li> <li>pytest test discovery</li> </ul> <p>PyCharm:</p> <ul> <li>Interpreter: Project virtual environment</li> <li>Code style: Black</li> <li>Import optimizer: isort</li> </ul>"},{"location":"guides/get-started/installation/#git-configuration","title":"Git Configuration","text":"<p>Pre-commit Hooks:</p> <pre><code># Hooks are set up automatically by bootstrap script\n# Manual setup if needed:\npip install pre-commit\npre-commit install\n</code></pre> <p>Git Flow:</p> <ul> <li>Branch from <code>develop</code> (not <code>main</code>)</li> <li>Feature branches: <code>feature/name</code></li> <li>Bug fixes: <code>bugfix/name</code></li> </ul>"},{"location":"guides/get-started/installation/#version-management","title":"Version Management","text":"<p>If you already have Python and Node.JS installed but are on different versions from the versions outlined in the requirements above you can switch to the correct versions for both languages for the project using version managers. The version manager for python is pyenv. Where the version manager that is recommended for Node is nvm. Guides for installing both version managers are linked down below if you need references to go off of.</p> <ul> <li>https://www.freecodecamp.org/news/node-version-manager-nvm-install-guide/</li> <li>https://github.com/pyenv/pyenv?tab=readme-ov-file#installation</li> <li>https://github.com/pyenv-win/pyenv-win?tab=readme-ov-file#installation   (If you're on windows and want to install pyenv)</li> </ul> <p>Once you have both version managers installed the following commands can be used to switch versions.</p>"},{"location":"guides/get-started/installation/#pyenv","title":"pyenv","text":"<pre><code>pyenv install 3.12\npyenv local 3.12\n</code></pre>"},{"location":"guides/get-started/installation/#nvm","title":"nvm","text":"<pre><code>nvm install v21.0.0\nnvm use v21.0.0\n</code></pre>"},{"location":"guides/get-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/get-started/installation/#common-dependency-issues","title":"Common Dependency Issues","text":"<p>One common issue when installing the dependencies for python is the installation failing due to compatibility issues with the python package <code>pyarrow</code>. The compatibility issues are due to a version mismatch between pyarrow and python itself. To resolve this issue,you MUST be on version 3.12 for python. Refer to commands above to switch to the correct version.</p> <p>Similarly, the installation for node dependencies has been known to fail for some developers due to a version mismatch caused by the underlying dependencies for the package <code>@glideapps/glide-data-grid</code>. However, getting around this issue is more straightforward with node packages. Running the installation command for node with the flag <code>--legacy-peer-deps</code> is enough for the installation to work if you run into this issue. The commands needed to run the installation manually from the project root are as such.</p> <pre><code>cd ./app/web_templates\nnpm install --legacy-peer-deps\n</code></pre>"},{"location":"guides/get-started/installation/#other-common-issues","title":"Other Common Issues","text":"<p>Import Errors:</p> <pre><code># Ensure virtual environment is activated\nsource venv/bin/activate  # macOS/Linux\nvenv\\Scripts\\Activate.ps1     # Windows\n\n# Reinstall dependencies\npip install -r requirements-dev.txt\n</code></pre> <p>Formatting Errors in CI:</p> <pre><code># Run formatters locally before committing\nisort .\nblack .\n</code></pre> <p>Test Failures:</p> <pre><code># Ensure test data is present\nls analyzers/*/test_data/\n\n# Check if specific analyzer tests pass\npytest analyzers/hashtags/ -v\n</code></pre>"},{"location":"guides/get-started/installation/#environment-variables","title":"Environment Variables","text":"<p>Optional Configuration:</p> <ul> <li><code>MANGOTANGO_DATA_DIR</code> - Override default data directory</li> <li><code>MANGOTANGO_LOG_LEVEL</code> - Set logging verbosity</li> </ul>"},{"location":"guides/get-started/installation/#next-steps","title":"Next Steps","text":"<p>Once you have everything installed and running without any problems, the next step is to check out the Contributor Workflow</p>"},{"location":"guides/get-started/overview/","title":"Overview","text":""},{"location":"guides/get-started/overview/#mango-tango-cli","title":"Mango Tango CLI","text":""},{"location":"guides/get-started/overview/#repository-overview","title":"Repository Overview","text":"<p>Mango Tango CLI is a Python terminal-based tool for social media data analysis and visualization. It provides a modular, extensible architecture that separates core application logic from analysis modules, ensuring consistent UX while allowing easy contribution of new analyzers. The following documentation in this section is meant to provide a general overview of how the codebase for the project is structured, and to provide some context on patterns used throughout the project.</p>"},{"location":"guides/get-started/overview/#purpose-domain","title":"Purpose &amp; Domain","text":"<ul> <li>Social Media Analytics: Hashtag analysis, n-gram analysis, temporal   patterns, user coordination</li> <li>Modular Architecture: Clear separation between data import/export,   analysis, and presentation</li> <li>Interactive Workflows: Terminal-based UI with web dashboard capabilities</li> <li>Extensible Design: Plugin-like analyzer system for easy expansion</li> </ul>"},{"location":"guides/get-started/overview/#tech-stack","title":"Tech Stack","text":"<ul> <li>Core: Python 3.12, Inquirer (CLI), TinyDB (metadata), Starlette &amp; Uvicorn (web-server)</li> <li>Data: Polars/Pandas, PyArrow, Parquet files</li> <li>Web: Dash, Shiny for Python, Plotly, React</li> <li>Dev Tools: Black, isort, pytest, PyInstaller</li> </ul>"},{"location":"reference/analyzer_interface/","title":"Analyzer Interface","text":""},{"location":"reference/analyzer_interface/#analyzer_interface","title":"<code>analyzer_interface</code>","text":"<p>Modules:</p> Name Description <code>column_automap</code> <code>context</code> <code>data_type_compatibility</code> <code>declaration</code> <code>interface</code> <code>params</code>"},{"location":"reference/analyzer_interface/#analyzer_interface-attributes","title":"Attributes","text":""},{"location":"reference/analyzer_interface/#analyzer_interface-classes","title":"Classes","text":""},{"location":"reference/analyzer_interface/#analyzer_interface-functions","title":"Functions","text":""},{"location":"reference/analyzer_interface/#analyzer_interface-modules","title":"Modules","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.column_automap","title":"<code>column_automap</code>","text":"<p>Functions:</p> Name Description <code>check_name_hint</code> <p>Returns true if every word in the hint (split by spaces) is present in the name,</p> <code>column_automap</code> <p>Matches user-provided columns to the expected columns based on the name hints.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.column_automap-attributes","title":"Attributes","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.column_automap-classes","title":"Classes","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.column_automap-functions","title":"Functions","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.column_automap.check_name_hint","title":"<code>check_name_hint(name, hint)</code>","text":"<p>Returns true if every word in the hint (split by spaces) is present in the name, in a case insensitive manner.</p> Source code in <code>analyzer_interface/column_automap.py</code> <pre><code>def check_name_hint(name: str, hint: str):\n    \"\"\"\n    Returns true if every word in the hint (split by spaces) is present in the name,\n    in a case insensitive manner.\n    \"\"\"\n    return all(word.lower().strip() in name.lower() for word in hint.split(\" \"))\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.column_automap.column_automap","title":"<code>column_automap(user_columns, input_schema_columns)</code>","text":"<p>Matches user-provided columns to the expected columns based on the name hints.</p> <p>The resulting dictionary is keyed by the expected input column name.</p> Source code in <code>analyzer_interface/column_automap.py</code> <pre><code>def column_automap(\n    user_columns: list[UserInputColumn], input_schema_columns: list[InputColumn]\n):\n    \"\"\"\n    Matches user-provided columns to the expected columns based on the name hints.\n\n    The resulting dictionary is keyed by the expected input column name.\n    \"\"\"\n    matches: dict[str, str] = {}\n    for input_column in input_schema_columns:\n        max_score = None\n        best_match_user_column = None\n        for user_column in user_columns:\n            current_score = get_data_type_compatibility_score(\n                input_column.data_type, user_column.data_type\n            )\n\n            # Don't consider type-incompatible columns\n            if current_score is None:\n                continue\n\n            # Boost the score if we have a name hint match such that\n            # - among similarly compatible matches, those with name hints are preferred\n            # - among name hint matches, those with the best data type compatibility are preferred\n            if any(\n                check_name_hint(user_column.name, hint)\n                for hint in input_column.name_hints\n            ):\n                current_score += 10\n\n            if max_score is None or current_score &gt; max_score:\n                max_score = current_score\n                best_match_user_column = user_column\n\n        if best_match_user_column is not None:\n            matches[input_column.name] = best_match_user_column.name\n\n    return matches\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.context","title":"<code>context</code>","text":"<p>Classes:</p> Name Description <code>AssetsReader</code> <code>BaseDerivedModuleContext</code> <p>Common interface for secondary analyzers and web presenters runtime contexts.</p> <code>FactoryOutputContext</code> <p>Output interface for both factory and api_facotry functions for web</p> <code>InputTableReader</code> <code>PrimaryAnalyzerContext</code> <code>SecondaryAnalyzerContext</code> <code>ShinyContext</code> <p>Output interface for Shiny dashboards</p> <code>TableReader</code> <code>TableWriter</code> <code>WebPresenterContext</code>"},{"location":"reference/analyzer_interface/#analyzer_interface.context-classes","title":"Classes","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.context.AssetsReader","title":"<code>AssetsReader</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Methods:</p> Name Description <code>table</code> <p>Gets the table reader for the specified output.</p> Source code in <code>analyzer_interface/context.py</code> <pre><code>class AssetsReader(ABC):\n    @abstractmethod\n    def table(self, output_id: str) -&gt; \"TableReader\":\n        \"\"\"\n        Gets the table reader for the specified output.\n        \"\"\"\n        pass\n</code></pre> Functions <code></code> <code>table(output_id)</code> <code>abstractmethod</code> <p>Gets the table reader for the specified output.</p> Source code in <code>analyzer_interface/context.py</code> <pre><code>@abstractmethod\ndef table(self, output_id: str) -&gt; \"TableReader\":\n    \"\"\"\n    Gets the table reader for the specified output.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.BaseDerivedModuleContext","title":"<code>BaseDerivedModuleContext</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>ABC</code>, <code>BaseModel</code></p> <p>Common interface for secondary analyzers and web presenters runtime contexts.</p> <p>Fields:</p> <ul> <li> <code>temp_dir</code>                 (<code>str</code>)             </li> </ul> Source code in <code>analyzer_interface/context.py</code> <pre><code>class BaseDerivedModuleContext(ABC, BaseModel):\n    \"\"\"\n    Common interface for secondary analyzers and web presenters runtime contexts.\n    \"\"\"\n\n    temp_dir: str\n    \"\"\"\n  Gets the temporary directory that the module can freely write content to\n  during its lifetime. This directory will not persist between runs.\n  \"\"\"\n\n    @property\n    @abstractmethod\n    def base_params(self) -&gt; dict[str, ParamValue]:\n        \"\"\"\n        Gets the primary analysis parameters.\n        \"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def base(self) -&gt; \"AssetsReader\":\n        \"\"\"\n        Gets the base primary analyzer's context, which lets you inspect and load its\n        outputs.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def dependency(\n        self, secondary_interface: SecondaryAnalyzerInterface\n    ) -&gt; \"AssetsReader\":\n        \"\"\"\n        Gets the context of a secondary analyzer the current module depends on, which\n        lets you inspect and load its outputs.\n        \"\"\"\n        pass\n</code></pre> Attributes <code></code> <code>base</code> <code>abstractmethod</code> <code>property</code> <p>Gets the base primary analyzer's context, which lets you inspect and load its outputs.</p> <code></code> <code>base_params</code> <code>abstractmethod</code> <code>property</code> <p>Gets the primary analysis parameters.</p> <code></code> <code>temp_dir</code> <code>pydantic-field</code> <p>Gets the temporary directory that the module can freely write content to during its lifetime. This directory will not persist between runs.</p> Functions <code></code> <code>dependency(secondary_interface)</code> <code>abstractmethod</code> <p>Gets the context of a secondary analyzer the current module depends on, which lets you inspect and load its outputs.</p> Source code in <code>analyzer_interface/context.py</code> <pre><code>@abstractmethod\ndef dependency(\n    self, secondary_interface: SecondaryAnalyzerInterface\n) -&gt; \"AssetsReader\":\n    \"\"\"\n    Gets the context of a secondary analyzer the current module depends on, which\n    lets you inspect and load its outputs.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.FactoryOutputContext","title":"<code>FactoryOutputContext</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Output interface for both factory and api_facotry functions for web presenters.</p> <p>Fields:</p> <ul> <li> <code>shiny</code>                 (<code>Optional[ShinyContext]</code>)             </li> <li> <code>api</code>                 (<code>Optional[dict[str, Any]]</code>)             </li> <li> <code>data_frames</code>                 (<code>Optional[dict[str, DataFrame]]</code>)             </li> </ul> Source code in <code>analyzer_interface/context.py</code> <pre><code>class FactoryOutputContext(BaseModel):\n    \"\"\"\n    Output interface for both factory and api_facotry functions for web\n    presenters.\n    \"\"\"\n\n    shiny: Optional[ShinyContext] = None\n    \"\"\"\n    Factory oputput for shiny dashboards\n    \"\"\"\n\n    api: Optional[dict[str, Any]] = None\n    \"\"\"\n    API factory output for React dashboard REST API\n    \"\"\"\n\n    data_frames: Optional[dict[str, DataFrame]] = None\n    \"\"\"\n    API factory dataframe output for React dashboard REST API\n    \"\"\"\n\n    class Config:\n        arbitrary_types_allowed = True\n</code></pre> Attributes <code></code> <code>api = None</code> <code>pydantic-field</code> <p>API factory output for React dashboard REST API</p> <code></code> <code>data_frames = None</code> <code>pydantic-field</code> <p>API factory dataframe output for React dashboard REST API</p> <code></code> <code>shiny = None</code> <code>pydantic-field</code> <p>Factory oputput for shiny dashboards</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.InputTableReader","title":"<code>InputTableReader</code>","text":"<p>               Bases: <code>TableReader</code></p> <p>Methods:</p> Name Description <code>preprocess</code> <p>Given the manually loaded user input dataframe, apply column mapping and</p> Source code in <code>analyzer_interface/context.py</code> <pre><code>class InputTableReader(TableReader):\n    @abstractmethod\n    def preprocess[\n        PolarsDataFrameLike\n    ](self, df: PolarsDataFrameLike) -&gt; PolarsDataFrameLike:\n        \"\"\"\n        Given the manually loaded user input dataframe, apply column mapping and\n        semantic transformations to give the input dataframe that the analyzer\n        expects.\n        \"\"\"\n        pass\n</code></pre> Functions <code></code> <code>preprocess(df)</code> <code>abstractmethod</code> <p>Given the manually loaded user input dataframe, apply column mapping and semantic transformations to give the input dataframe that the analyzer expects.</p> Source code in <code>analyzer_interface/context.py</code> <pre><code>@abstractmethod\ndef preprocess[\n    PolarsDataFrameLike\n](self, df: PolarsDataFrameLike) -&gt; PolarsDataFrameLike:\n    \"\"\"\n    Given the manually loaded user input dataframe, apply column mapping and\n    semantic transformations to give the input dataframe that the analyzer\n    expects.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.PrimaryAnalyzerContext","title":"<code>PrimaryAnalyzerContext</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>ABC</code>, <code>BaseModel</code></p> <p>Fields:</p> <ul> <li> <code>temp_dir</code>                 (<code>str</code>)             </li> </ul> Source code in <code>analyzer_interface/context.py</code> <pre><code>class PrimaryAnalyzerContext(ABC, BaseModel):\n    temp_dir: str\n    \"\"\"\n  Gets the temporary directory that the module can freely write content to\n  during its lifetime. This directory will not persist between runs.\n  \"\"\"\n\n    @abstractmethod\n    def input(self) -&gt; \"InputTableReader\":\n        \"\"\"\n        Gets the input reader context.\n\n        **Note that this is in function form** even though one input is expected,\n        in anticipation that we may want to support multiple inputs in the future.\n        \"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def params(self) -&gt; dict[str, ParamValue]:\n        \"\"\"\n        Gets the analysis parameters.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def output(self, output_id: str) -&gt; \"TableWriter\":\n        \"\"\"\n        Gets the output writer context for the specified output ID.\n        \"\"\"\n        pass\n</code></pre> Attributes <code></code> <code>params</code> <code>abstractmethod</code> <code>property</code> <p>Gets the analysis parameters.</p> <code></code> <code>temp_dir</code> <code>pydantic-field</code> <p>Gets the temporary directory that the module can freely write content to during its lifetime. This directory will not persist between runs.</p> Functions <code></code> <code>input()</code> <code>abstractmethod</code> <p>Gets the input reader context.</p> <p>Note that this is in function form even though one input is expected, in anticipation that we may want to support multiple inputs in the future.</p> Source code in <code>analyzer_interface/context.py</code> <pre><code>@abstractmethod\ndef input(self) -&gt; \"InputTableReader\":\n    \"\"\"\n    Gets the input reader context.\n\n    **Note that this is in function form** even though one input is expected,\n    in anticipation that we may want to support multiple inputs in the future.\n    \"\"\"\n    pass\n</code></pre> <code></code> <code>output(output_id)</code> <code>abstractmethod</code> <p>Gets the output writer context for the specified output ID.</p> Source code in <code>analyzer_interface/context.py</code> <pre><code>@abstractmethod\ndef output(self, output_id: str) -&gt; \"TableWriter\":\n    \"\"\"\n    Gets the output writer context for the specified output ID.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.SecondaryAnalyzerContext","title":"<code>SecondaryAnalyzerContext</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseDerivedModuleContext</code></p> <p>Fields:</p> <ul> <li> <code>temp_dir</code>                 (<code>str</code>)             </li> </ul> Source code in <code>analyzer_interface/context.py</code> <pre><code>class SecondaryAnalyzerContext(BaseDerivedModuleContext):\n    @abstractmethod\n    def output(self, output_id: str) -&gt; \"TableWriter\":\n        \"\"\"\n        Gets the output writer context\n        \"\"\"\n        pass\n</code></pre> Functions <code></code> <code>output(output_id)</code> <code>abstractmethod</code> <p>Gets the output writer context</p> Source code in <code>analyzer_interface/context.py</code> <pre><code>@abstractmethod\ndef output(self, output_id: str) -&gt; \"TableWriter\":\n    \"\"\"\n    Gets the output writer context\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.ShinyContext","title":"<code>ShinyContext</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Output interface for Shiny dashboards</p> <p>Fields:</p> <ul> <li> <code>panel</code>                 (<code>NavPanel</code>)             </li> <li> <code>server_handler</code>                 (<code>Optional[ServerCallback]</code>)             </li> </ul> Source code in <code>analyzer_interface/context.py</code> <pre><code>class ShinyContext(BaseModel):\n    \"\"\"\n    Output interface for Shiny dashboards\n    \"\"\"\n\n    panel: NavPanel = None\n    \"\"\"\n    UI navigation panel to be added to shiny dashboard\n    \"\"\"\n\n    server_handler: Optional[ServerCallback] = None\n    \"\"\"\n    Server handler callback to be called by the shiny application instance\n    \"\"\"\n\n    class Config:\n        arbitrary_types_allowed = True\n</code></pre> Attributes <code></code> <code>panel = None</code> <code>pydantic-field</code> <p>UI navigation panel to be added to shiny dashboard</p> <code></code> <code>server_handler = None</code> <code>pydantic-field</code> <p>Server handler callback to be called by the shiny application instance</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.TableReader","title":"<code>TableReader</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Attributes:</p> Name Type Description <code>parquet_path</code> <code>str</code> <p>Gets the path to the table's parquet file. The module should expect a parquet</p> Source code in <code>analyzer_interface/context.py</code> <pre><code>class TableReader(ABC):\n    @property\n    @abstractmethod\n    def parquet_path(self) -&gt; str:\n        \"\"\"\n        Gets the path to the table's parquet file. The module should expect a parquet\n        file here.\n        \"\"\"\n        pass\n</code></pre> Attributes <code></code> <code>parquet_path</code> <code>abstractmethod</code> <code>property</code> <p>Gets the path to the table's parquet file. The module should expect a parquet file here.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.TableWriter","title":"<code>TableWriter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Attributes:</p> Name Type Description <code>parquet_path</code> <code>str</code> <p>Gets the path to the table's parquet file. The module should write a parquet</p> Source code in <code>analyzer_interface/context.py</code> <pre><code>class TableWriter(ABC):\n    @property\n    @abstractmethod\n    def parquet_path(self) -&gt; str:\n        \"\"\"\n        Gets the path to the table's parquet file. The module should write a parquet\n        file to it.\n        \"\"\"\n        pass\n</code></pre> Attributes <code></code> <code>parquet_path</code> <code>abstractmethod</code> <code>property</code> <p>Gets the path to the table's parquet file. The module should write a parquet file to it.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.context.WebPresenterContext","title":"<code>WebPresenterContext</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseDerivedModuleContext</code></p> <p>Fields:</p> <ul> <li> <code>temp_dir</code>                 (<code>str</code>)             </li> <li> <code>dash_app</code>                 (<code>Dash</code>)             </li> </ul> Source code in <code>analyzer_interface/context.py</code> <pre><code>class WebPresenterContext(BaseDerivedModuleContext):\n    dash_app: Dash\n    \"\"\"\n  The Dash app that is being built.\n  \"\"\"\n\n    @property\n    @abstractmethod\n    def state_dir(self) -&gt; str:\n        \"\"\"\n        Gets the directory where the web presenter can store state that persists\n        between runs. This state space is unique for each\n        project/primary analyzer/web presenter combination.\n        \"\"\"\n        pass\n\n    class Config:\n        arbitrary_types_allowed = True\n</code></pre> Attributes <code></code> <code>dash_app</code> <code>pydantic-field</code> <p>The Dash app that is being built.</p> <code></code> <code>state_dir</code> <code>abstractmethod</code> <code>property</code> <p>Gets the directory where the web presenter can store state that persists between runs. This state space is unique for each project/primary analyzer/web presenter combination.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.data_type_compatibility","title":"<code>data_type_compatibility</code>","text":"<p>Functions:</p> Name Description <code>get_data_type_compatibility_score</code> <p>Returns a score for the compatibility of the actual data type with the</p> <p>Attributes:</p> Name Type Description <code>data_type_mapping_preference</code> <code>dict[DataType, list[list[DataType]]]</code> <p>For each data type, a list of lists of data types that are considered compatible</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.data_type_compatibility-attributes","title":"Attributes","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.data_type_compatibility.data_type_mapping_preference","title":"<code>data_type_mapping_preference = {'text': [['text'], ['identifier', 'url']], 'integer': [['integer']], 'float': [['float', 'integer']], 'boolean': [['boolean']], 'datetime': [['datetime']], 'time': [['time'], ['datetime']], 'identifier': [['identifier'], ['url', 'datetime'], ['integer'], ['text']], 'url': [['url']]}</code>  <code>module-attribute</code>","text":"<p>For each data type, a list of lists of data types that are considered compatible with it. The first list is the most preferred, the last list is the least. The items in each list are considered equally compatible.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.data_type_compatibility-functions","title":"Functions","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.data_type_compatibility.get_data_type_compatibility_score","title":"<code>get_data_type_compatibility_score(expected_data_type, actual_data_type)</code>","text":"<p>Returns a score for the compatibility of the actual data type with the expected data type. Higher (less negative) scores are better. <code>None</code> means the data types are not compatible.</p> Source code in <code>analyzer_interface/data_type_compatibility.py</code> <pre><code>def get_data_type_compatibility_score(\n    expected_data_type: DataType, actual_data_type: DataType\n):\n    \"\"\"\n    Returns a score for the compatibility of the actual data type with the\n    expected data type. Higher (less negative) scores are better.\n    `None` means the data types are not compatible.\n    \"\"\"\n    if expected_data_type == actual_data_type:\n        return 0\n\n    for i, preference_list in enumerate(\n        data_type_mapping_preference[expected_data_type]\n    ):\n        if actual_data_type in preference_list:\n            return -(i + 1)\n\n    return None\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.declaration","title":"<code>declaration</code>","text":"<p>Classes:</p> Name Description <code>AnalyzerDeclaration</code> <code>SecondaryAnalyzerDeclaration</code> <code>WebPresenterDeclaration</code>"},{"location":"reference/analyzer_interface/#analyzer_interface.declaration-classes","title":"Classes","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.declaration.AnalyzerDeclaration","title":"<code>AnalyzerDeclaration</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>AnalyzerInterface</code></p> <p>Fields:</p> <ul> <li> <code>id</code>                 (<code>str</code>)             </li> <li> <code>version</code>                 (<code>str</code>)             </li> <li> <code>name</code>                 (<code>str</code>)             </li> <li> <code>short_description</code>                 (<code>str</code>)             </li> <li> <code>long_description</code>                 (<code>Optional[str]</code>)             </li> <li> <code>input</code>                 (<code>AnalyzerInput</code>)             </li> <li> <code>params</code>                 (<code>list[AnalyzerParam]</code>)             </li> <li> <code>outputs</code>                 (<code>list[AnalyzerOutput]</code>)             </li> <li> <code>kind</code>                 (<code>Literal['primary']</code>)             </li> <li> <code>entry_point</code>                 (<code>Callable[[PrimaryAnalyzerContext], None]</code>)             </li> <li> <code>default_params</code>                 (<code>Callable[[PrimaryAnalyzerContext], dict[str, ParamValue]]</code>)             </li> <li> <code>is_distributed</code>                 (<code>bool</code>)             </li> </ul> Source code in <code>analyzer_interface/declaration.py</code> <pre><code>class AnalyzerDeclaration(AnalyzerInterface):\n    entry_point: Callable[[PrimaryAnalyzerContext], None]\n    default_params: Callable[[PrimaryAnalyzerContext], dict[str, ParamValue]]\n    is_distributed: bool\n\n    def __init__(\n        self,\n        interface: AnalyzerInterface,\n        main: Callable,\n        *,\n        is_distributed: bool = False,\n        default_params: Callable[[PrimaryAnalyzerContext], dict[str, ParamValue]] = (\n            lambda _: dict()\n        )\n    ):\n        \"\"\"Creates a primary analyzer declaration\n\n        Args:\n          interface (AnalyzerInterface): The metadata interface for the primary analyzer.\n\n          main (Callable):\n            The entry point function for the primary analyzer. This function should\n            take a single argument of type `PrimaryAnalyzerContext` and should ensure\n            that the outputs specified in the interface are generated.\n\n          is_distributed (bool):\n            Set this explicitly to `True` once the analyzer is ready to be shipped\n            to end users; it will make the analyzer available in the distributed\n            executable.\n        \"\"\"\n        super().__init__(\n            **interface.model_dump(),\n            entry_point=main,\n            default_params=default_params,\n            is_distributed=is_distributed\n        )\n</code></pre> Functions <code></code> <code>__init__(interface, main, *, is_distributed=False, default_params=lambda _: dict())</code> <p>Creates a primary analyzer declaration</p> <p>Parameters:</p> Name Type Description Default <code>interface</code> <code>AnalyzerInterface</code> <p>The metadata interface for the primary analyzer.</p> required <code>main</code> <code>Callable</code> <p>The entry point function for the primary analyzer. This function should take a single argument of type <code>PrimaryAnalyzerContext</code> and should ensure that the outputs specified in the interface are generated.</p> required <code>is_distributed</code> <code>bool</code> <p>Set this explicitly to <code>True</code> once the analyzer is ready to be shipped to end users; it will make the analyzer available in the distributed executable.</p> <code>False</code> Source code in <code>analyzer_interface/declaration.py</code> <pre><code>def __init__(\n    self,\n    interface: AnalyzerInterface,\n    main: Callable,\n    *,\n    is_distributed: bool = False,\n    default_params: Callable[[PrimaryAnalyzerContext], dict[str, ParamValue]] = (\n        lambda _: dict()\n    )\n):\n    \"\"\"Creates a primary analyzer declaration\n\n    Args:\n      interface (AnalyzerInterface): The metadata interface for the primary analyzer.\n\n      main (Callable):\n        The entry point function for the primary analyzer. This function should\n        take a single argument of type `PrimaryAnalyzerContext` and should ensure\n        that the outputs specified in the interface are generated.\n\n      is_distributed (bool):\n        Set this explicitly to `True` once the analyzer is ready to be shipped\n        to end users; it will make the analyzer available in the distributed\n        executable.\n    \"\"\"\n    super().__init__(\n        **interface.model_dump(),\n        entry_point=main,\n        default_params=default_params,\n        is_distributed=is_distributed\n    )\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.declaration.SecondaryAnalyzerDeclaration","title":"<code>SecondaryAnalyzerDeclaration</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>SecondaryAnalyzerInterface</code></p> <p>Fields:</p> <ul> <li> <code>id</code>                 (<code>str</code>)             </li> <li> <code>version</code>                 (<code>str</code>)             </li> <li> <code>name</code>                 (<code>str</code>)             </li> <li> <code>short_description</code>                 (<code>str</code>)             </li> <li> <code>long_description</code>                 (<code>Optional[str]</code>)             </li> <li> <code>base_analyzer</code>                 (<code>AnalyzerInterface</code>)             </li> <li> <code>depends_on</code>                 (<code>list[SecondaryAnalyzerInterface]</code>)             </li> <li> <code>outputs</code>                 (<code>list[AnalyzerOutput]</code>)             </li> <li> <code>kind</code>                 (<code>Literal['secondary']</code>)             </li> <li> <code>entry_point</code>                 (<code>Callable[[SecondaryAnalyzerContext], None]</code>)             </li> </ul> Source code in <code>analyzer_interface/declaration.py</code> <pre><code>class SecondaryAnalyzerDeclaration(SecondaryAnalyzerInterface):\n    entry_point: Callable[[\"SecondaryAnalyzerContext\"], None]\n\n    def __init__(self, interface: SecondaryAnalyzerInterface, main: Callable):\n        \"\"\"Creates a secondary analyzer declaration\n\n        Args:\n          interface (SecondaryAnalyzerInterface): The metadata interface for the secondary analyzer.\n\n          main (Callable):\n            The entry point function for the secondary analyzer. This function should\n            take a single argument of type `SecondaryAnalyzerContext` and should ensure\n            that the outputs specified in the interface are generated.\n        \"\"\"\n        super().__init__(**interface.model_dump(), entry_point=main)\n</code></pre> Functions <code></code> <code>__init__(interface, main)</code> <p>Creates a secondary analyzer declaration</p> <p>Parameters:</p> Name Type Description Default <code>interface</code> <code>SecondaryAnalyzerInterface</code> <p>The metadata interface for the secondary analyzer.</p> required <code>main</code> <code>Callable</code> <p>The entry point function for the secondary analyzer. This function should take a single argument of type <code>SecondaryAnalyzerContext</code> and should ensure that the outputs specified in the interface are generated.</p> required Source code in <code>analyzer_interface/declaration.py</code> <pre><code>def __init__(self, interface: SecondaryAnalyzerInterface, main: Callable):\n    \"\"\"Creates a secondary analyzer declaration\n\n    Args:\n      interface (SecondaryAnalyzerInterface): The metadata interface for the secondary analyzer.\n\n      main (Callable):\n        The entry point function for the secondary analyzer. This function should\n        take a single argument of type `SecondaryAnalyzerContext` and should ensure\n        that the outputs specified in the interface are generated.\n    \"\"\"\n    super().__init__(**interface.model_dump(), entry_point=main)\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.declaration.WebPresenterDeclaration","title":"<code>WebPresenterDeclaration</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>WebPresenterInterface</code></p> <p>Fields:</p> <ul> <li> <code>id</code>                 (<code>str</code>)             </li> <li> <code>version</code>                 (<code>str</code>)             </li> <li> <code>name</code>                 (<code>str</code>)             </li> <li> <code>short_description</code>                 (<code>str</code>)             </li> <li> <code>long_description</code>                 (<code>Optional[str]</code>)             </li> <li> <code>base_analyzer</code>                 (<code>AnalyzerInterface</code>)             </li> <li> <code>depends_on</code>                 (<code>list[SecondaryAnalyzerInterface]</code>)             </li> <li> <code>kind</code>                 (<code>Literal['web']</code>)             </li> <li> <code>factory</code>                 (<code>Callable[[WebPresenterContext], Union[FactoryOutputContext, None]]</code>)             </li> <li> <code>shiny</code>                 (<code>bool</code>)             </li> <li> <code>server_name</code>                 (<code>str</code>)             </li> </ul> Source code in <code>analyzer_interface/declaration.py</code> <pre><code>class WebPresenterDeclaration(WebPresenterInterface):\n    factory: Callable[[\"WebPresenterContext\"], Union[FactoryOutputContext, None]]\n    shiny: bool\n    server_name: str\n\n    def __init__(\n        self,\n        interface: WebPresenterInterface,\n        factory: Callable,\n        name: str,\n        shiny: bool,\n    ):\n        \"\"\"Creates a web presenter declaration\n\n        Args:\n          interface (WebPresenterInterface): The metadata interface for the web presenter.\n\n          factory (Callable):\n            The factory function that creates a Dash app for the web presenter. It should\n            modify the Dash app in the context to add whatever plotting interface\n            the web presenter needs.\n\n          server_name (str):\n            The server name for the Dash app. Typically, you will use the global\n            variable `__name__` here.\n\n            If your web presenter has assets like images, CSS or JavaScript files,\n            you can put them in a folder named `assets` in the same directory\n            as the file where `__name__` is used. The Dash app will serve these\n            files at the `/assets/` URL, using the python module name in `__name__`\n            to determine the absolute path to the assets folder.\n\n            See Dash documentation for more details: https://dash.plotly.com\n            See also Python documentation for the `__name__` variable:\n            https://docs.python.org/3/tutorial/modules.html\n\n        \"\"\"\n        super().__init__(\n            **interface.model_dump(), factory=factory, server_name=name, shiny=shiny\n        )\n</code></pre> Functions <code></code> <code>__init__(interface, factory, name, shiny)</code> <p>Creates a web presenter declaration</p> <p>Parameters:</p> Name Type Description Default <code>interface</code> <code>WebPresenterInterface</code> <p>The metadata interface for the web presenter.</p> required <code>factory</code> <code>Callable</code> <p>The factory function that creates a Dash app for the web presenter. It should modify the Dash app in the context to add whatever plotting interface the web presenter needs.</p> required <code>server_name</code> <code>str</code> <p>The server name for the Dash app. Typically, you will use the global variable <code>__name__</code> here.</p> <p>If your web presenter has assets like images, CSS or JavaScript files, you can put them in a folder named <code>assets</code> in the same directory as the file where <code>__name__</code> is used. The Dash app will serve these files at the <code>/assets/</code> URL, using the python module name in <code>__name__</code> to determine the absolute path to the assets folder.</p> <p>See Dash documentation for more details: https://dash.plotly.com See also Python documentation for the <code>__name__</code> variable: https://docs.python.org/3/tutorial/modules.html</p> required Source code in <code>analyzer_interface/declaration.py</code> <pre><code>def __init__(\n    self,\n    interface: WebPresenterInterface,\n    factory: Callable,\n    name: str,\n    shiny: bool,\n):\n    \"\"\"Creates a web presenter declaration\n\n    Args:\n      interface (WebPresenterInterface): The metadata interface for the web presenter.\n\n      factory (Callable):\n        The factory function that creates a Dash app for the web presenter. It should\n        modify the Dash app in the context to add whatever plotting interface\n        the web presenter needs.\n\n      server_name (str):\n        The server name for the Dash app. Typically, you will use the global\n        variable `__name__` here.\n\n        If your web presenter has assets like images, CSS or JavaScript files,\n        you can put them in a folder named `assets` in the same directory\n        as the file where `__name__` is used. The Dash app will serve these\n        files at the `/assets/` URL, using the python module name in `__name__`\n        to determine the absolute path to the assets folder.\n\n        See Dash documentation for more details: https://dash.plotly.com\n        See also Python documentation for the `__name__` variable:\n        https://docs.python.org/3/tutorial/modules.html\n\n    \"\"\"\n    super().__init__(\n        **interface.model_dump(), factory=factory, server_name=name, shiny=shiny\n    )\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface","title":"<code>interface</code>","text":"<p>Classes:</p> Name Description <code>AnalyzerInterface</code> <code>AnalyzerOutput</code> <code>AnalyzerParam</code> <code>BaseAnalyzerInterface</code> <code>DerivedAnalyzerInterface</code> <code>InputColumn</code> <code>SecondaryAnalyzerInterface</code> <p>Attributes:</p> Name Type Description <code>DataType</code> <p>The semantic data type for a data column. This is not quite the same as</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface-attributes","title":"Attributes","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.interface.DataType","title":"<code>DataType = Literal['text', 'integer', 'float', 'boolean', 'datetime', 'identifier', 'url', 'time']</code>  <code>module-attribute</code>","text":"<p>The semantic data type for a data column. This is not quite the same as structural data types like polars or pandas or even arrow types, but they represent how the data is intended to be interpreted.</p> <ul> <li><code>text</code> is expected to be a free-form human-readable text content.</li> <li><code>integer</code> and <code>float</code> are meant to be manipulated arithmetically.</li> <li><code>boolean</code> is a binary value.</li> <li><code>datetime</code> represents time and are meant to be manipulated as time values.</li> <li><code>time</code> represents time within a day, not including the date information.</li> <li><code>identifier</code> is a unique identifier for a record. It is not expected to be manipulated in any way.</li> <li><code>url</code> is a string that represents a URL.</li> </ul>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface-classes","title":"Classes","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.interface.AnalyzerInterface","title":"<code>AnalyzerInterface</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseAnalyzerInterface</code></p> <p>Fields:</p> <ul> <li> <code>id</code>                 (<code>str</code>)             </li> <li> <code>version</code>                 (<code>str</code>)             </li> <li> <code>name</code>                 (<code>str</code>)             </li> <li> <code>short_description</code>                 (<code>str</code>)             </li> <li> <code>long_description</code>                 (<code>Optional[str]</code>)             </li> <li> <code>input</code>                 (<code>AnalyzerInput</code>)             </li> <li> <code>params</code>                 (<code>list[AnalyzerParam]</code>)             </li> <li> <code>outputs</code>                 (<code>list[AnalyzerOutput]</code>)             </li> <li> <code>kind</code>                 (<code>Literal['primary']</code>)             </li> </ul> Source code in <code>analyzer_interface/interface.py</code> <pre><code>class AnalyzerInterface(BaseAnalyzerInterface):\n    input: AnalyzerInput\n    \"\"\"\n  Specifies the input data schema for the analyzer.\n  \"\"\"\n\n    params: list[AnalyzerParam] = []\n    \"\"\"\n  A list of parameters that the analyzer accepts.\n  \"\"\"\n\n    outputs: list[\"AnalyzerOutput\"]\n    \"\"\"\n  Specifies the output data schema for the analyzer.\n  \"\"\"\n\n    kind: Literal[\"primary\"] = \"primary\"\n</code></pre> Attributes <code></code> <code>input</code> <code>pydantic-field</code> <p>Specifies the input data schema for the analyzer.</p> <code></code> <code>outputs</code> <code>pydantic-field</code> <p>Specifies the output data schema for the analyzer.</p> <code></code> <code>params = []</code> <code>pydantic-field</code> <p>A list of parameters that the analyzer accepts.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.AnalyzerOutput","title":"<code>AnalyzerOutput</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Fields:</p> <ul> <li> <code>id</code>                 (<code>str</code>)             </li> <li> <code>name</code>                 (<code>str</code>)             </li> <li> <code>description</code>                 (<code>Optional[str]</code>)             </li> <li> <code>columns</code>                 (<code>list[OutputColumn]</code>)             </li> <li> <code>internal</code>                 (<code>bool</code>)             </li> </ul> Source code in <code>analyzer_interface/interface.py</code> <pre><code>class AnalyzerOutput(BaseModel):\n    id: str\n    \"\"\"\n  Uniquely identifies the output data schema for the analyzer. The analyzer\n  must include this key in the output dictionary.\n  \"\"\"\n\n    name: str\n    \"\"\"The human-friendly for the output.\"\"\"\n\n    description: Optional[str] = None\n\n    columns: list[\"OutputColumn\"]\n\n    internal: bool = False\n\n    def get_column_by_name(self, name: str):\n        for column in self.columns:\n            if column.name == name:\n                return column\n        return None\n\n    def transform_output(self, output_df: pl.LazyFrame | pl.DataFrame):\n        output_columns = output_df.lazy().collect_schema().names()\n        return output_df.select(\n            [\n                pl.col(col_name).alias(\n                    output_spec.human_readable_name_or_fallback()\n                    if output_spec\n                    else col_name\n                )\n                for col_name in output_columns\n                if (output_spec := self.get_column_by_name(col_name)) or True\n            ]\n        )\n</code></pre> Attributes <code></code> <code>id</code> <code>pydantic-field</code> <p>Uniquely identifies the output data schema for the analyzer. The analyzer must include this key in the output dictionary.</p> <code></code> <code>name</code> <code>pydantic-field</code> <p>The human-friendly for the output.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.AnalyzerParam","title":"<code>AnalyzerParam</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Fields:</p> <ul> <li> <code>id</code>                 (<code>str</code>)             </li> <li> <code>human_readable_name</code>                 (<code>Optional[str]</code>)             </li> <li> <code>description</code>                 (<code>Optional[str]</code>)             </li> <li> <code>type</code>                 (<code>ParamType</code>)             </li> <li> <code>default</code>                 (<code>Optional[ParamValue]</code>)             </li> <li> <code>backfill_value</code>                 (<code>Optional[ParamValue]</code>)             </li> </ul> Source code in <code>analyzer_interface/interface.py</code> <pre><code>class AnalyzerParam(BaseModel):\n    id: str\n    \"\"\"\n    The name of the parameter. This becomes the key in the parameters dictionary\n    that is passed to the analyzer.\n    \"\"\"\n\n    human_readable_name: Optional[str] = None\n    \"\"\"\n    The human-friendly name for the parameter. This is used in the UI to\n    represent the parameter.\n    \"\"\"\n\n    description: Optional[str] = None\n    \"\"\"\n    A short description of the parameter. This is used in the UI to represent\n    the parameter.\n    \"\"\"\n\n    type: ParamType\n    \"\"\"\n    The type of the parameter. This is used for validation and for customizing\n    the UX for parameter input.\n    \"\"\"\n\n    default: Optional[ParamValue] = None\n    \"\"\"\n    Optional: define a static default value for this parameter. A parameter\n    without a default will need to be chosen explicitly by the user.\n    \"\"\"\n\n    backfill_value: Optional[ParamValue] = None\n    \"\"\"\n    Recommended if this is a parameter that is newly introduced in a previously\n    released analyzer. The backfill is show what this parameter was before it\n    became customizable.\n    \"\"\"\n\n    @property\n    def print_name(self):\n        return self.human_readable_name or self.id\n</code></pre> Attributes <code></code> <code>backfill_value = None</code> <code>pydantic-field</code> <p>Recommended if this is a parameter that is newly introduced in a previously released analyzer. The backfill is show what this parameter was before it became customizable.</p> <code></code> <code>default = None</code> <code>pydantic-field</code> <p>Optional: define a static default value for this parameter. A parameter without a default will need to be chosen explicitly by the user.</p> <code></code> <code>description = None</code> <code>pydantic-field</code> <p>A short description of the parameter. This is used in the UI to represent the parameter.</p> <code></code> <code>human_readable_name = None</code> <code>pydantic-field</code> <p>The human-friendly name for the parameter. This is used in the UI to represent the parameter.</p> <code></code> <code>id</code> <code>pydantic-field</code> <p>The name of the parameter. This becomes the key in the parameters dictionary that is passed to the analyzer.</p> <code></code> <code>type</code> <code>pydantic-field</code> <p>The type of the parameter. This is used for validation and for customizing the UX for parameter input.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.BaseAnalyzerInterface","title":"<code>BaseAnalyzerInterface</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Fields:</p> <ul> <li> <code>id</code>                 (<code>str</code>)             </li> <li> <code>version</code>                 (<code>str</code>)             </li> <li> <code>name</code>                 (<code>str</code>)             </li> <li> <code>short_description</code>                 (<code>str</code>)             </li> <li> <code>long_description</code>                 (<code>Optional[str]</code>)             </li> </ul> Source code in <code>analyzer_interface/interface.py</code> <pre><code>class BaseAnalyzerInterface(BaseModel):\n    id: str\n    \"\"\"\n  The static ID for the analyzer that, with the version, uniquely identifies the\n  analyzer and will be stored as metadata as part of the output data.\n  \"\"\"\n\n    version: str\n    \"\"\"\n  The version ID for the analyzer. In future, we may choose to support output\n  migration between versions of the same analyzer.\n  \"\"\"\n\n    name: str\n    \"\"\"\n  The short human-readable name of the analyzer.\n  \"\"\"\n\n    short_description: str\n    \"\"\"\n  A short, one-liner description of what the analyzer does.\n  \"\"\"\n\n    long_description: Optional[str] = None\n    \"\"\"\n  A longer description of what the analyzer does that will be shown separately.\n  \"\"\"\n</code></pre> Attributes <code></code> <code>id</code> <code>pydantic-field</code> <p>The static ID for the analyzer that, with the version, uniquely identifies the analyzer and will be stored as metadata as part of the output data.</p> <code></code> <code>long_description = None</code> <code>pydantic-field</code> <p>A longer description of what the analyzer does that will be shown separately.</p> <code></code> <code>name</code> <code>pydantic-field</code> <p>The short human-readable name of the analyzer.</p> <code></code> <code>short_description</code> <code>pydantic-field</code> <p>A short, one-liner description of what the analyzer does.</p> <code></code> <code>version</code> <code>pydantic-field</code> <p>The version ID for the analyzer. In future, we may choose to support output migration between versions of the same analyzer.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.DerivedAnalyzerInterface","title":"<code>DerivedAnalyzerInterface</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseAnalyzerInterface</code></p> <p>Fields:</p> <ul> <li> <code>id</code>                 (<code>str</code>)             </li> <li> <code>version</code>                 (<code>str</code>)             </li> <li> <code>name</code>                 (<code>str</code>)             </li> <li> <code>short_description</code>                 (<code>str</code>)             </li> <li> <code>long_description</code>                 (<code>Optional[str]</code>)             </li> <li> <code>base_analyzer</code>                 (<code>AnalyzerInterface</code>)             </li> <li> <code>depends_on</code>                 (<code>list[SecondaryAnalyzerInterface]</code>)             </li> </ul> Source code in <code>analyzer_interface/interface.py</code> <pre><code>class DerivedAnalyzerInterface(BaseAnalyzerInterface):\n    base_analyzer: AnalyzerInterface\n    \"\"\"\n  The base analyzer that this secondary analyzer extends. This is always a primary\n  analyzer. If your module depends on other secondary analyzers (which must have\n  the same base analyzer), you can specify them in the `depends_on` field.\n  \"\"\"\n\n    depends_on: list[\"SecondaryAnalyzerInterface\"] = []\n    \"\"\"\n  A dictionary of secondary analyzers that must be run before the current analyzer\n  secondary analyzer is run. These secondary analyzers must have the same\n  primary base.\n  \"\"\"\n</code></pre> Attributes <code></code> <code>base_analyzer</code> <code>pydantic-field</code> <p>The base analyzer that this secondary analyzer extends. This is always a primary analyzer. If your module depends on other secondary analyzers (which must have the same base analyzer), you can specify them in the <code>depends_on</code> field.</p> <code></code> <code>depends_on = []</code> <code>pydantic-field</code> <p>A dictionary of secondary analyzers that must be run before the current analyzer secondary analyzer is run. These secondary analyzers must have the same primary base.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.InputColumn","title":"<code>InputColumn</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>Column</code></p> <p>Fields:</p> <ul> <li> <code>name</code>                 (<code>str</code>)             </li> <li> <code>human_readable_name</code>                 (<code>Optional[str]</code>)             </li> <li> <code>description</code>                 (<code>Optional[str]</code>)             </li> <li> <code>data_type</code>                 (<code>DataType</code>)             </li> <li> <code>name_hints</code>                 (<code>list[str]</code>)             </li> </ul> Source code in <code>analyzer_interface/interface.py</code> <pre><code>class InputColumn(Column):\n    name_hints: list[str] = []\n    \"\"\"\n  Specifies a list of space-separated words that are likely to be found in the\n  column name of the user-provided data. This is used to help the user map the\n  input columns to the expected columns.\n\n  Any individual hint matching is sufficient for a match to be called. The hint\n  in turn is matched if every word matches some part of the column name.\n  \"\"\"\n</code></pre> Attributes <code></code> <code>name_hints = []</code> <code>pydantic-field</code> <p>Specifies a list of space-separated words that are likely to be found in the column name of the user-provided data. This is used to help the user map the input columns to the expected columns.</p> <p>Any individual hint matching is sufficient for a match to be called. The hint in turn is matched if every word matches some part of the column name.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.interface.SecondaryAnalyzerInterface","title":"<code>SecondaryAnalyzerInterface</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>DerivedAnalyzerInterface</code></p> <p>Fields:</p> <ul> <li> <code>id</code>                 (<code>str</code>)             </li> <li> <code>version</code>                 (<code>str</code>)             </li> <li> <code>name</code>                 (<code>str</code>)             </li> <li> <code>short_description</code>                 (<code>str</code>)             </li> <li> <code>long_description</code>                 (<code>Optional[str]</code>)             </li> <li> <code>base_analyzer</code>                 (<code>AnalyzerInterface</code>)             </li> <li> <code>depends_on</code>                 (<code>list[SecondaryAnalyzerInterface]</code>)             </li> <li> <code>outputs</code>                 (<code>list[AnalyzerOutput]</code>)             </li> <li> <code>kind</code>                 (<code>Literal['secondary']</code>)             </li> </ul> Source code in <code>analyzer_interface/interface.py</code> <pre><code>class SecondaryAnalyzerInterface(DerivedAnalyzerInterface):\n    outputs: list[AnalyzerOutput]\n    \"\"\"\n  Specifies the output data schema for the analyzer.\n  \"\"\"\n\n    kind: Literal[\"secondary\"] = \"secondary\"\n</code></pre> Attributes <code></code> <code>outputs</code> <code>pydantic-field</code> <p>Specifies the output data schema for the analyzer.</p>"},{"location":"reference/analyzer_interface/#analyzer_interface.params","title":"<code>params</code>","text":"<p>Classes:</p> Name Description <code>IntegerParam</code> <p>Represents an integer value</p> <code>TimeBinningParam</code> <p>Represents a time bin.</p> <code>TimeBinningValue</code>"},{"location":"reference/analyzer_interface/#analyzer_interface.params-classes","title":"Classes","text":""},{"location":"reference/analyzer_interface/#analyzer_interface.params.IntegerParam","title":"<code>IntegerParam</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents an integer value</p> <p>The corresponding value will be of type <code>int</code>.</p> <p>Fields:</p> <ul> <li> <code>type</code>                 (<code>Literal['integer']</code>)             </li> <li> <code>min</code>                 (<code>int</code>)             </li> <li> <code>max</code>                 (<code>int</code>)             </li> </ul> Source code in <code>analyzer_interface/params.py</code> <pre><code>class IntegerParam(BaseModel):\n    \"\"\"\n    Represents an integer value\n\n    The corresponding value will be of type `int`.\n    \"\"\"\n\n    type: Literal[\"integer\"] = \"integer\"\n    min: int\n    max: int\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.params.TimeBinningParam","title":"<code>TimeBinningParam</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a time bin.</p> <p>The corresponding value will be of type <code>TimeBinningValue</code>.</p> <p>Fields:</p> <ul> <li> <code>type</code>                 (<code>Literal['time_binning']</code>)             </li> </ul> Source code in <code>analyzer_interface/params.py</code> <pre><code>class TimeBinningParam(BaseModel):\n    \"\"\"\n    Represents a time bin.\n\n    The corresponding value will be of type `TimeBinningValue`.\n    \"\"\"\n\n    type: Literal[\"time_binning\"] = \"time_binning\"\n</code></pre>"},{"location":"reference/analyzer_interface/#analyzer_interface.params.TimeBinningValue","title":"<code>TimeBinningValue</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Config:</p> <ul> <li><code>arbitrary_types_allowed</code>: <code>True</code></li> </ul> <p>Fields:</p> <ul> <li> <code>unit</code>                 (<code>TimeBinningUnit</code>)             </li> <li> <code>amount</code>                 (<code>int</code>)             </li> </ul> Source code in <code>analyzer_interface/params.py</code> <pre><code>class TimeBinningValue(BaseModel):\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    unit: TimeBinningUnit\n    amount: int\n\n    def to_polars_truncate_spec(self) -&gt; str:\n        \"\"\"\n        Converts the value to a string that can be used in Polars truncate spec.\n        See https://docs.pola.rs/api/python/stable/reference/expressions/api/polars.Expr.dt.truncate.html\n        \"\"\"\n        amount = self.amount\n        unit = self.unit\n        if unit == \"year\":\n            return f\"{amount}y\"\n        if unit == \"month\":\n            return f\"{amount}mo\"\n        if unit == \"week\":\n            return f\"{amount}w\"\n        if unit == \"day\":\n            return f\"{amount}d\"\n        if unit == \"hour\":\n            return f\"{amount}h\"\n        if unit == \"minute\":\n            return f\"{amount}m\"\n        if unit == \"second\":\n            return f\"{amount}s\"\n\n        raise ValueError(\"Invalid time binning value\")\n\n    def to_human_readable_text(self) -&gt; str:\n        amount = self.amount\n        unit = self.unit\n\n        if unit == \"year\":\n            return f\"{amount} year{'s' if amount &gt; 1 else ''}\"\n        if unit == \"month\":\n            return f\"{amount} month{'s' if amount &gt; 1 else ''}\"\n        if unit == \"week\":\n            return f\"{amount} week{'s' if amount &gt; 1 else ''}\"\n        if unit == \"day\":\n            return f\"{amount} day{'s' if amount &gt; 1 else ''}\"\n        if unit == \"hour\":\n            return f\"{amount} hour{'s' if amount &gt; 1 else ''}\"\n        if unit == \"minute\":\n            return f\"{amount} minute{'s' if amount &gt; 1 else ''}\"\n        if unit == \"second\":\n            return f\"{amount} second{'s' if amount &gt; 1 else ''}\"\n\n        raise ValueError(\"Invalid time binning value\")\n</code></pre> Functions <code></code> <code>to_polars_truncate_spec()</code> <p>Converts the value to a string that can be used in Polars truncate spec. See https://docs.pola.rs/api/python/stable/reference/expressions/api/polars.Expr.dt.truncate.html</p> Source code in <code>analyzer_interface/params.py</code> <pre><code>def to_polars_truncate_spec(self) -&gt; str:\n    \"\"\"\n    Converts the value to a string that can be used in Polars truncate spec.\n    See https://docs.pola.rs/api/python/stable/reference/expressions/api/polars.Expr.dt.truncate.html\n    \"\"\"\n    amount = self.amount\n    unit = self.unit\n    if unit == \"year\":\n        return f\"{amount}y\"\n    if unit == \"month\":\n        return f\"{amount}mo\"\n    if unit == \"week\":\n        return f\"{amount}w\"\n    if unit == \"day\":\n        return f\"{amount}d\"\n    if unit == \"hour\":\n        return f\"{amount}h\"\n    if unit == \"minute\":\n        return f\"{amount}m\"\n    if unit == \"second\":\n        return f\"{amount}s\"\n\n    raise ValueError(\"Invalid time binning value\")\n</code></pre>"},{"location":"reference/app/","title":"App","text":""},{"location":"reference/app/#app","title":"<code>app</code>","text":"<p>Modules:</p> Name Description <code>logger</code> <p>Application-wide logging system for Mango Tango CLI.</p>"},{"location":"reference/app/#app-modules","title":"Modules","text":""},{"location":"reference/app/#app.logger","title":"<code>logger</code>","text":"<p>Application-wide logging system for Mango Tango CLI.</p> <p>Provides structured JSON logging with: - Console output (ERROR and CRITICAL levels only) to stderr - File output (INFO and above) with automatic rotation - Configurable log levels via CLI flag</p> <p>Classes:</p> Name Description <code>ContextEnrichmentFilter</code> <p>Filter that enriches log records with contextual information.</p> <p>Functions:</p> Name Description <code>get_logger</code> <p>Get a logger instance for the specified module.</p> <code>setup_logging</code> <p>Configure application-wide logging with structured JSON output.</p>"},{"location":"reference/app/#app.logger-classes","title":"Classes","text":""},{"location":"reference/app/#app.logger.ContextEnrichmentFilter","title":"<code>ContextEnrichmentFilter</code>","text":"<p>               Bases: <code>Filter</code></p> <p>Filter that enriches log records with contextual information.</p> <p>Adds: - process_id: Current process ID - thread_id: Current thread ID - app_version: Application version (if available)</p> Source code in <code>app/logger.py</code> <pre><code>class ContextEnrichmentFilter(logging.Filter):\n    \"\"\"\n    Filter that enriches log records with contextual information.\n\n    Adds:\n    - process_id: Current process ID\n    - thread_id: Current thread ID\n    - app_version: Application version (if available)\n    \"\"\"\n\n    def __init__(self, app_version: str = \"unknown\"):\n        super().__init__()\n        self.app_version = app_version\n        self.process_id = os.getpid()\n\n    def filter(self, record: logging.LogRecord) -&gt; bool:\n        # Add contextual information to the log record\n        record.process_id = self.process_id\n        record.thread_id = threading.get_ident()\n        record.app_version = self.app_version\n        return True\n</code></pre>"},{"location":"reference/app/#app.logger-functions","title":"Functions","text":""},{"location":"reference/app/#app.logger.get_logger","title":"<code>get_logger(name)</code>","text":"<p>Get a logger instance for the specified module.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Logger name (typically name)</p> required <p>Returns:</p> Type Description <code>Logger</code> <p>Configured logger instance</p> Source code in <code>app/logger.py</code> <pre><code>def get_logger(name: str) -&gt; logging.Logger:\n    \"\"\"\n    Get a logger instance for the specified module.\n\n    Args:\n        name: Logger name (typically __name__)\n\n    Returns:\n        Configured logger instance\n    \"\"\"\n    return logging.getLogger(name)\n</code></pre>"},{"location":"reference/app/#app.logger.setup_logging","title":"<code>setup_logging(log_file_path, level=logging.INFO, app_version='unknown')</code>","text":"<p>Configure application-wide logging with structured JSON output.</p> <p>Parameters:</p> Name Type Description Default <code>log_file_path</code> <code>Path</code> <p>Path to the log file</p> required <code>level</code> <code>int</code> <p>Minimum logging level (default: logging.INFO)</p> <code>INFO</code> <code>app_version</code> <code>str</code> <p>Application version to include in logs</p> <code>'unknown'</code> Source code in <code>app/logger.py</code> <pre><code>def setup_logging(\n    log_file_path: Path, level: int = logging.INFO, app_version: str = \"unknown\"\n) -&gt; None:\n    \"\"\"\n    Configure application-wide logging with structured JSON output.\n\n    Args:\n        log_file_path: Path to the log file\n        level: Minimum logging level (default: logging.INFO)\n        app_version: Application version to include in logs\n    \"\"\"\n    # Ensure the log directory exists\n    log_file_path.parent.mkdir(parents=True, exist_ok=True)\n\n    # Logging configuration dictionary\n    config: Dict[str, Any] = {\n        \"version\": 1,\n        \"disable_existing_loggers\": False,\n        \"formatters\": {\n            \"json\": {\n                \"()\": \"pythonjsonlogger.jsonlogger.JsonFormatter\",\n                \"format\": \"%(asctime)s %(name)s %(levelname)s %(message)s %(process_id)s %(thread_id)s %(app_version)s\",\n                \"rename_fields\": {\"levelname\": \"level\", \"asctime\": \"timestamp\"},\n            }\n        },\n        \"filters\": {\n            \"context_enrichment\": {\n                \"()\": ContextEnrichmentFilter,\n                \"app_version\": app_version,\n            }\n        },\n        \"handlers\": {\n            \"console\": {\n                \"class\": \"logging.StreamHandler\",\n                \"level\": \"ERROR\",\n                \"formatter\": \"json\",\n                \"filters\": [\"context_enrichment\"],\n                \"stream\": sys.stderr,\n            },\n            \"file\": {\n                \"class\": \"logging.handlers.RotatingFileHandler\",\n                \"level\": level,\n                \"formatter\": \"json\",\n                \"filters\": [\"context_enrichment\"],\n                \"filename\": str(log_file_path),\n                \"maxBytes\": 10485760,  # 10MB\n                \"backupCount\": 5,\n                \"encoding\": \"utf-8\",\n            },\n        },\n        \"root\": {\"level\": level, \"handlers\": [\"console\", \"file\"]},\n        \"loggers\": {\n            # Third-party library loggers - keep them quieter by default\n            \"urllib3\": {\"level\": \"WARNING\", \"propagate\": True},\n            \"requests\": {\"level\": \"WARNING\", \"propagate\": True},\n            \"dash\": {\"level\": \"WARNING\", \"propagate\": True},\n            \"plotly\": {\"level\": \"WARNING\", \"propagate\": True},\n            \"shiny\": {\"level\": \"WARNING\", \"propagate\": True},\n            \"uvicorn\": {\"level\": \"WARNING\", \"propagate\": True},\n            \"starlette\": {\"level\": \"WARNING\", \"propagate\": True},\n            # Application loggers - inherit from root level\n            \"mangotango\": {\"level\": level, \"propagate\": True},\n            \"app\": {\"level\": level, \"propagate\": True},\n            \"analyzers\": {\"level\": level, \"propagate\": True},\n            \"components\": {\"level\": level, \"propagate\": True},\n            \"storage\": {\"level\": level, \"propagate\": True},\n            \"importing\": {\"level\": level, \"propagate\": True},\n        },\n    }\n\n    # Apply the configuration\n    logging.config.dictConfig(config)\n\n    # Set up global exception handler\n    def handle_exception(exc_type, exc_value, exc_traceback):\n        \"\"\"Handle uncaught exceptions by logging them.\"\"\"\n        if issubclass(exc_type, KeyboardInterrupt):\n            # Let KeyboardInterrupt be handled normally\n            sys.__excepthook__(exc_type, exc_value, exc_traceback)\n            return\n\n        logger = logging.getLogger(\"uncaught_exception\")\n        logger.critical(\n            \"Uncaught exception\",\n            exc_info=(exc_type, exc_value, exc_traceback),\n            extra={\n                \"exception_type\": exc_type.__name__,\n                \"exception_message\": str(exc_value),\n            },\n        )\n\n    # Install the global exception handler\n    sys.excepthook = handle_exception\n</code></pre>"},{"location":"reference/components/","title":"Components","text":""},{"location":"reference/components/#components","title":"<code>components</code>","text":"<p>The application's terminal components that will be accessed by the entry module.</p> <p>Modules:</p> Name Description <code>splash</code>"},{"location":"reference/components/#components-modules","title":"Modules","text":""},{"location":"reference/components/#components.splash","title":"<code>splash</code>","text":""},{"location":"reference/importing/","title":"Importing","text":""},{"location":"reference/importing/#importing","title":"<code>importing</code>","text":"<p>Modules:</p> Name Description <code>csv</code> <code>importer</code>"},{"location":"reference/importing/#importing-classes","title":"Classes","text":""},{"location":"reference/importing/#importing-modules","title":"Modules","text":""},{"location":"reference/importing/#importing.csv","title":"<code>csv</code>","text":"<p>Classes:</p> Name Description <code>CSVImporter</code>"},{"location":"reference/importing/#importing.csv-classes","title":"Classes","text":""},{"location":"reference/importing/#importing.csv.CSVImporter","title":"<code>CSVImporter</code>","text":"<p>               Bases: <code>Importer['CsvImportSession']</code></p> Source code in <code>importing/csv.py</code> <pre><code>class CSVImporter(Importer[\"CsvImportSession\"]):\n    @property\n    def name(self) -&gt; str:\n        return \"CSV\"\n\n    def suggest(self, input_path: str) -&gt; bool:\n        return input_path.endswith(\".csv\")\n\n    def _detect_skip_rows_and_dialect(self, input_path: str) -&gt; tuple[int, csv.Dialect]:\n        \"\"\"Detect the number of rows to skip before CSV data begins and the CSV dialect.\"\"\"\n        skip_rows = 0\n\n        try:\n            with open(input_path, \"r\", encoding=\"utf8\") as file:\n\n                MAX_LINES = 50  # check the first 50 lines only\n                lines = [line.strip() for i, line in enumerate(file) if i &lt;= MAX_LINES]\n                total_lines = len(lines)\n\n                # Only analyze if we have enough lines\n                if len(lines) &gt;= 2:\n                    # Parse each line and analyze content, keeping track of original line numbers\n                    parsed_rows = []\n                    line_numbers = []\n                    for line_idx, line in enumerate(lines):\n                        if not line:  # Skip empty lines\n                            continue\n                        try:\n                            reader = csv.reader([line])\n                            row = next(reader)\n                            parsed_rows.append(row)\n                            line_numbers.append(line_idx)\n                        except Exception:\n                            parsed_rows.append([line])  # Fallback for problematic lines\n                            line_numbers.append(line_idx)\n\n                    if len(parsed_rows) &gt;= 2:\n                        # Look for the actual CSV header (column names)\n                        for i, row in enumerate(parsed_rows):\n                            if self._looks_like_csv_header(row):\n                                skip_rows = line_numbers[i]\n                                break\n                        else:\n                            # Fallback: use field count analysis\n                            field_counts = [len(row) for row in parsed_rows]\n                            from collections import Counter\n\n                            count_frequency = Counter(field_counts)\n                            most_common_count = count_frequency.most_common(1)[0][0]\n\n                            # Find first row that matches the most common field count\n                            for i, count in enumerate(field_counts):\n                                if count == most_common_count:\n                                    skip_rows = line_numbers[i]\n                                    break\n\n                # Validate skip_rows doesn't exceed available lines\n                if skip_rows &gt;= total_lines:\n                    skip_rows = 0  # Reset to safe default\n\n                # Now detect dialect from the CSV content (after skip_rows)\n                file.seek(0)\n                for _ in range(skip_rows):\n                    file.readline()\n\n                sample = file.read(65536)\n                dialect = Sniffer().sniff(sample)\n\n        except Exception:\n            # If anything fails, use defaults and try basic dialect detection\n            skip_rows = 0\n            try:\n                with open(input_path, \"r\", encoding=\"utf8\") as file:\n                    sample = file.read(65536)\n                    dialect = Sniffer().sniff(sample)\n            except Exception:\n                # Create a default dialect if everything fails\n                class DefaultDialect:\n                    delimiter = \",\"\n                    quotechar = '\"'\n\n                dialect = DefaultDialect()\n\n        return skip_rows, dialect\n\n    def _looks_like_csv_header(self, row: list[str]) -&gt; bool:\n        \"\"\"Check if a row looks like a CSV header with column names.\"\"\"\n        if not row or len(row) &lt; 2:\n            return False\n\n        # Skip rows where most fields are empty (likely CSV notes with trailing commas)\n        non_empty_fields = [field.strip() for field in row if field.strip()]\n        if len(non_empty_fields) &lt; len(row) // 2:\n            return False\n\n        # Look for typical CSV header characteristics\n        header_indicators = 0\n\n        for field in non_empty_fields:\n            field = field.lower().strip()\n\n            # Common column name patterns\n            if any(\n                word in field\n                for word in [\n                    \"id\",\n                    \"name\",\n                    \"date\",\n                    \"time\",\n                    \"user\",\n                    \"tweet\",\n                    \"text\",\n                    \"count\",\n                    \"number\",\n                    \"sent\",\n                    \"screen\",\n                    \"retweeted\",\n                    \"favorited\",\n                ]\n            ):\n                header_indicators += 1\n\n            # Short descriptive column names (not long sentences like CSV notes)\n            if 3 &lt;= len(field) &lt;= 30 and not field.startswith(\n                (\"http\", \"www\", \"from \", \"if you\")\n            ):\n                header_indicators += 1\n\n        # Consider it a CSV header if at least 50% of non-empty fields look like column names\n        return header_indicators &gt;= len(non_empty_fields) * 0.5\n\n    def init_session(self, input_path: str):\n        skip_rows, dialect = self._detect_skip_rows_and_dialect(input_path)\n\n        return CsvImportSession(\n            input_file=input_path,\n            separator=dialect.delimiter,\n            quote_char=dialect.quotechar,\n            has_header=True,\n            skip_rows=skip_rows,\n        )\n\n    def manual_init_session(self, input_path: str):\n        separator = self._separator_option(None)\n        if separator is None:\n            return None\n\n        quote_char = self._quote_char_option(None)\n        if quote_char is None:\n            return None\n\n        has_header = self._header_option(None)\n        if has_header is None:\n            return None\n\n        skip_rows = self._skip_rows_option(None)\n        if skip_rows is None:\n            return None\n\n        return CsvImportSession(\n            input_file=input_path,\n            separator=separator,\n            quote_char=quote_char,\n            has_header=has_header,\n            skip_rows=skip_rows,\n        )\n\n    def modify_session(\n        self,\n        input_path: str,\n        import_session: \"CsvImportSession\",\n        reset_screen: Callable[[], None],\n    ):\n        is_first_time = True\n        while True:\n            reset_screen(import_session)\n            action = prompts.list_input(\n                \"What would you like to change?\",\n                choices=[\n                    (\"Column separator\", \"separator\"),\n                    (\"Quote character\", \"quote_char\"),\n                    (\"Header\", \"header\"),\n                    (\"Skip rows\", \"skip_rows\"),\n                    (\"Done. Use these options.\", \"done\"),\n                ],\n                default=None if is_first_time else \"done\",\n            )\n            is_first_time = False\n            if action is None:\n                return None\n\n            if action == \"done\":\n                return import_session\n\n            if action == \"separator\":\n                separator = self._separator_option(import_session.separator)\n                if separator is None:\n                    continue\n                import_session.separator = separator\n\n            if action == \"quote_char\":\n                quote_char = self._quote_char_option(import_session.quote_char)\n                if quote_char is None:\n                    continue\n                import_session.quote_char = quote_char\n\n            if action == \"header\":\n                has_header = self._header_option(import_session.has_header)\n                if has_header is None:\n                    continue\n                import_session.has_header = has_header\n\n            if action == \"skip_rows\":\n                skip_rows = self._skip_rows_option(import_session.skip_rows)\n                if skip_rows is None:\n                    continue\n                import_session.skip_rows = skip_rows\n\n    @staticmethod\n    def _separator_option(previous_value: Optional[str]) -&gt; Optional[str]:\n        input: Optional[str] = prompts.list_input(\n            \"Select the column separator\",\n            choices=[\n                (\"comma (,)\", \",\"),\n                (\"semicolon (;)\", \";\"),\n                (\"Pipe (|)\", \"|\"),\n                (\"Tab\", \"\\t\"),\n                (\"Other\", \"other\"),\n            ],\n            default=(\n                previous_value\n                if previous_value in [\",\", \";\", \"\\t\"]\n                else \"other\" if previous_value is not None else None\n            ),\n        )\n        if input is None:\n            return None\n        if input != \"other\":\n            return input\n\n        input = prompts.text(\"Enter the separator\")\n        if input is None:\n            return None\n        input = input.strip()\n        if len(input) == 0:\n            return None\n\n    @staticmethod\n    def _quote_char_option(previous_value: Optional[str]) -&gt; Optional[str]:\n        input: Optional[str] = prompts.list_input(\n            \"Select the quote character\",\n            choices=[\n                ('Double quote (\")', '\"'),\n                (\"Single quote (')\", \"'\"),\n                (\"Other\", \"other\"),\n            ],\n            default=(\n                previous_value\n                if previous_value in ['\"', \"'\"]\n                else \"other\" if previous_value is not None else None\n            ),\n        )\n        if input is None:\n            return None\n        if input != \"other\":\n            return input\n\n        input = prompts.text(\"Enter the quote character\")\n        if input is None:\n            return None\n        input = input.strip()\n        if len(input) == 0:\n            return None\n\n    def _header_option(self, previous_value: Optional[bool]) -&gt; Optional[bool]:\n        return prompts.list_input(\n            \"Does the file have a header?\",\n            choices=[\n                (\"Yes\", True),\n                (\"No\", False),\n            ],\n            default=previous_value,\n        )\n\n    @staticmethod\n    def _skip_rows_option(previous_value: Optional[int]) -&gt; Optional[int]:\n        while True:\n            input_str = prompts.text(\n                f\"Number of rows to skip at the beginning of file (current: {previous_value or 0}).\",\n                default=str(previous_value) if previous_value is not None else \"0\",\n            )\n            if input_str is None:  # User cancelled\n                return None\n\n            try:\n                skip_rows = int(input_str.strip())\n                if skip_rows &lt; 0:\n                    print_message(\n                        \"Skip rows cannot be negative. Please try again.\", \"error\"\n                    )\n                    continue\n                if skip_rows &gt; 10:\n                    confirm = prompts.confirm(\n                        f\"Skip {skip_rows} rows? This seems high. Continue?\",\n                        default=True,\n                    )\n                    if not confirm:\n                        continue  # Ask for input again instead of returning None\n                return skip_rows\n            except ValueError:\n                print_message(\"Please enter a valid number.\", \"error\")\n                continue\n</code></pre>"},{"location":"reference/importing/#importing.importer","title":"<code>importer</code>","text":"<p>Classes:</p> Name Description <code>Importer</code> <code>ImporterSession</code> <p>The ImporterSession interface handles the ongoing configuration of an import.</p>"},{"location":"reference/importing/#importing.importer-classes","title":"Classes","text":""},{"location":"reference/importing/#importing.importer.Importer","title":"<code>Importer</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Methods:</p> Name Description <code>init_session</code> <p>Produces an initial import session object that contains all the configuration</p> <code>modify_session</code> <p>Performs the interactive UI sequence that customizes the import session</p> <code>suggest</code> <p>Check if the importer can handle the given file. This should be fairly</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the importer. It will be quoted in the UI in texts such as</p> Source code in <code>importing/importer.py</code> <pre><code>class Importer[SessionType](ABC):\n    @property\n    @abstractmethod\n    def name(self) -&gt; str:\n        \"\"\"\n        The name of the importer. It will be quoted in the UI in texts such as\n        \"Imported as `name`, so keep it to a format name.\"\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def suggest(self, input_path: str) -&gt; bool:\n        \"\"\"\n        Check if the importer can handle the given file. This should be fairly\n        restrictive based on reasonable assumptions, as it is only used for the\n        initial importer suggestion. The user can always override the suggestion.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def init_session(self, input_path: str) -&gt; Optional[SessionType]:\n        \"\"\"\n        Produces an initial import session object that contains all the configuration\n        needed for the import. The user can either accept this configuration or\n        customize it.\n\n        Return None here if the importer cannot figure out how to configure the\n        import parameters. This doesn't necessarily mean that the file cannot be\n        loaded; the UI will force the user to customize the import session if the\n        user wants to proceed with this importer.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def manual_init_session(self, input_path: str) -&gt; Optional[SessionType]:\n        pass\n\n    @abstractmethod\n    def modify_session(\n        self,\n        input_path: str,\n        import_session: SessionType,\n        reset_screen: Callable[[SessionType], None],\n    ) -&gt; Optional[SessionType]:\n        \"\"\"\n        Performs the interactive UI sequence that customizes the import session\n        from the initial one.\n\n        Return None here if the user interrupts the customization process.\n        \"\"\"\n        pass\n</code></pre> Attributes <code></code> <code>name</code> <code>abstractmethod</code> <code>property</code> <p>The name of the importer. It will be quoted in the UI in texts such as \"Imported as <code>name</code>, so keep it to a format name.\"</p> Functions <code></code> <code>init_session(input_path)</code> <code>abstractmethod</code> <p>Produces an initial import session object that contains all the configuration needed for the import. The user can either accept this configuration or customize it.</p> <p>Return None here if the importer cannot figure out how to configure the import parameters. This doesn't necessarily mean that the file cannot be loaded; the UI will force the user to customize the import session if the user wants to proceed with this importer.</p> Source code in <code>importing/importer.py</code> <pre><code>@abstractmethod\ndef init_session(self, input_path: str) -&gt; Optional[SessionType]:\n    \"\"\"\n    Produces an initial import session object that contains all the configuration\n    needed for the import. The user can either accept this configuration or\n    customize it.\n\n    Return None here if the importer cannot figure out how to configure the\n    import parameters. This doesn't necessarily mean that the file cannot be\n    loaded; the UI will force the user to customize the import session if the\n    user wants to proceed with this importer.\n    \"\"\"\n    pass\n</code></pre> <code></code> <code>modify_session(input_path, import_session, reset_screen)</code> <code>abstractmethod</code> <p>Performs the interactive UI sequence that customizes the import session from the initial one.</p> <p>Return None here if the user interrupts the customization process.</p> Source code in <code>importing/importer.py</code> <pre><code>@abstractmethod\ndef modify_session(\n    self,\n    input_path: str,\n    import_session: SessionType,\n    reset_screen: Callable[[SessionType], None],\n) -&gt; Optional[SessionType]:\n    \"\"\"\n    Performs the interactive UI sequence that customizes the import session\n    from the initial one.\n\n    Return None here if the user interrupts the customization process.\n    \"\"\"\n    pass\n</code></pre> <code></code> <code>suggest(input_path)</code> <code>abstractmethod</code> <p>Check if the importer can handle the given file. This should be fairly restrictive based on reasonable assumptions, as it is only used for the initial importer suggestion. The user can always override the suggestion.</p> Source code in <code>importing/importer.py</code> <pre><code>@abstractmethod\ndef suggest(self, input_path: str) -&gt; bool:\n    \"\"\"\n    Check if the importer can handle the given file. This should be fairly\n    restrictive based on reasonable assumptions, as it is only used for the\n    initial importer suggestion. The user can always override the suggestion.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/importing/#importing.importer.ImporterSession","title":"<code>ImporterSession</code>","text":"<p>               Bases: <code>ABC</code></p> <p>The ImporterSession interface handles the ongoing configuration of an import. It keeps the configuration state, knows how to print the configuration to the console, and can load a preview of the data from the input file.</p> <p>Methods:</p> Name Description <code>import_as_parquet</code> <p>Import the data from the input file to the output file in the Parquet format.</p> <code>load_preview</code> <p>Attempt to load a preview of the data from the input file.</p> <code>print_config</code> <p>Print the configuration of the import session to the console.</p> Source code in <code>importing/importer.py</code> <pre><code>class ImporterSession(ABC):\n    \"\"\"\n    The ImporterSession interface handles the ongoing configuration of an import.\n    It keeps the configuration state, knows how to print the configuration to the\n    console, and can load a preview of the data from the input file.\n    \"\"\"\n\n    @abstractmethod\n    def print_config(self) -&gt; None:\n        \"\"\"\n        Print the configuration of the import session to the console.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def load_preview(self, n_records: int) -&gt; Optional[pl.DataFrame]:\n        \"\"\"\n        Attempt to load a preview of the data from the input file.\n\n        Return None here if it is sure that the file cannot be loaded with the current\n        configuration. Only throw an execption in the case of unexpected errors.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def import_as_parquet(self, output_path: str) -&gt; None:\n        \"\"\"\n        Import the data from the input file to the output file in the Parquet format.\n        \"\"\"\n        pass\n</code></pre> Functions <code></code> <code>import_as_parquet(output_path)</code> <code>abstractmethod</code> <p>Import the data from the input file to the output file in the Parquet format.</p> Source code in <code>importing/importer.py</code> <pre><code>@abstractmethod\ndef import_as_parquet(self, output_path: str) -&gt; None:\n    \"\"\"\n    Import the data from the input file to the output file in the Parquet format.\n    \"\"\"\n    pass\n</code></pre> <code></code> <code>load_preview(n_records)</code> <code>abstractmethod</code> <p>Attempt to load a preview of the data from the input file.</p> <p>Return None here if it is sure that the file cannot be loaded with the current configuration. Only throw an execption in the case of unexpected errors.</p> Source code in <code>importing/importer.py</code> <pre><code>@abstractmethod\ndef load_preview(self, n_records: int) -&gt; Optional[pl.DataFrame]:\n    \"\"\"\n    Attempt to load a preview of the data from the input file.\n\n    Return None here if it is sure that the file cannot be loaded with the current\n    configuration. Only throw an execption in the case of unexpected errors.\n    \"\"\"\n    pass\n</code></pre> <code></code> <code>print_config()</code> <code>abstractmethod</code> <p>Print the configuration of the import session to the console.</p> Source code in <code>importing/importer.py</code> <pre><code>@abstractmethod\ndef print_config(self) -&gt; None:\n    \"\"\"\n    Print the configuration of the import session to the console.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/meta/","title":"Meta","text":""},{"location":"reference/meta/#meta","title":"<code>meta</code>","text":""},{"location":"reference/preprocessing/","title":"Preprocessing","text":""},{"location":"reference/preprocessing/#preprocessing","title":"<code>preprocessing</code>","text":"<p>Modules:</p> Name Description <code>series_semantic</code>"},{"location":"reference/preprocessing/#preprocessing-modules","title":"Modules","text":""},{"location":"reference/preprocessing/#preprocessing.series_semantic","title":"<code>series_semantic</code>","text":"<p>Functions:</p> Name Description <code>constant_series</code> <p>Create a series with a constant value for each row of <code>series</code>.</p> <code>parse_datetime_with_tz</code> <p>Parse datetime strings with timezone info (both abbreviations and offsets)</p> <code>parse_time_military</code> <p>Parse time strings with multiple format attempts</p>"},{"location":"reference/preprocessing/#preprocessing.series_semantic-attributes","title":"Attributes","text":""},{"location":"reference/preprocessing/#preprocessing.series_semantic-functions","title":"Functions","text":""},{"location":"reference/preprocessing/#preprocessing.series_semantic.constant_series","title":"<code>constant_series(series, constant)</code>","text":"<p>Create a series with a constant value for each row of <code>series</code>.</p> Source code in <code>preprocessing/series_semantic.py</code> <pre><code>def constant_series(series: pl.Series, constant) -&gt; pl.Series:\n    \"\"\"Create a series with a constant value for each row of `series`.\"\"\"\n    return pl.Series([constant] * series.len(), dtype=pl.Boolean)\n</code></pre>"},{"location":"reference/preprocessing/#preprocessing.series_semantic.parse_datetime_with_tz","title":"<code>parse_datetime_with_tz(s)</code>","text":"<p>Parse datetime strings with timezone info (both abbreviations and offsets)</p> Source code in <code>preprocessing/series_semantic.py</code> <pre><code>def parse_datetime_with_tz(s: pl.Series) -&gt; pl.Series:\n    \"\"\"Parse datetime strings with timezone info (both abbreviations and offsets)\"\"\"\n    import warnings\n\n    # Handle timezone abbreviations like \"UTC\", \"EST\"\n    tz_abbrev_regex = r\" ([A-Z]{3,4})$\"  # UTC, EST, etc.\n\n    # Handle timezone offsets like \"-05:00\", \"+00:00\"\n    tz_offset_regex = r\"[+-]\\d{2}:\\d{2}$\"  # -05:00, +00:00, etc.\n\n    # Check for multiple different timezones\n    abbrev_matches = s.str.extract_all(tz_abbrev_regex)\n    offset_matches = s.str.extract_all(tz_offset_regex)\n\n    # Get unique timezone abbreviations\n    unique_abbrevs = set()\n    if not abbrev_matches.is_empty():\n        for match_list in abbrev_matches.to_list():\n            if match_list:  # Not empty\n                unique_abbrevs.update(match_list)\n\n    # Get unique timezone offsets\n    unique_offsets = set()\n    if not offset_matches.is_empty():\n        for match_list in offset_matches.to_list():\n            if match_list:  # Not empty\n                unique_offsets.update(match_list)\n\n    # Warn if multiple different timezones found\n    total_unique_tz = len(unique_abbrevs) + len(unique_offsets)\n    if total_unique_tz &gt; 1:\n        all_tz = list(unique_abbrevs) + list(unique_offsets)\n        warnings.warn(\n            f\"Multiple timezones found in datetime column: {all_tz}. \"\n            f\"Assuming all timestamps represent the same timezone for analysis purposes.\",\n            UserWarning,\n        )\n\n    # Try to remove timezone abbreviations first\n    result = s.str.replace(tz_abbrev_regex, \"\")\n\n    # Then remove timezone offsets\n    result = result.str.replace(tz_offset_regex, \"\")\n\n    return result.str.strptime(pl.Datetime(), strict=False)\n</code></pre>"},{"location":"reference/preprocessing/#preprocessing.series_semantic.parse_time_military","title":"<code>parse_time_military(s)</code>","text":"<p>Parse time strings with multiple format attempts</p> Source code in <code>preprocessing/series_semantic.py</code> <pre><code>def parse_time_military(s: pl.Series) -&gt; pl.Series:\n    \"\"\"Parse time strings with multiple format attempts\"\"\"\n    # Try different time formats\n    FORMATS_TO_TRY = [\"%H:%M:%S\", \"%H:%M\", \"%I:%M:%S %p\", \"%I:%M %p\"]\n\n    for fmt in FORMATS_TO_TRY:\n        try:\n            result = s.str.strptime(pl.Time, format=fmt, strict=False)\n            if result.is_not_null().sum() &gt; 0:  # If any parsed successfully\n                return result\n        except:\n            continue\n\n    # If all formats fail, return nulls\n    return pl.Series([None] * s.len(), dtype=pl.Time)\n</code></pre>"},{"location":"reference/storage/","title":"Storage","text":""},{"location":"reference/storage/#storage","title":"<code>storage</code>","text":"<p>Classes:</p> Name Description <code>Storage</code>"},{"location":"reference/storage/#storage-classes","title":"Classes","text":""},{"location":"reference/storage/#storage.Storage","title":"<code>Storage</code>","text":"Source code in <code>storage/__init__.py</code> <pre><code>class Storage:\n    def __init__(self, *, app_name: str, app_author: str):\n        self.user_data_dir = platformdirs.user_data_dir(\n            appname=app_name, appauthor=app_author, ensure_exists=True\n        )\n        self.temp_dir = platformdirs.user_cache_dir(\n            appname=app_name, appauthor=app_author, ensure_exists=True\n        )\n        self.db = TinyDB(self._get_db_path())\n        with self._lock_database():\n            self._bootstrap_analyses_v1()\n\n        self.file_selector_state = AppFileSelectorStateManager(self)\n\n    def init_project(self, *, display_name: str, input_temp_file: str):\n        with self._lock_database():\n            project_id = self._find_unique_project_id(display_name)\n            project = ProjectModel(id=project_id, display_name=display_name)\n            self.db.insert(project.model_dump())\n\n        project_dir = self._get_project_path(project_id)\n        os.makedirs(project_dir, exist_ok=True)\n\n        shutil.move(input_temp_file, self._get_project_input_path(project_id))\n        return project\n\n    def list_projects(self):\n        q = Query()\n        projects = self.db.search(q[\"class_\"] == \"project\")\n        return sorted(\n            (ProjectModel(**project) for project in projects),\n            key=lambda project: project.display_name,\n        )\n\n    def get_project(self, project_id: str):\n        q = Query()\n        project = self.db.search((q[\"class_\"] == \"project\") &amp; (q[\"id\"] == project_id))\n        if project:\n            return ProjectModel(**project[0])\n        return None\n\n    def delete_project(self, project_id: str):\n        with self._lock_database():\n            q = Query()\n            self.db.remove((q[\"id\"] == project_id) &amp; (q[\"class_\"] == \"project\"))\n        project_path = self._get_project_path(project_id)\n        shutil.rmtree(project_path, ignore_errors=True)\n\n    def rename_project(self, project_id: str, name: str):\n        with self._lock_database():\n            q = Query()\n            self.db.update(\n                {\"display_name\": name},\n                (q[\"id\"] == project_id) &amp; (q[\"class_\"] == \"project\"),\n            )\n\n    def load_project_input(self, project_id: str, *, n_records: Optional[int] = None):\n        input_path = self._get_project_input_path(project_id)\n        return pl.read_parquet(input_path, n_rows=n_records)\n\n    def get_project_input_stats(self, project_id: str):\n        input_path = self._get_project_input_path(project_id)\n        num_rows = pl.scan_parquet(input_path).select(pl.count()).collect().item()\n        return TableStats(num_rows=num_rows)\n\n    def save_project_primary_outputs(\n        self, analysis: AnalysisModel, outputs: dict[str, pl.DataFrame]\n    ):\n        for output_id, output_df in outputs.items():\n            self._save_output(\n                os.path.join(\n                    self._get_project_primary_output_root_path(analysis),\n                    output_id,\n                ),\n                output_df,\n                \"parquet\",\n            )\n\n    def save_project_secondary_outputs(\n        self,\n        analysis: AnalysisModel,\n        secondary_id: str,\n        outputs: dict[str, pl.DataFrame],\n    ):\n        for output_id, output_df in outputs.items():\n            self._save_output(\n                os.path.join(\n                    self._get_project_secondary_output_root_path(\n                        analysis, secondary_id\n                    ),\n                    output_id,\n                ),\n                output_df,\n                \"parquet\",\n            )\n\n    def save_project_secondary_output(\n        self,\n        analysis: AnalysisModel,\n        secondary_id: str,\n        output_id: str,\n        output_df: pl.DataFrame,\n        extension: SupportedOutputExtension,\n    ):\n        root_path = self._get_project_secondary_output_root_path(analysis, secondary_id)\n        self._save_output(\n            os.path.join(root_path, output_id),\n            output_df,\n            extension,\n        )\n\n    def _save_output(\n        self,\n        output_path_without_extension,\n        output_df: pl.DataFrame | pl.LazyFrame,\n        extension: SupportedOutputExtension,\n    ):\n        output_df = output_df.lazy()\n        os.makedirs(os.path.dirname(output_path_without_extension), exist_ok=True)\n        output_path = f\"{output_path_without_extension}.{extension}\"\n        if extension == \"parquet\":\n            output_df.sink_parquet(output_path)\n        elif extension == \"csv\":\n            output_df.sink_csv(output_path)\n        elif extension == \"xlsx\":\n            # See https://xlsxwriter.readthedocs.io/working_with_dates_and_time.html#timezone-handling\n            with Workbook(output_path, {\"remove_timezone\": True}) as workbook:\n                output_df.collect().write_excel(workbook)\n        elif extension == \"json\":\n            output_df.collect().write_json(output_path)\n        else:\n            raise ValueError(f\"Unsupported format: {extension}\")\n        return output_path\n\n    def load_project_primary_output(self, analysis: AnalysisModel, output_id: str):\n        output_path = self.get_primary_output_parquet_path(analysis, output_id)\n        return pl.read_parquet(output_path)\n\n    def get_primary_output_parquet_path(self, analysis: AnalysisModel, output_id: str):\n        return os.path.join(\n            self._get_project_primary_output_root_path(analysis),\n            f\"{output_id}.parquet\",\n        )\n\n    def load_project_secondary_output(\n        self, analysis: AnalysisModel, secondary_id: str, output_id: str\n    ):\n        output_path = self.get_secondary_output_parquet_path(\n            analysis, secondary_id, output_id\n        )\n        return pl.read_parquet(output_path)\n\n    def get_secondary_output_parquet_path(\n        self, analysis: AnalysisModel, secondary_id: str, output_id: str\n    ):\n        return os.path.join(\n            self._get_project_secondary_output_root_path(analysis, secondary_id),\n            f\"{output_id}.parquet\",\n        )\n\n    def export_project_primary_output(\n        self,\n        analysis: AnalysisModel,\n        output_id: str,\n        *,\n        extension: SupportedOutputExtension,\n        spec: AnalyzerOutput,\n        export_chunk_size: Optional[int] = None,\n    ):\n        return self._export_output(\n            self.get_primary_output_parquet_path(analysis, output_id),\n            os.path.join(self._get_project_exports_root_path(analysis), output_id),\n            extension=extension,\n            spec=spec,\n            export_chunk_size=export_chunk_size,\n        )\n\n    def export_project_secondary_output(\n        self,\n        analysis: AnalysisModel,\n        secondary_id: str,\n        output_id: str,\n        *,\n        extension: SupportedOutputExtension,\n        spec: AnalyzerOutput,\n        export_chunk_size: Optional[int] = None,\n    ):\n        exported_path = os.path.join(\n            self._get_project_exports_root_path(analysis),\n            (\n                secondary_id\n                if secondary_id == output_id\n                else f\"{secondary_id}__{output_id}\"\n            ),\n        )\n        return self._export_output(\n            self.get_secondary_output_parquet_path(analysis, secondary_id, output_id),\n            exported_path,\n            extension=extension,\n            spec=spec,\n            export_chunk_size=export_chunk_size,\n        )\n\n    def _export_output(\n        self,\n        input_path: str,\n        output_path: str,\n        *,\n        extension: SupportedOutputExtension,\n        spec: AnalyzerOutput,\n        export_chunk_size: Optional[int] = None,\n    ):\n        with pq.ParquetFile(input_path) as reader:\n            num_chunks = (\n                math.ceil(reader.metadata.num_rows / export_chunk_size)\n                if export_chunk_size\n                else 1\n            )\n\n        if num_chunks == 1:\n            df = pl.scan_parquet(input_path)\n            self._save_output(output_path, spec.transform_output(df), extension)\n            return f\"{output_path}.{extension}\"\n\n        with pq.ParquetFile(input_path) as reader:\n            get_batches = (\n                df\n                for batch in reader.iter_batches()\n                if (df := pl.from_arrow(batch)) is not None\n            )\n            for chunk_id, chunk in enumerate(\n                collect_dataframe_chunks(get_batches, export_chunk_size)\n            ):\n                chunk = spec.transform_output(chunk)\n                self._save_output(f\"{output_path}_{chunk_id}\", chunk, extension)\n                yield chunk_id / num_chunks\n            return f\"{output_path}_[*].{extension}\"\n\n    def list_project_analyses(self, project_id: str):\n        with self._lock_database():\n            q = Query()\n            analysis_models = self.db.search(\n                (q[\"class_\"] == \"analysis\") &amp; (q[\"project_id\"] == project_id)\n            )\n        return [AnalysisModel(**analysis) for analysis in analysis_models]\n\n    def init_analysis(\n        self,\n        project_id: str,\n        display_name: str,\n        primary_analyzer_id: str,\n        column_mapping: dict[str, str],\n        param_values: dict[str, ParamValue],\n    ) -&gt; AnalysisModel:\n        with self._lock_database():\n            analysis_id = self._find_unique_analysis_id(project_id, display_name)\n            analysis = AnalysisModel(\n                analysis_id=analysis_id,\n                project_id=project_id,\n                display_name=display_name,\n                primary_analyzer_id=primary_analyzer_id,\n                path=os.path.join(\"analysis\", analysis_id),\n                column_mapping=column_mapping,\n                create_timestamp=datetime.now().timestamp(),\n                param_values=param_values,\n                is_draft=True,\n            )\n            self.db.insert(analysis.model_dump())\n        return analysis\n\n    def save_analysis(self, analysis: AnalysisModel):\n        with self._lock_database():\n            q = Query()\n            self.db.update(\n                analysis.model_dump(),\n                (q[\"class_\"] == \"analysis\")\n                &amp; (q[\"project_id\"] == analysis.project_id)\n                &amp; (q[\"analysis_id\"] == analysis.analysis_id),\n            )\n\n    def delete_analysis(self, analysis: AnalysisModel):\n        with self._lock_database():\n            q = Query()\n            self.db.remove(\n                (q[\"class_\"] == \"analysis\")\n                &amp; (q[\"project_id\"] == analysis.project_id)\n                &amp; (q[\"analysis_id\"] == analysis.analysis_id)\n            )\n            analysis_path = os.path.join(\n                self._get_project_path(analysis.project_id), analysis.path\n            )\n            shutil.rmtree(analysis_path, ignore_errors=True)\n\n    def _find_unique_analysis_id(self, project_id: str, display_name: str):\n        return self._get_unique_name(\n            self._slugify_name(display_name),\n            lambda analysis_id: self._is_analysis_id_unique(project_id, analysis_id),\n        )\n\n    def _is_analysis_id_unique(self, project_id: str, analysis_id: str):\n        q = Query()\n        id_unique = not self.db.search(\n            (q[\"class_\"] == \"analysis\")\n            &amp; (q[\"project_id\"] == project_id)\n            &amp; (q[\"analysis_id\"] == analysis_id)\n        )\n        dir_unique = not os.path.exists(\n            os.path.join(self._get_project_path(project_id), \"analysis\", analysis_id)\n        )\n        return id_unique and dir_unique\n\n    def _bootstrap_analyses_v1(self):\n        legacy_v1_analysis_dirname = \"analyzers\"\n        projects = self.list_projects()\n        for project in projects:\n            project_id = project.id\n            project_path = self._get_project_path(project_id)\n            try:\n                v1_analyses = os.listdir(\n                    os.path.join(project_path, legacy_v1_analysis_dirname)\n                )\n            except FileNotFoundError:\n                continue\n            for analyzer_id in v1_analyses:\n                db_analyzer_id = f\"__v1__{analyzer_id}\"\n                modified_time = os.path.getmtime(\n                    os.path.join(project_path, legacy_v1_analysis_dirname, analyzer_id)\n                )\n                self.db.upsert(\n                    AnalysisModel(\n                        analysis_id=db_analyzer_id,\n                        project_id=project_id,\n                        display_name=analyzer_id,\n                        primary_analyzer_id=analyzer_id,\n                        path=os.path.join(legacy_v1_analysis_dirname, analyzer_id),\n                        create_timestamp=modified_time,\n                    ).model_dump(),\n                    (Query()[\"class_\"] == \"analysis\")\n                    &amp; (Query()[\"project_id\"] == project_id)\n                    &amp; (Query()[\"analysis_id\"] == db_analyzer_id),\n                )\n\n    def list_secondary_analyses(self, analysis: AnalysisModel) -&gt; list[str]:\n        try:\n            analyzers = os.listdir(\n                os.path.join(\n                    self._get_project_path(analysis.project_id),\n                    analysis.path,\n                    \"secondary_outputs\",\n                ),\n            )\n            return analyzers\n        except FileNotFoundError:\n            return []\n\n    def _find_unique_project_id(self, display_name: str):\n        \"\"\"Turn the display name into a unique project ID\"\"\"\n        return self._get_unique_name(\n            self._slugify_name(display_name), self._is_project_id_unique\n        )\n\n    def _is_project_id_unique(self, project_id: str):\n        \"\"\"Check the database if the project ID is unique\"\"\"\n        q = Query()\n        id_unique = not self.db.search(\n            q[\"class_\"] == \"project\" and q[\"id\"] == project_id\n        )\n        dir_unique = not os.path.exists(self._get_project_path(project_id))\n        return id_unique and dir_unique\n\n    def _get_db_path(self):\n        return os.path.join(self.user_data_dir, \"db.json\")\n\n    def _get_project_path(self, project_id: str):\n        return os.path.join(self.user_data_dir, \"projects\", project_id)\n\n    def _get_project_input_path(self, project_id: str):\n        return os.path.join(self._get_project_path(project_id), \"input.parquet\")\n\n    def _get_project_primary_output_root_path(self, analysis: AnalysisModel):\n        return os.path.join(\n            self._get_project_path(analysis.project_id),\n            analysis.path,\n            \"primary_outputs\",\n        )\n\n    def _get_project_secondary_output_root_path(\n        self, analysis: AnalysisModel, secondary_id: str\n    ):\n        return os.path.join(\n            self._get_project_path(analysis.project_id),\n            analysis.path,\n            \"secondary_outputs\",\n            secondary_id,\n        )\n\n    def _get_project_exports_root_path(self, analysis: AnalysisModel):\n        return os.path.join(\n            self._get_project_path(analysis.project_id), analysis.path, \"exports\"\n        )\n\n    def _get_web_presenter_state_path(self, analysis: AnalysisModel, presenter_id: str):\n        return os.path.join(\n            self._get_project_path(analysis.project_id),\n            analysis.path,\n            \"web_presenters\",\n            presenter_id,\n            \"state\",\n        )\n\n    def _lock_database(self):\n        \"\"\"\n        Locks the database to prevent concurrent access, in case multiple instances\n        of the application are running.\n        \"\"\"\n        lock_path = os.path.join(self.temp_dir, \"db.lock\")\n        return FileLock(lock_path)\n\n    def get_settings(self):\n        with self._lock_database():\n            return self._get_settings()\n\n    def _get_settings(self):\n        q = Query()\n        settings = self.db.search(q[\"class_\"] == \"settings\")\n        if settings:\n            return SettingsModel(**settings[0])\n        return SettingsModel()\n\n    def save_settings(self, **kwargs):\n        with self._lock_database():\n            q = Query()\n            settings = self._get_settings()\n            new_settings = SettingsModel(\n                **{\n                    **settings.model_dump(),\n                    **{\n                        key: value for key, value in kwargs.items() if value is not None\n                    },\n                }\n            )\n            self.db.upsert(new_settings.model_dump(), q[\"class_\"] == \"settings\")\n\n    @staticmethod\n    def _slugify_name(name: str):\n        return re.sub(r\"\\W+\", \"_\", name.lower()).strip(\"_\")\n\n    @staticmethod\n    def _get_unique_name(base_name: str, validator: Callable[[str], bool]):\n        if validator(base_name):\n            return base_name\n        i = 1\n        while True:\n            candidate = f\"{base_name}_{i}\"\n            if validator(candidate):\n                return candidate\n            i += 1\n</code></pre>"},{"location":"reference/terminal_tools/","title":"Terminal Tools","text":""},{"location":"reference/terminal_tools/#terminal-tools","title":"Terminal tools","text":""},{"location":"reference/terminal_tools/#terminal_tools","title":"<code>terminal_tools</code>","text":"<p>Modules:</p> Name Description <code>inception</code> <p>The inception module aids in creating nested terminal blocks (hence the name</p> <code>prompts</code> <code>utils</code>"},{"location":"reference/terminal_tools/#terminal_tools-functions","title":"Functions","text":""},{"location":"reference/terminal_tools/#terminal_tools-modules","title":"Modules","text":""},{"location":"reference/terminal_tools/#terminal_tools.inception","title":"<code>inception</code>","text":"<p>The inception module aids in creating nested terminal blocks (hence the name \"inception\"). It provides a <code>Context</code> class that manages a list of <code>Scope</code> instances. Each <code>Scope</code> instance represents a block of text that are buffered in memory and printed to the terminal at each refresh.</p> <p>Classes:</p> Name Description <code>Scope</code>"},{"location":"reference/terminal_tools/#terminal_tools.inception-classes","title":"Classes","text":""},{"location":"reference/terminal_tools/#terminal_tools.inception.Scope","title":"<code>Scope</code>","text":"<p>Methods:</p> Name Description <code>refresh</code> <p>Clear the terminal and repaint with every scope up and including to this one</p> Source code in <code>terminal_tools/inception.py</code> <pre><code>class Scope:\n    def __init__(self, context: TerminalContext, text: str):\n        self.context = context\n        self.text = text\n\n    def print(self):\n        print(self.text)\n\n    def refresh(self):\n        \"\"\"Clear the terminal and repaint with every scope up and including to this one\"\"\"\n        self.context._refresh()\n\n    def __enter__(self):\n        self.context._append_scope(self)\n        self.context._refresh()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.context._remove_scope(self)\n</code></pre> Functions <code></code> <code>refresh()</code> <p>Clear the terminal and repaint with every scope up and including to this one</p> Source code in <code>terminal_tools/inception.py</code> <pre><code>def refresh(self):\n    \"\"\"Clear the terminal and repaint with every scope up and including to this one\"\"\"\n    self.context._refresh()\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.inception-functions","title":"Functions","text":""},{"location":"reference/terminal_tools/#terminal_tools.prompts","title":"<code>prompts</code>","text":"<p>Functions:</p> Name Description <code>checkbox</code> <p>Wraps <code>inquirer</code>'s checkbox and catches KeyboardInterrupt</p> <code>confirm</code> <p>Wraps <code>inquirer</code>'s confirm input and catches KeyboardInterrupt</p> <code>file_selector</code> <p>Lets the user select a file from the filesystem.</p> <code>get_drives</code> <p>Returns a list of the logically assigned drives on a windows system.</p> <code>int_input</code> <p>Wraps <code>inquirer</code>'s text input and catches KeyboardInterrupt</p> <code>list_input</code> <p>Wraps <code>inquirer</code>'s list input and catches KeyboardInterrupt</p> <code>text</code> <p>Wraps <code>inquirer</code>'s text input and catches KeyboardInterrupt</p> <code>wrap_keyboard_interrupt</code> <p>Calls <code>fn</code> and catches KeyboardInterrupt, returning <code>fallback</code> if it occurs.</p>"},{"location":"reference/terminal_tools/#terminal_tools.prompts-functions","title":"Functions","text":""},{"location":"reference/terminal_tools/#terminal_tools.prompts.checkbox","title":"<code>checkbox(message, **kwargs)</code>","text":"<p>Wraps <code>inquirer</code>'s checkbox and catches KeyboardInterrupt</p> Source code in <code>terminal_tools/prompts.py</code> <pre><code>def checkbox(message: str, **kwargs):\n    \"\"\"\n    Wraps `inquirer`'s checkbox and catches KeyboardInterrupt\n    \"\"\"\n    return wrap_keyboard_interrupt(lambda: inquirer_checkbox(message, **kwargs))\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.prompts.confirm","title":"<code>confirm(message, *, cancel_fallback=False, **kwargs)</code>","text":"<p>Wraps <code>inquirer</code>'s confirm input and catches KeyboardInterrupt</p> Source code in <code>terminal_tools/prompts.py</code> <pre><code>def confirm(message: str, *, cancel_fallback: Optional[bool] = False, **kwargs):\n    \"\"\"\n    Wraps `inquirer`'s confirm input and catches KeyboardInterrupt\n    \"\"\"\n    return wrap_keyboard_interrupt(\n        lambda: inquirer_confirm(message, **kwargs), cancel_fallback\n    )\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.prompts.file_selector","title":"<code>file_selector(message='select a file', *, state=None)</code>","text":"<p>Lets the user select a file from the filesystem.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The prompt message. Defaults to \"select a file\".</p> <code>'select a file'</code> <code>initial_path</code> <code>str</code> <p>Where to start the directory listing. Defaults to current working directory.</p> required <p>Returns:</p> Type Description <code>(str, optional)</code> <p>The absolute path selected by the user, or None if the user cancels the prompt.</p> Source code in <code>terminal_tools/prompts.py</code> <pre><code>def file_selector(\n    message: str = \"select a file\", *, state: Optional[FileSelectorStateManager] = None\n):\n    \"\"\"Lets the user select a file from the filesystem.\n\n    Args:\n        message (str, optional): The prompt message. Defaults to \"select a file\".\n        initial_path (str, optional): Where to start the directory listing.\n          Defaults to current working directory.\n\n    Returns:\n        (str, optional): The absolute path selected by the user, or None if the\n          user cancels the prompt.\n    \"\"\"\n    initial_dir = state and state.get_current_path()\n    if initial_dir and not os.path.isdir(initial_dir):\n        initial_dir = None\n\n    current_path = os.path.realpath(initial_dir or os.curdir)\n\n    if os.name == \"nt\":\n        drives = get_drives()\n        drive_choices = [(drive, drive) for drive in drives]\n\n    def is_dir(entry: str):\n        return os.path.isdir(os.path.join(current_path, entry))\n\n    while True:\n        print(f\"current path: {current_path}\")\n        choices = [\n            (\"[..]\", \"..\"),\n            *(\n                (f\"[{entry}]\" if is_dir(entry) else entry, entry)\n                for entry in sorted(os.listdir(current_path))\n            ),\n        ]\n\n        # Add change drive option to the list of choices if on Windows\n        if os.name == \"nt\":\n            cur_drive = os.path.splitdrive(current_path)[0]\n            choices.insert(\n                0, (f\"[Change Drive (current - {cur_drive})]\", \"change_drive\")\n            )\n\n        selected_entry = list_input(message, choices=choices)\n\n        if selected_entry is not None and selected_entry == \"change_drive\":\n            selected_drive = list_input(\"Select a drive:\", choices=drive_choices)\n            if selected_drive is None:\n                return None\n\n            current_path = selected_entry = f\"{selected_drive}\\\\\"\n            # clear the prompted lines\n            clear_printed_lines(len(drives) + 1)\n\n        # inquirer will show up to 14 lines including the header\n        # we have one line for the current path to rewrite\n        clear_printed_lines(min(len(choices), 13) + 2)\n\n        if selected_entry is None:\n            return None\n\n        if is_dir(selected_entry):\n            current_path = os.path.realpath(os.path.join(current_path, selected_entry))\n        else:\n            if state is not None:\n                state.set_current_path(current_path)\n            return os.path.join(current_path, selected_entry)\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.prompts.get_drives","title":"<code>get_drives()</code>","text":"<p>Returns a list of the logically assigned drives on a windows system.</p> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of drive letters available and accessible on the system.</p> Source code in <code>terminal_tools/prompts.py</code> <pre><code>def get_drives():\n    \"\"\"\n    Returns a list of the logically assigned drives on a windows system.\n\n    Args:\n        None\n\n    Returns:\n        list: A list of drive letters available and accessible on the system.\n    \"\"\"\n\n    drives = []\n    bitmask = windll.kernel32.GetLogicalDrives()\n\n    for letter in ascii_uppercase:\n        if bitmask &amp; 1:\n            drives.append(letter + \":\")\n        bitmask &gt;&gt;= 1\n\n    return drives\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.prompts.int_input","title":"<code>int_input(message, *, min=None, max=None, default=None, **kwargs)</code>","text":"<p>Wraps <code>inquirer</code>'s text input and catches KeyboardInterrupt</p> Source code in <code>terminal_tools/prompts.py</code> <pre><code>def int_input(\n    message: str,\n    *,\n    min: Optional[int] = None,\n    max: Optional[int] = None,\n    default: Optional[int] = None,\n    **kwargs,\n) -&gt; Optional[int]:\n    \"\"\"\n    Wraps `inquirer`'s text input and catches KeyboardInterrupt\n    \"\"\"\n\n    def validate_value(value):\n        try:\n            value = int(value)\n        except ValueError:\n            raise ValidationError(\"Please enter a valid integer.\")\n\n        if min is not None and value &lt; min:\n            raise ValidationError(\n                f\"Please enter a value greater than or equal to {min}.\"\n            )\n\n        if max is not None and value &gt; max:\n            raise ValidationError(f\"Please enter a value less than or equal to {max}.\")\n\n        return True\n\n    result = wrap_keyboard_interrupt(\n        lambda: inquirer_text(\n            message,\n            validate=lambda previous_answers, value: validate_value(value),\n            default=str(default) if default is not None else None,\n            **kwargs,\n        ),\n        None,\n    )\n    return int(result) if result is not None else None\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.prompts.list_input","title":"<code>list_input(message, **kwargs)</code>","text":"<p>Wraps <code>inquirer</code>'s list input and catches KeyboardInterrupt</p> Source code in <code>terminal_tools/prompts.py</code> <pre><code>def list_input(message: str, **kwargs):\n    \"\"\"\n    Wraps `inquirer`'s list input and catches KeyboardInterrupt\n    \"\"\"\n    return wrap_keyboard_interrupt(lambda: inquirer_list_input(message, **kwargs))\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.prompts.text","title":"<code>text(message, **kwargs)</code>","text":"<p>Wraps <code>inquirer</code>'s text input and catches KeyboardInterrupt</p> Source code in <code>terminal_tools/prompts.py</code> <pre><code>def text(message: str, **kwargs):\n    \"\"\"\n    Wraps `inquirer`'s text input and catches KeyboardInterrupt\n    \"\"\"\n    return wrap_keyboard_interrupt(lambda: inquirer_text(message, **kwargs))\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.prompts.wrap_keyboard_interrupt","title":"<code>wrap_keyboard_interrupt(fn, fallback=None)</code>","text":"<p>Calls <code>fn</code> and catches KeyboardInterrupt, returning <code>fallback</code> if it occurs.</p> Source code in <code>terminal_tools/prompts.py</code> <pre><code>def wrap_keyboard_interrupt(fn, fallback=None):\n    \"\"\"\n    Calls `fn` and catches KeyboardInterrupt, returning `fallback` if it occurs.\n    \"\"\"\n    try:\n        return fn()\n    except KeyboardInterrupt:\n        return fallback\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.utils","title":"<code>utils</code>","text":"<p>Functions:</p> Name Description <code>clear_printed_lines</code> <p>Clear the last <code>count</code> lines of the terminal. Useful for repainting</p> <code>clear_terminal</code> <p>Clears the terminal</p> <code>draw_box</code> <p>Draw a box around the given text, which will be centered in the box.</p> <code>enable_windows_ansi_support</code> <p>Set up the Windows terminal to support ANSI escape codes, which will be needed</p> <code>is_wsl</code> <p>Check if the environment is WSL2.</p> <code>print_data_frame_summary</code> <p>Print a summary table for dataframes with many columns</p> <code>print_message</code> <p>Print styled messages with consistent formatting and emoji prefixes.</p> <code>smart_print_data_frame</code> <p>Smart dataframe printing with adaptive display based on terminal width.</p> <code>wait_for_key</code> <p>Waits for the user to press any key</p>"},{"location":"reference/terminal_tools/#terminal_tools.utils-functions","title":"Functions","text":""},{"location":"reference/terminal_tools/#terminal_tools.utils.clear_printed_lines","title":"<code>clear_printed_lines(count)</code>","text":"<p>Clear the last <code>count</code> lines of the terminal. Useful for repainting terminal output.</p> <p>Parameters:</p> Name Type Description Default <code>count</code> <code>int</code> <p>The number of lines to clear</p> required Source code in <code>terminal_tools/utils.py</code> <pre><code>def clear_printed_lines(count: int):\n    \"\"\"\n    Clear the last `count` lines of the terminal. Useful for repainting\n    terminal output.\n\n    Args:\n        count (int): The number of lines to clear\n    \"\"\"\n    for _ in range(count + 1):\n        sys.stdout.write(\"\\033[2K\")  # Clear the current line\n        sys.stdout.write(\"\\033[F\")  # Move cursor up one line\n    sys.stdout.write(\"\\033[2K\\r\")  # Clear the last line and move to start\n    sys.stdout.flush()\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.utils.clear_terminal","title":"<code>clear_terminal()</code>","text":"<p>Clears the terminal</p> Source code in <code>terminal_tools/utils.py</code> <pre><code>def clear_terminal():\n    \"\"\"Clears the terminal\"\"\"\n    if os.name == \"nt\":\n        os.system(\"cls\")\n    else:\n        os.system(\"clear\")\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.utils.draw_box","title":"<code>draw_box(text, *, padding_spaces=5, padding_lines=1)</code>","text":"<p>Draw a box around the given text, which will be centered in the box.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to be drawn, may be multiline. ANSI formatting and emojis are not supported, as they mess with both the character count calculation and the monospace font.</p> required <code>padding_spaces</code> <code>int</code> <p>Extra spaces on either side of the longest line. Defaults to 5.</p> <code>5</code> <code>padding_lines</code> <code>int</code> <p>Extra lines above and below the text. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The text surrounded by a box.</p> Source code in <code>terminal_tools/utils.py</code> <pre><code>def draw_box(text: str, *, padding_spaces: int = 5, padding_lines: int = 1) -&gt; str:\n    \"\"\"\n    Draw a box around the given text, which will be centered in the box.\n\n    Args:\n        text (str): The text to be drawn, may be multiline.\n          ANSI formatting and emojis are not supported, as they mess with\n          both the character count calculation and the monospace font.\n\n        padding_spaces (int, optional): Extra spaces on either side of the longest line. Defaults to 5.\n        padding_lines (int, optional): Extra lines above and below the text. Defaults to 1.\n\n    Returns:\n        str: The text surrounded by a box.\n    \"\"\"\n    lines = text.split(\"\\n\")\n    width = max(len(line) for line in lines) + padding_spaces * 2\n\n    box = \"\"\n    box += \"\u250c\" + \"\u2500\" * width + \"\u2510\\n\"\n    for _ in range(padding_lines):\n        box += \"\u2502\" + \" \" * width + \"\u2502\\n\"\n    for line in lines:\n        padding = \" \" * padding_spaces\n        box += \"\u2502\" + padding + line.center(width - 2 * padding_spaces) + padding + \"\u2502\\n\"\n    for _ in range(padding_lines):\n        box += \"\u2502\" + \" \" * width + \"\u2502\\n\"\n    box += \"\u2514\" + \"\u2500\" * width + \"\u2518\\n\"\n    return box\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.utils.enable_windows_ansi_support","title":"<code>enable_windows_ansi_support()</code>","text":"<p>Set up the Windows terminal to support ANSI escape codes, which will be needed for colored text, line clearing, and other terminal features.</p> Source code in <code>terminal_tools/utils.py</code> <pre><code>def enable_windows_ansi_support():\n    \"\"\"\n    Set up the Windows terminal to support ANSI escape codes, which will be needed\n    for colored text, line clearing, and other terminal features.\n    \"\"\"\n    if os.name == \"nt\":\n        # Enable ANSI escape code support for Windows\n        # On Windows, calling os.system('') with an empty string doesn't\n        # run any actual command. However, there's an undocumented side\n        # effect: it forces the Windows terminal to initialize or refresh\n        # its state, enabling certain features like the processing of ANSI\n        # escape codes, which might not otherwise be active.\n        os.system(\"\")\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.utils.is_wsl","title":"<code>is_wsl()</code>","text":"<p>Check if the environment is WSL2.</p> Source code in <code>terminal_tools/utils.py</code> <pre><code>def is_wsl() -&gt; bool:\n    \"\"\"Check if the environment is WSL2.\"\"\"\n    try:\n        with open(\"/proc/version\", \"r\") as f:\n            return \"microsoft\" in f.read().lower()\n    except FileNotFoundError:\n        return False\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.utils.print_data_frame_summary","title":"<code>print_data_frame_summary(data_frame, title, apply_color='column_data_type', caption=None)</code>","text":"<p>Print a summary table for dataframes with many columns</p> Source code in <code>terminal_tools/utils.py</code> <pre><code>def print_data_frame_summary(\n    data_frame, title: str, apply_color: str = \"column_data_type\", caption: str = None\n):\n    \"\"\"Print a summary table for dataframes with many columns\"\"\"\n    from preprocessing.series_semantic import infer_series_semantic\n\n    MAX_ROW_CHAR = 25\n\n    # Create summary data\n    summary_data = []\n    for col in data_frame.columns:\n        dtype = data_frame.select(col).dtypes[0]\n\n        # Get example value (first non-null if possible)\n        example_series = data_frame.select(col).to_series()\n        example_val = None\n        for val in example_series:\n            if val is not None:\n                example_val = str(val)\n                break\n        if example_val is None:\n            example_val = \"null\"\n\n        # Truncate long examples\n        if len(example_val) &gt; MAX_ROW_CHAR:\n            example_val = example_val[:MAX_ROW_CHAR] + \"...\"\n\n        # Get semantic analysis type\n        try:\n            semantic = infer_series_semantic(data_frame.select(col).to_series())\n            analysis_type = semantic.data_type if semantic else \"unknown\"\n        except Exception:\n            analysis_type = \"unknown\"\n\n        summary_data.append([col, str(dtype), example_val, analysis_type])\n\n    # Create summary dataframe\n    summary_df = pl.DataFrame(\n        {\n            \"Column Name\": [row[0] for row in summary_data],\n            \"Data Type\": [row[1] for row in summary_data],\n            \"Example Value\": [row[2] for row in summary_data],\n            \"Inferred Analyzer Input Type\": [row[3] for row in summary_data],\n        }\n    )\n\n    # Print with specified coloring mode\n    print_data_frame(summary_df, title, apply_color, caption)\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.utils.print_message","title":"<code>print_message(text, style='regular')</code>","text":"<p>Print styled messages with consistent formatting and emoji prefixes.</p> <p>A simple abstraction over Rich's console printing that provides predefined styles and emoji prefixes for different message types. Handles terminal background compatibility by using Rich's adaptive color system.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The message text to display</p> required <code>style</code> <code>str</code> <p>Message style type. Available options: - \"main\": Orange bold text with mango emoji (\ud83e\udd6d) - for primary messages - \"regular\": Plain text with no styling or emoji - for normal output - \"hint\": Green text with lightbulb emoji (\ud83d\udca1) - for helpful tips - \"error\": Bright red bold text with X emoji (\u274c) - for error messages - \"progress\": Plain text with hourglass emoji (\u23f3) - for loading states</p> <code>'regular'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; print_message(\"Dataset imported successfully!\", \"main\")\n\ud83e\udd6d Dataset imported successfully!\n</code></pre> <pre><code>&gt;&gt;&gt; print_message(\"Check your file path\", \"hint\")\n\ud83d\udca1 Hint: Check your file path\n</code></pre> <pre><code>&gt;&gt;&gt; print_message(\"File not found\", \"error\")\n\u274c File not found\n</code></pre> Source code in <code>terminal_tools/utils.py</code> <pre><code>def print_message(text: str, style: str = \"regular\"):\n    \"\"\"Print styled messages with consistent formatting and emoji prefixes.\n\n    A simple abstraction over Rich's console printing that provides predefined\n    styles and emoji prefixes for different message types. Handles terminal\n    background compatibility by using Rich's adaptive color system.\n\n    Args:\n        text: The message text to display\n        style: Message style type. Available options:\n            - \"main\": Orange bold text with mango emoji (\ud83e\udd6d) - for primary messages\n            - \"regular\": Plain text with no styling or emoji - for normal output\n            - \"hint\": Green text with lightbulb emoji (\ud83d\udca1) - for helpful tips\n            - \"error\": Bright red bold text with X emoji (\u274c) - for error messages\n            - \"progress\": Plain text with hourglass emoji (\u23f3) - for loading states\n\n    Examples:\n        &gt;&gt;&gt; print_message(\"Dataset imported successfully!\", \"main\")\n        \ud83e\udd6d Dataset imported successfully!\n\n        &gt;&gt;&gt; print_message(\"Check your file path\", \"hint\")\n        \ud83d\udca1 Hint: Check your file path\n\n        &gt;&gt;&gt; print_message(\"File not found\", \"error\")\n        \u274c File not found\n    \"\"\"\n\n    styles_dict = {\n        \"main\": \"orange1 bold\",\n        \"regular\": None,  # default styling\n        \"hint\": \"#609949\",\n        \"error\": \"bright_red bold\",\n        \"progress\": None,\n    }\n\n    text_emoji_prefix = {\n        \"main\": \"\ud83e\udd6d\",\n        \"hint\": \"\ud83d\udca1 Hint:\",\n        \"regular\": \"\",\n        \"progress\": \"\u23f3\",\n        \"error\": \"\u274c\",\n    }\n\n    text = text_emoji_prefix[style] + \" \" + text\n\n    console = Console()\n    msg = Text(text, style=styles_dict[style])\n    console.print(msg)\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.utils.smart_print_data_frame","title":"<code>smart_print_data_frame(data_frame, title, apply_color='column_data_type', smart_print=True, caption=None)</code>","text":"<p>Smart dataframe printing with adaptive display based on terminal width.</p> <p>Automatically chooses between full table display and summary view based on terminal width and number of columns. Provides Rich-styled tables with configurable coloring.</p> <p>Parameters:</p> Name Type Description Default <code>data_frame</code> <code>DataFrame</code> <p>Polars DataFrame to display</p> required <code>title</code> <code>str</code> <p>Title text to display above the table</p> required <code>apply_color</code> <code>str | None</code> <p>Color mode for the table display: - \"column_data_type\": Colors columns based on their Polars data types - \"column-wise\": Cycles through colors for each column - \"row-wise\": Cycles through colors for each row - None: No coloring (plain black and white display)</p> <code>'column_data_type'</code> <code>smart_print</code> <code>bool</code> <p>Controls adaptive display behavior: - True: Uses summary view for wide tables (&gt;8 cols or narrow terminal) - False: Always uses full table display regardless of width</p> <code>True</code> <code>caption</code> <code>str | None</code> <p>Optional caption text displayed below the table</p> <code>None</code> Display Logic <ul> <li>If smart_print=False: Always shows full table</li> <li>If smart_print=True and (&gt;8 columns OR estimated column width &lt;12):   Shows summary with column info, data types, and examples</li> <li>Otherwise: Shows full table with all data</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; smart_print_data_frame(df, \"My Data\", apply_color=None)\n&gt;&gt;&gt; smart_print_data_frame(df, \"Analysis Results\", caption=\"Processing complete\")\n&gt;&gt;&gt; smart_print_data_frame(df, \"Wide Dataset\", smart_print=False)\n</code></pre> Source code in <code>terminal_tools/utils.py</code> <pre><code>def smart_print_data_frame(\n    data_frame: pl.DataFrame,\n    title: str,\n    apply_color: str | None = \"column_data_type\",\n    smart_print: bool = True,\n    caption: str | None = None,\n) -&gt; None:\n    \"\"\"Smart dataframe printing with adaptive display based on terminal width.\n\n    Automatically chooses between full table display and summary view based on terminal\n    width and number of columns. Provides Rich-styled tables with configurable coloring.\n\n    Args:\n        data_frame: Polars DataFrame to display\n        title: Title text to display above the table\n        apply_color: Color mode for the table display:\n            - \"column_data_type\": Colors columns based on their Polars data types\n            - \"column-wise\": Cycles through colors for each column\n            - \"row-wise\": Cycles through colors for each row\n            - None: No coloring (plain black and white display)\n        smart_print: Controls adaptive display behavior:\n            - True: Uses summary view for wide tables (&gt;8 cols or narrow terminal)\n            - False: Always uses full table display regardless of width\n        caption: Optional caption text displayed below the table\n\n    Display Logic:\n        - If smart_print=False: Always shows full table\n        - If smart_print=True and (&gt;8 columns OR estimated column width &lt;12):\n          Shows summary with column info, data types, and examples\n        - Otherwise: Shows full table with all data\n\n    Examples:\n        &gt;&gt;&gt; smart_print_data_frame(df, \"My Data\", apply_color=None)\n        &gt;&gt;&gt; smart_print_data_frame(df, \"Analysis Results\", caption=\"Processing complete\")\n        &gt;&gt;&gt; smart_print_data_frame(df, \"Wide Dataset\", smart_print=False)\n    \"\"\"\n    if not smart_print:\n        # Always use full dataframe display when smart_print is disabled\n        print_data_frame(data_frame, title, apply_color, caption)\n        return\n\n    # Smart adaptive logic\n    terminal_width = console.size.width\n    n_cols = len(data_frame.columns)\n\n    # Calculate if columns will be too narrow for readability\n    estimated_col_width = max(60, terminal_width - 4) // max(n_cols, 1)\n    min_readable_width = 12  # Minimum width for readable columns\n\n    # Use summary if too many columns or columns would be too narrow\n    if n_cols &gt; 8 or estimated_col_width &lt; min_readable_width:\n        print_data_frame_summary(\n            data_frame,\n            title + \" (Dataset has a large nr. of columns, showing summary instead)\",\n            apply_color,\n            caption,\n        )\n    else:\n        print_data_frame(data_frame, title, apply_color, caption)\n</code></pre>"},{"location":"reference/terminal_tools/#terminal_tools.utils.wait_for_key","title":"<code>wait_for_key(prompt=False)</code>","text":"<p>Waits for the user to press any key</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>bool</code> <p>If true, a default text</p> <code>False</code> Source code in <code>terminal_tools/utils.py</code> <pre><code>def wait_for_key(prompt: bool = False):\n    \"\"\"Waits for the user to press any key\n\n    Args:\n        prompt (bool, optional): If true, a default text\n        `Press any key to continue` will be shown. Defaults to False.\n    \"\"\"\n    if prompt:\n        print(\"Press any key to continue...\", end=\"\", flush=True)\n    _wait_for_key()\n</code></pre>"},{"location":"reference/testing/","title":"Testing","text":""},{"location":"reference/testing/#testing","title":"<code>testing</code>","text":"<p>Modules:</p> Name Description <code>testers</code>"},{"location":"reference/testing/#testing-modules","title":"Modules","text":""},{"location":"reference/testing/#testing.testers","title":"<code>testers</code>","text":""},{"location":"reference/testing/#testing.testers-classes","title":"Classes","text":""},{"location":"reference/tokenizer/","title":"Tokenizer","text":""},{"location":"reference/tokenizer/#services.tokenizer.core","title":"<code>services.tokenizer.core</code>","text":"<p>Core tokenizer components.</p> <p>Exports: - AbstractTokenizer - TokenizerConfig - TokenList - LanguageFamily - TokenType - CaseHandling</p> Intended usage <p>from services.tokenizer.core import (     AbstractTokenizer,     TokenizerConfig,     TokenList,     LanguageFamily,     TokenType,     CaseHandling, )</p> <p>Modules:</p> Name Description <code>base</code> <p>AbstractTokenizer abstract base class</p> <code>types</code> <p>TokenizerConfig, enums, and shared types</p> <p>Classes:</p> Name Description <code>AbstractTokenizer</code> <p>Abstract base class for all tokenizer implementations.</p> <code>CaseHandling</code> <p>How to handle character case during tokenization.</p> <code>LanguageFamily</code> <p>Language families that affect tokenization strategies.</p> <code>TokenType</code> <p>Types of tokens that can be extracted.</p> <code>TokenizerConfig</code> <p>Configuration for tokenizer behavior.</p>"},{"location":"reference/tokenizer/#services.tokenizer.core.AbstractTokenizer","title":"<code>AbstractTokenizer</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all tokenizer implementations.</p> <p>This class defines the core interface that all tokenizer plugins must implement. It provides a clean contract for tokenization operations while allowing for different implementation strategies.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the tokenizer with configuration.</p> <code>tokenize</code> <p>Tokenize input text into a list of tokens.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>TokenizerConfig</code> <p>Get the current tokenizer configuration.</p> Source code in <code>services/tokenizer/core/base.py</code> <pre><code>class AbstractTokenizer(ABC):\n    \"\"\"\n    Abstract base class for all tokenizer implementations.\n\n    This class defines the core interface that all tokenizer plugins must implement.\n    It provides a clean contract for tokenization operations while allowing for\n    different implementation strategies.\n    \"\"\"\n\n    def __init__(self, config: Optional[TokenizerConfig] = None):\n        \"\"\"\n        Initialize the tokenizer with configuration.\n\n        Args:\n            config: Tokenizer configuration. If None, default config will be used.\n        \"\"\"\n        self._config = config or TokenizerConfig()\n\n    @property\n    def config(self) -&gt; TokenizerConfig:\n        \"\"\"Get the current tokenizer configuration.\"\"\"\n        return self._config\n\n    @abstractmethod\n    def tokenize(self, text: str) -&gt; TokenList:\n        \"\"\"\n        Tokenize input text into a list of tokens.\n\n        This is the main tokenization method that all implementations must provide.\n\n        Args:\n            text: Input text to tokenize\n\n        Returns:\n            List of tokens extracted from the input text\n        \"\"\"\n        pass\n\n    def _preprocess_text(self, text: str) -&gt; str:\n        \"\"\"\n        Apply preprocessing to text before tokenization.\n\n        This method applies configuration-based preprocessing such as\n        case handling and Unicode normalization.\n\n        Args:\n            text: Input text to preprocess\n\n        Returns:\n            Preprocessed text\n        \"\"\"\n        if not text:\n            return text\n\n        # Apply Unicode normalization\n        if self._config.normalize_unicode:\n            import unicodedata\n\n            text = unicodedata.normalize(\"NFKC\", text)\n\n        # Apply case handling\n        from .types import CaseHandling\n\n        if self._config.case_handling == CaseHandling.LOWERCASE:\n            text = text.lower()\n        elif self._config.case_handling == CaseHandling.UPPERCASE:\n            text = text.upper()\n        elif self._config.case_handling == CaseHandling.NORMALIZE:\n            # TODO: Implement proper noun detection for smart normalization\n            # Currently using simple lowercase as a placeholder\n            text = text.lower()\n\n        return text\n\n    def _postprocess_tokens(self, tokens: TokenList) -&gt; TokenList:\n        \"\"\"\n        Apply post-processing to extracted tokens.\n\n        This method applies configuration-based filtering and cleanup\n        to the token list.\n\n        Args:\n            tokens: List of raw tokens\n\n        Returns:\n            Processed token list\n        \"\"\"\n        if not tokens:\n            return tokens\n\n        processed_tokens = []\n\n        for token in tokens:\n            # Strip whitespace if configured\n            if self._config.strip_whitespace:\n                token = token.strip()\n\n            # Skip empty tokens\n            if not token:\n                continue\n\n            # Filter emojis if not included\n            if not self._config.include_emoji and self._is_emoji(token):\n                continue\n\n            # Apply length filtering\n            if len(token) &lt; self._config.min_token_length:\n                continue\n\n            if (\n                self._config.max_token_length is not None\n                and len(token) &gt; self._config.max_token_length\n            ):\n                continue\n\n            processed_tokens.append(token)\n\n        return processed_tokens\n\n    @staticmethod\n    def _is_emoji(token: str) -&gt; bool:\n        \"\"\"\n        Check if a token is an emoji character.\n\n        Args:\n            token: Token to check\n\n        Returns:\n            True if the token is an emoji, False otherwise\n        \"\"\"\n        if not token:\n            return False\n\n        # Accept sequences made of emoji code points plus common modifiers\n        EMOJI_RANGES = (\n            (0x1F600, 0x1F64F),  # Emoticons\n            (0x1F300, 0x1F5FF),  # Misc Symbols &amp; Pictographs\n            (0x1F680, 0x1F6FF),  # Transport &amp; Map\n            (0x1F1E6, 0x1F1FF),  # Regional Indicators\n            (0x2600, 0x26FF),  # Misc symbols\n            (0x2700, 0x27BF),  # Dingbats\n            (0x1F900, 0x1F9FF),  # Supplemental Symbols &amp; Pictographs\n            (0x1FA70, 0x1FAFF),  # Symbols &amp; Pictographs Extended-A\n        )\n        MODIFIERS = {0x200D, 0xFE0E, 0xFE0F}  # ZWJ, VS15, VS16\n        SKIN_TONE = (0x1F3FB, 0x1F3FF)\n        TAGS = (0xE0020, 0xE007F)  # Emoji tag sequences\n\n        def in_any_range(cp: int, ranges) -&gt; bool:\n            for a, b in ranges:\n                if a &lt;= cp &lt;= b:\n                    return True\n            return False\n\n        def is_modifier(cp: int) -&gt; bool:\n            return (\n                cp in MODIFIERS\n                or SKIN_TONE[0] &lt;= cp &lt;= SKIN_TONE[1]\n                or TAGS[0] &lt;= cp &lt;= TAGS[1]\n            )\n\n        for ch in token:\n            cp = ord(ch)\n            if not (in_any_range(cp, EMOJI_RANGES) or is_modifier(cp)):\n                return False\n        return True\n</code></pre>"},{"location":"reference/tokenizer/#services.tokenizer.core.AbstractTokenizer.__init__","title":"<code>__init__(config=None)</code>","text":"<p>Initialize the tokenizer with configuration.</p> <p>Parameters:</p> Name Type Description Default <code>Optional[TokenizerConfig]</code> <p>Tokenizer configuration. If None, default config will be used.</p> <code>None</code> Source code in <code>services/tokenizer/core/base.py</code> <pre><code>def __init__(self, config: Optional[TokenizerConfig] = None):\n    \"\"\"\n    Initialize the tokenizer with configuration.\n\n    Args:\n        config: Tokenizer configuration. If None, default config will be used.\n    \"\"\"\n    self._config = config or TokenizerConfig()\n</code></pre>"},{"location":"reference/tokenizer/#services.tokenizer.core.AbstractTokenizer.__init__(config)","title":"<code>config</code>","text":""},{"location":"reference/tokenizer/#services.tokenizer.core.AbstractTokenizer.config","title":"<code>config</code>  <code>property</code>","text":"<p>Get the current tokenizer configuration.</p>"},{"location":"reference/tokenizer/#services.tokenizer.core.AbstractTokenizer.tokenize","title":"<code>tokenize(text)</code>  <code>abstractmethod</code>","text":"<p>Tokenize input text into a list of tokens.</p> <p>This is the main tokenization method that all implementations must provide.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>Input text to tokenize</p> required <p>Returns:</p> Type Description <code>TokenList</code> <p>List of tokens extracted from the input text</p> Source code in <code>services/tokenizer/core/base.py</code> <pre><code>@abstractmethod\ndef tokenize(self, text: str) -&gt; TokenList:\n    \"\"\"\n    Tokenize input text into a list of tokens.\n\n    This is the main tokenization method that all implementations must provide.\n\n    Args:\n        text: Input text to tokenize\n\n    Returns:\n        List of tokens extracted from the input text\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tokenizer/#services.tokenizer.core.AbstractTokenizer.tokenize(text)","title":"<code>text</code>","text":""},{"location":"reference/tokenizer/#services.tokenizer.core.CaseHandling","title":"<code>CaseHandling</code>","text":"<p>               Bases: <code>Enum</code></p> <p>How to handle character case during tokenization.</p> Source code in <code>services/tokenizer/core/types.py</code> <pre><code>class CaseHandling(Enum):\n    \"\"\"How to handle character case during tokenization.\"\"\"\n\n    PRESERVE = \"preserve\"  # Keep original case\n    LOWERCASE = \"lowercase\"  # Convert to lowercase\n    UPPERCASE = \"uppercase\"  # Convert to uppercase\n    NORMALIZE = \"normalize\"  # Smart case normalization\n</code></pre>"},{"location":"reference/tokenizer/#services.tokenizer.core.LanguageFamily","title":"<code>LanguageFamily</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Language families that affect tokenization strategies.</p> Source code in <code>services/tokenizer/core/types.py</code> <pre><code>class LanguageFamily(str, Enum):\n    \"\"\"Language families that affect tokenization strategies.\"\"\"\n\n    LATIN = \"latin\"  # Space-separated languages (English, French, etc.)\n    CJK = \"cjk\"  # Chinese, Japanese, Korean\n    ARABIC = \"arabic\"  # Arabic script languages\n    MIXED = \"mixed\"  # Mixed content requiring multiple strategies\n    UNKNOWN = \"unknown\"  # Language detection failed or not performed\n</code></pre>"},{"location":"reference/tokenizer/#services.tokenizer.core.TokenType","title":"<code>TokenType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Types of tokens that can be extracted.</p> Source code in <code>services/tokenizer/core/types.py</code> <pre><code>class TokenType(str, Enum):\n    \"\"\"Types of tokens that can be extracted.\"\"\"\n\n    WORD = \"word\"  # Regular words\n    PUNCTUATION = \"punctuation\"  # Punctuation marks\n    NUMERIC = \"numeric\"  # Numbers\n    EMOJI = \"emoji\"  # Emoji characters\n    HASHTAG = \"hashtag\"  # Social media hashtags\n    MENTION = \"mention\"  # Social media mentions\n    URL = \"url\"  # URLs and links\n    EMAIL = \"email\"  # Email addresses\n    WHITESPACE = \"whitespace\"  # Whitespace (when preserved)\n</code></pre>"},{"location":"reference/tokenizer/#services.tokenizer.core.TokenizerConfig","title":"<code>TokenizerConfig</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for tokenizer behavior.</p> <p>Controls all aspects of text tokenization including script handling, social media entity processing, and output formatting.</p> <p>Social Media Entity Behavior: - extract_hashtags/extract_mentions: When False, splits into component words - include_urls/include_emails: When False, completely excludes (no fragmentation)</p> <p>Fields:</p> <ul> <li> <code>fallback_language_family</code>                 (<code>LanguageFamily</code>)             </li> <li> <code>include_punctuation</code>                 (<code>bool</code>)             </li> <li> <code>include_numeric</code>                 (<code>bool</code>)             </li> <li> <code>include_emoji</code>                 (<code>bool</code>)             </li> <li> <code>case_handling</code>                 (<code>CaseHandling</code>)             </li> <li> <code>normalize_unicode</code>                 (<code>bool</code>)             </li> <li> <code>extract_hashtags</code>                 (<code>bool</code>)             </li> <li> <code>extract_mentions</code>                 (<code>bool</code>)             </li> <li> <code>extract_cashtags</code>                 (<code>bool</code>)             </li> <li> <code>include_urls</code>                 (<code>bool</code>)             </li> <li> <code>include_emails</code>                 (<code>bool</code>)             </li> <li> <code>min_token_length</code>                 (<code>int</code>)             </li> <li> <code>max_token_length</code>                 (<code>Optional[int]</code>)             </li> <li> <code>strip_whitespace</code>                 (<code>bool</code>)             </li> </ul> Source code in <code>services/tokenizer/core/types.py</code> <pre><code>class TokenizerConfig(BaseModel):\n    \"\"\"Configuration for tokenizer behavior.\n\n    Controls all aspects of text tokenization including script handling,\n    social media entity processing, and output formatting.\n\n    Social Media Entity Behavior:\n    - extract_hashtags/extract_mentions: When False, splits into component words\n    - include_urls/include_emails: When False, completely excludes (no fragmentation)\n    \"\"\"\n\n    # Language detection settings\n    fallback_language_family: LanguageFamily = LanguageFamily.MIXED\n    \"\"\"Default language family when detection fails or mixed content is found.\"\"\"\n\n    # Token type filtering\n    include_punctuation: bool = False\n    \"\"\"Whether to include punctuation marks as separate tokens.\"\"\"\n\n    include_numeric: bool = True\n    \"\"\"Whether to include numeric tokens (integers, decimals, etc.).\"\"\"\n\n    include_emoji: bool = False\n    \"\"\"Whether to include emoji characters as tokens.\"\"\"\n\n    # Text preprocessing\n    case_handling: CaseHandling = CaseHandling.LOWERCASE\n    \"\"\"How to handle character case during tokenization.\"\"\"\n\n    normalize_unicode: bool = False\n    \"\"\"Whether to apply Unicode NFKC normalization. Default False to preserve unicode tricks for bot detection.\"\"\"\n\n    # Social media features\n    extract_hashtags: bool = True\n    \"\"\"Whether to preserve hashtags as single tokens. If False, splits into component words.\"\"\"\n\n    extract_mentions: bool = True\n    \"\"\"Whether to preserve @mentions as single tokens. If False, splits into component words.\"\"\"\n\n    extract_cashtags: bool = True\n    \"\"\"Whether to preserve cashtags ($AAPL) as single tokens. If False, splits into components.\"\"\"\n\n    include_urls: bool = True\n    \"\"\"Whether to include URLs as tokens. If False, URLs are completely excluded (not fragmented).\"\"\"\n\n    include_emails: bool = True\n    \"\"\"Whether to include email addresses as tokens. If False, emails are completely excluded (not fragmented).\"\"\"\n\n    # Output formatting\n    min_token_length: int = 1\n    \"\"\"Minimum length for tokens to be included in output.\"\"\"\n\n    max_token_length: Optional[int] = None\n    \"\"\"Maximum length for tokens. If None, no length limit is applied.\"\"\"\n\n    strip_whitespace: bool = True\n    \"\"\"Whether to strip leading/trailing whitespace from tokens.\"\"\"\n</code></pre>"},{"location":"reference/tokenizer/#services.tokenizer.core.TokenizerConfig.case_handling","title":"<code>case_handling = CaseHandling.LOWERCASE</code>  <code>pydantic-field</code>","text":"<p>How to handle character case during tokenization.</p>"},{"location":"reference/tokenizer/#services.tokenizer.core.TokenizerConfig.extract_cashtags","title":"<code>extract_cashtags = True</code>  <code>pydantic-field</code>","text":"<p>Whether to preserve cashtags ($AAPL) as single tokens. If False, splits into components.</p>"},{"location":"reference/tokenizer/#services.tokenizer.core.TokenizerConfig.extract_hashtags","title":"<code>extract_hashtags = True</code>  <code>pydantic-field</code>","text":"<p>Whether to preserve hashtags as single tokens. If False, splits into component words.</p>"},{"location":"reference/tokenizer/#services.tokenizer.core.TokenizerConfig.extract_mentions","title":"<code>extract_mentions = True</code>  <code>pydantic-field</code>","text":"<p>Whether to preserve @mentions as single tokens. If False, splits into component words.</p>"},{"location":"reference/tokenizer/#services.tokenizer.core.TokenizerConfig.fallback_language_family","title":"<code>fallback_language_family = LanguageFamily.MIXED</code>  <code>pydantic-field</code>","text":"<p>Default language family when detection fails or mixed content is found.</p>"},{"location":"reference/tokenizer/#services.tokenizer.core.TokenizerConfig.include_emails","title":"<code>include_emails = True</code>  <code>pydantic-field</code>","text":"<p>Whether to include email addresses as tokens. If False, emails are completely excluded (not fragmented).</p>"},{"location":"reference/tokenizer/#services.tokenizer.core.TokenizerConfig.include_emoji","title":"<code>include_emoji = False</code>  <code>pydantic-field</code>","text":"<p>Whether to include emoji characters as tokens.</p>"},{"location":"reference/tokenizer/#services.tokenizer.core.TokenizerConfig.include_numeric","title":"<code>include_numeric = True</code>  <code>pydantic-field</code>","text":"<p>Whether to include numeric tokens (integers, decimals, etc.).</p>"},{"location":"reference/tokenizer/#services.tokenizer.core.TokenizerConfig.include_punctuation","title":"<code>include_punctuation = False</code>  <code>pydantic-field</code>","text":"<p>Whether to include punctuation marks as separate tokens.</p>"},{"location":"reference/tokenizer/#services.tokenizer.core.TokenizerConfig.include_urls","title":"<code>include_urls = True</code>  <code>pydantic-field</code>","text":"<p>Whether to include URLs as tokens. If False, URLs are completely excluded (not fragmented).</p>"},{"location":"reference/tokenizer/#services.tokenizer.core.TokenizerConfig.max_token_length","title":"<code>max_token_length = None</code>  <code>pydantic-field</code>","text":"<p>Maximum length for tokens. If None, no length limit is applied.</p>"},{"location":"reference/tokenizer/#services.tokenizer.core.TokenizerConfig.min_token_length","title":"<code>min_token_length = 1</code>  <code>pydantic-field</code>","text":"<p>Minimum length for tokens to be included in output.</p>"},{"location":"reference/tokenizer/#services.tokenizer.core.TokenizerConfig.normalize_unicode","title":"<code>normalize_unicode = False</code>  <code>pydantic-field</code>","text":"<p>Whether to apply Unicode NFKC normalization. Default False to preserve unicode tricks for bot detection.</p>"},{"location":"reference/tokenizer/#services.tokenizer.core.TokenizerConfig.strip_whitespace","title":"<code>strip_whitespace = True</code>  <code>pydantic-field</code>","text":"<p>Whether to strip leading/trailing whitespace from tokens.</p>"},{"location":"reference/tokenizer/#services.tokenizer.core.base","title":"<code>base</code>","text":"<p>AbstractTokenizer abstract base class</p> <p>This module contains the abstract base class that defines the interface for all tokenizer implementations.</p> <p>Classes:</p> Name Description <code>AbstractTokenizer</code> <p>Abstract base class for all tokenizer implementations.</p>"},{"location":"reference/tokenizer/#services.tokenizer.core.base.AbstractTokenizer","title":"<code>AbstractTokenizer</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all tokenizer implementations.</p> <p>This class defines the core interface that all tokenizer plugins must implement. It provides a clean contract for tokenization operations while allowing for different implementation strategies.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the tokenizer with configuration.</p> <code>tokenize</code> <p>Tokenize input text into a list of tokens.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>TokenizerConfig</code> <p>Get the current tokenizer configuration.</p> Source code in <code>services/tokenizer/core/base.py</code> <pre><code>class AbstractTokenizer(ABC):\n    \"\"\"\n    Abstract base class for all tokenizer implementations.\n\n    This class defines the core interface that all tokenizer plugins must implement.\n    It provides a clean contract for tokenization operations while allowing for\n    different implementation strategies.\n    \"\"\"\n\n    def __init__(self, config: Optional[TokenizerConfig] = None):\n        \"\"\"\n        Initialize the tokenizer with configuration.\n\n        Args:\n            config: Tokenizer configuration. If None, default config will be used.\n        \"\"\"\n        self._config = config or TokenizerConfig()\n\n    @property\n    def config(self) -&gt; TokenizerConfig:\n        \"\"\"Get the current tokenizer configuration.\"\"\"\n        return self._config\n\n    @abstractmethod\n    def tokenize(self, text: str) -&gt; TokenList:\n        \"\"\"\n        Tokenize input text into a list of tokens.\n\n        This is the main tokenization method that all implementations must provide.\n\n        Args:\n            text: Input text to tokenize\n\n        Returns:\n            List of tokens extracted from the input text\n        \"\"\"\n        pass\n\n    def _preprocess_text(self, text: str) -&gt; str:\n        \"\"\"\n        Apply preprocessing to text before tokenization.\n\n        This method applies configuration-based preprocessing such as\n        case handling and Unicode normalization.\n\n        Args:\n            text: Input text to preprocess\n\n        Returns:\n            Preprocessed text\n        \"\"\"\n        if not text:\n            return text\n\n        # Apply Unicode normalization\n        if self._config.normalize_unicode:\n            import unicodedata\n\n            text = unicodedata.normalize(\"NFKC\", text)\n\n        # Apply case handling\n        from .types import CaseHandling\n\n        if self._config.case_handling == CaseHandling.LOWERCASE:\n            text = text.lower()\n        elif self._config.case_handling == CaseHandling.UPPERCASE:\n            text = text.upper()\n        elif self._config.case_handling == CaseHandling.NORMALIZE:\n            # TODO: Implement proper noun detection for smart normalization\n            # Currently using simple lowercase as a placeholder\n            text = text.lower()\n\n        return text\n\n    def _postprocess_tokens(self, tokens: TokenList) -&gt; TokenList:\n        \"\"\"\n        Apply post-processing to extracted tokens.\n\n        This method applies configuration-based filtering and cleanup\n        to the token list.\n\n        Args:\n            tokens: List of raw tokens\n\n        Returns:\n            Processed token list\n        \"\"\"\n        if not tokens:\n            return tokens\n\n        processed_tokens = []\n\n        for token in tokens:\n            # Strip whitespace if configured\n            if self._config.strip_whitespace:\n                token = token.strip()\n\n            # Skip empty tokens\n            if not token:\n                continue\n\n            # Filter emojis if not included\n            if not self._config.include_emoji and self._is_emoji(token):\n                continue\n\n            # Apply length filtering\n            if len(token) &lt; self._config.min_token_length:\n                continue\n\n            if (\n                self._config.max_token_length is not None\n                and len(token) &gt; self._config.max_token_length\n            ):\n                continue\n\n            processed_tokens.append(token)\n\n        return processed_tokens\n\n    @staticmethod\n    def _is_emoji(token: str) -&gt; bool:\n        \"\"\"\n        Check if a token is an emoji character.\n\n        Args:\n            token: Token to check\n\n        Returns:\n            True if the token is an emoji, False otherwise\n        \"\"\"\n        if not token:\n            return False\n\n        # Accept sequences made of emoji code points plus common modifiers\n        EMOJI_RANGES = (\n            (0x1F600, 0x1F64F),  # Emoticons\n            (0x1F300, 0x1F5FF),  # Misc Symbols &amp; Pictographs\n            (0x1F680, 0x1F6FF),  # Transport &amp; Map\n            (0x1F1E6, 0x1F1FF),  # Regional Indicators\n            (0x2600, 0x26FF),  # Misc symbols\n            (0x2700, 0x27BF),  # Dingbats\n            (0x1F900, 0x1F9FF),  # Supplemental Symbols &amp; Pictographs\n            (0x1FA70, 0x1FAFF),  # Symbols &amp; Pictographs Extended-A\n        )\n        MODIFIERS = {0x200D, 0xFE0E, 0xFE0F}  # ZWJ, VS15, VS16\n        SKIN_TONE = (0x1F3FB, 0x1F3FF)\n        TAGS = (0xE0020, 0xE007F)  # Emoji tag sequences\n\n        def in_any_range(cp: int, ranges) -&gt; bool:\n            for a, b in ranges:\n                if a &lt;= cp &lt;= b:\n                    return True\n            return False\n\n        def is_modifier(cp: int) -&gt; bool:\n            return (\n                cp in MODIFIERS\n                or SKIN_TONE[0] &lt;= cp &lt;= SKIN_TONE[1]\n                or TAGS[0] &lt;= cp &lt;= TAGS[1]\n            )\n\n        for ch in token:\n            cp = ord(ch)\n            if not (in_any_range(cp, EMOJI_RANGES) or is_modifier(cp)):\n                return False\n        return True\n</code></pre>"},{"location":"reference/tokenizer/#services.tokenizer.core.base.AbstractTokenizer.__init__","title":"<code>__init__(config=None)</code>","text":"<p>Initialize the tokenizer with configuration.</p> <p>Parameters:</p> Name Type Description Default <code>Optional[TokenizerConfig]</code> <p>Tokenizer configuration. If None, default config will be used.</p> <code>None</code> Source code in <code>services/tokenizer/core/base.py</code> <pre><code>def __init__(self, config: Optional[TokenizerConfig] = None):\n    \"\"\"\n    Initialize the tokenizer with configuration.\n\n    Args:\n        config: Tokenizer configuration. If None, default config will be used.\n    \"\"\"\n    self._config = config or TokenizerConfig()\n</code></pre>"},{"location":"reference/tokenizer/#services.tokenizer.core.base.AbstractTokenizer.__init__(config)","title":"<code>config</code>","text":""},{"location":"reference/tokenizer/#services.tokenizer.core.base.AbstractTokenizer.config","title":"<code>config</code>  <code>property</code>","text":"<p>Get the current tokenizer configuration.</p>"},{"location":"reference/tokenizer/#services.tokenizer.core.base.AbstractTokenizer.tokenize","title":"<code>tokenize(text)</code>  <code>abstractmethod</code>","text":"<p>Tokenize input text into a list of tokens.</p> <p>This is the main tokenization method that all implementations must provide.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>Input text to tokenize</p> required <p>Returns:</p> Type Description <code>TokenList</code> <p>List of tokens extracted from the input text</p> Source code in <code>services/tokenizer/core/base.py</code> <pre><code>@abstractmethod\ndef tokenize(self, text: str) -&gt; TokenList:\n    \"\"\"\n    Tokenize input text into a list of tokens.\n\n    This is the main tokenization method that all implementations must provide.\n\n    Args:\n        text: Input text to tokenize\n\n    Returns:\n        List of tokens extracted from the input text\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tokenizer/#services.tokenizer.core.base.AbstractTokenizer.tokenize(text)","title":"<code>text</code>","text":""},{"location":"reference/tokenizer/#services.tokenizer.core.types","title":"<code>types</code>","text":"<p>TokenizerConfig, enums, and shared types</p> <p>This module contains configuration models, enumerations, and shared type definitions used across the tokenizer service.</p> <p>Classes:</p> Name Description <code>CaseHandling</code> <p>How to handle character case during tokenization.</p> <code>LanguageFamily</code> <p>Language families that affect tokenization strategies.</p> <code>TokenType</code> <p>Types of tokens that can be extracted.</p> <code>TokenizerConfig</code> <p>Configuration for tokenizer behavior.</p>"},{"location":"reference/tokenizer/#services.tokenizer.core.types.CaseHandling","title":"<code>CaseHandling</code>","text":"<p>               Bases: <code>Enum</code></p> <p>How to handle character case during tokenization.</p> Source code in <code>services/tokenizer/core/types.py</code> <pre><code>class CaseHandling(Enum):\n    \"\"\"How to handle character case during tokenization.\"\"\"\n\n    PRESERVE = \"preserve\"  # Keep original case\n    LOWERCASE = \"lowercase\"  # Convert to lowercase\n    UPPERCASE = \"uppercase\"  # Convert to uppercase\n    NORMALIZE = \"normalize\"  # Smart case normalization\n</code></pre>"},{"location":"reference/tokenizer/#services.tokenizer.core.types.LanguageFamily","title":"<code>LanguageFamily</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Language families that affect tokenization strategies.</p> Source code in <code>services/tokenizer/core/types.py</code> <pre><code>class LanguageFamily(str, Enum):\n    \"\"\"Language families that affect tokenization strategies.\"\"\"\n\n    LATIN = \"latin\"  # Space-separated languages (English, French, etc.)\n    CJK = \"cjk\"  # Chinese, Japanese, Korean\n    ARABIC = \"arabic\"  # Arabic script languages\n    MIXED = \"mixed\"  # Mixed content requiring multiple strategies\n    UNKNOWN = \"unknown\"  # Language detection failed or not performed\n</code></pre>"},{"location":"reference/tokenizer/#services.tokenizer.core.types.TokenType","title":"<code>TokenType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Types of tokens that can be extracted.</p> Source code in <code>services/tokenizer/core/types.py</code> <pre><code>class TokenType(str, Enum):\n    \"\"\"Types of tokens that can be extracted.\"\"\"\n\n    WORD = \"word\"  # Regular words\n    PUNCTUATION = \"punctuation\"  # Punctuation marks\n    NUMERIC = \"numeric\"  # Numbers\n    EMOJI = \"emoji\"  # Emoji characters\n    HASHTAG = \"hashtag\"  # Social media hashtags\n    MENTION = \"mention\"  # Social media mentions\n    URL = \"url\"  # URLs and links\n    EMAIL = \"email\"  # Email addresses\n    WHITESPACE = \"whitespace\"  # Whitespace (when preserved)\n</code></pre>"},{"location":"reference/tokenizer/#services.tokenizer.core.types.TokenizerConfig","title":"<code>TokenizerConfig</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for tokenizer behavior.</p> <p>Controls all aspects of text tokenization including script handling, social media entity processing, and output formatting.</p> <p>Social Media Entity Behavior: - extract_hashtags/extract_mentions: When False, splits into component words - include_urls/include_emails: When False, completely excludes (no fragmentation)</p> <p>Fields:</p> <ul> <li> <code>fallback_language_family</code>                 (<code>LanguageFamily</code>)             </li> <li> <code>include_punctuation</code>                 (<code>bool</code>)             </li> <li> <code>include_numeric</code>                 (<code>bool</code>)             </li> <li> <code>include_emoji</code>                 (<code>bool</code>)             </li> <li> <code>case_handling</code>                 (<code>CaseHandling</code>)             </li> <li> <code>normalize_unicode</code>                 (<code>bool</code>)             </li> <li> <code>extract_hashtags</code>                 (<code>bool</code>)             </li> <li> <code>extract_mentions</code>                 (<code>bool</code>)             </li> <li> <code>extract_cashtags</code>                 (<code>bool</code>)             </li> <li> <code>include_urls</code>                 (<code>bool</code>)             </li> <li> <code>include_emails</code>                 (<code>bool</code>)             </li> <li> <code>min_token_length</code>                 (<code>int</code>)             </li> <li> <code>max_token_length</code>                 (<code>Optional[int]</code>)             </li> <li> <code>strip_whitespace</code>                 (<code>bool</code>)             </li> </ul> Source code in <code>services/tokenizer/core/types.py</code> <pre><code>class TokenizerConfig(BaseModel):\n    \"\"\"Configuration for tokenizer behavior.\n\n    Controls all aspects of text tokenization including script handling,\n    social media entity processing, and output formatting.\n\n    Social Media Entity Behavior:\n    - extract_hashtags/extract_mentions: When False, splits into component words\n    - include_urls/include_emails: When False, completely excludes (no fragmentation)\n    \"\"\"\n\n    # Language detection settings\n    fallback_language_family: LanguageFamily = LanguageFamily.MIXED\n    \"\"\"Default language family when detection fails or mixed content is found.\"\"\"\n\n    # Token type filtering\n    include_punctuation: bool = False\n    \"\"\"Whether to include punctuation marks as separate tokens.\"\"\"\n\n    include_numeric: bool = True\n    \"\"\"Whether to include numeric tokens (integers, decimals, etc.).\"\"\"\n\n    include_emoji: bool = False\n    \"\"\"Whether to include emoji characters as tokens.\"\"\"\n\n    # Text preprocessing\n    case_handling: CaseHandling = CaseHandling.LOWERCASE\n    \"\"\"How to handle character case during tokenization.\"\"\"\n\n    normalize_unicode: bool = False\n    \"\"\"Whether to apply Unicode NFKC normalization. Default False to preserve unicode tricks for bot detection.\"\"\"\n\n    # Social media features\n    extract_hashtags: bool = True\n    \"\"\"Whether to preserve hashtags as single tokens. If False, splits into component words.\"\"\"\n\n    extract_mentions: bool = True\n    \"\"\"Whether to preserve @mentions as single tokens. If False, splits into component words.\"\"\"\n\n    extract_cashtags: bool = True\n    \"\"\"Whether to preserve cashtags ($AAPL) as single tokens. If False, splits into components.\"\"\"\n\n    include_urls: bool = True\n    \"\"\"Whether to include URLs as tokens. If False, URLs are completely excluded (not fragmented).\"\"\"\n\n    include_emails: bool = True\n    \"\"\"Whether to include email addresses as tokens. If False, emails are completely excluded (not fragmented).\"\"\"\n\n    # Output formatting\n    min_token_length: int = 1\n    \"\"\"Minimum length for tokens to be included in output.\"\"\"\n\n    max_token_length: Optional[int] = None\n    \"\"\"Maximum length for tokens. If None, no length limit is applied.\"\"\"\n\n    strip_whitespace: bool = True\n    \"\"\"Whether to strip leading/trailing whitespace from tokens.\"\"\"\n</code></pre>"},{"location":"reference/tokenizer/#services.tokenizer.core.types.TokenizerConfig.case_handling","title":"<code>case_handling = CaseHandling.LOWERCASE</code>  <code>pydantic-field</code>","text":"<p>How to handle character case during tokenization.</p>"},{"location":"reference/tokenizer/#services.tokenizer.core.types.TokenizerConfig.extract_cashtags","title":"<code>extract_cashtags = True</code>  <code>pydantic-field</code>","text":"<p>Whether to preserve cashtags ($AAPL) as single tokens. If False, splits into components.</p>"},{"location":"reference/tokenizer/#services.tokenizer.core.types.TokenizerConfig.extract_hashtags","title":"<code>extract_hashtags = True</code>  <code>pydantic-field</code>","text":"<p>Whether to preserve hashtags as single tokens. If False, splits into component words.</p>"},{"location":"reference/tokenizer/#services.tokenizer.core.types.TokenizerConfig.extract_mentions","title":"<code>extract_mentions = True</code>  <code>pydantic-field</code>","text":"<p>Whether to preserve @mentions as single tokens. If False, splits into component words.</p>"},{"location":"reference/tokenizer/#services.tokenizer.core.types.TokenizerConfig.fallback_language_family","title":"<code>fallback_language_family = LanguageFamily.MIXED</code>  <code>pydantic-field</code>","text":"<p>Default language family when detection fails or mixed content is found.</p>"},{"location":"reference/tokenizer/#services.tokenizer.core.types.TokenizerConfig.include_emails","title":"<code>include_emails = True</code>  <code>pydantic-field</code>","text":"<p>Whether to include email addresses as tokens. If False, emails are completely excluded (not fragmented).</p>"},{"location":"reference/tokenizer/#services.tokenizer.core.types.TokenizerConfig.include_emoji","title":"<code>include_emoji = False</code>  <code>pydantic-field</code>","text":"<p>Whether to include emoji characters as tokens.</p>"},{"location":"reference/tokenizer/#services.tokenizer.core.types.TokenizerConfig.include_numeric","title":"<code>include_numeric = True</code>  <code>pydantic-field</code>","text":"<p>Whether to include numeric tokens (integers, decimals, etc.).</p>"},{"location":"reference/tokenizer/#services.tokenizer.core.types.TokenizerConfig.include_punctuation","title":"<code>include_punctuation = False</code>  <code>pydantic-field</code>","text":"<p>Whether to include punctuation marks as separate tokens.</p>"},{"location":"reference/tokenizer/#services.tokenizer.core.types.TokenizerConfig.include_urls","title":"<code>include_urls = True</code>  <code>pydantic-field</code>","text":"<p>Whether to include URLs as tokens. If False, URLs are completely excluded (not fragmented).</p>"},{"location":"reference/tokenizer/#services.tokenizer.core.types.TokenizerConfig.max_token_length","title":"<code>max_token_length = None</code>  <code>pydantic-field</code>","text":"<p>Maximum length for tokens. If None, no length limit is applied.</p>"},{"location":"reference/tokenizer/#services.tokenizer.core.types.TokenizerConfig.min_token_length","title":"<code>min_token_length = 1</code>  <code>pydantic-field</code>","text":"<p>Minimum length for tokens to be included in output.</p>"},{"location":"reference/tokenizer/#services.tokenizer.core.types.TokenizerConfig.normalize_unicode","title":"<code>normalize_unicode = False</code>  <code>pydantic-field</code>","text":"<p>Whether to apply Unicode NFKC normalization. Default False to preserve unicode tricks for bot detection.</p>"},{"location":"reference/tokenizer/#services.tokenizer.core.types.TokenizerConfig.strip_whitespace","title":"<code>strip_whitespace = True</code>  <code>pydantic-field</code>","text":"<p>Whether to strip leading/trailing whitespace from tokens.</p>"},{"location":"reference/tokenizer/#services.tokenizer.basic","title":"<code>services.tokenizer.basic</code>","text":"<p>Basic tokenizer implementation.</p> <p>This module exports the BasicTokenizer implementation that provides fundamental Unicode-aware tokenization capabilities for social media text.</p> <p>Modules:</p> Name Description <code>patterns</code> <p>Regex patterns for text tokenization.</p> <code>tokenizer</code> <p>BasicTokenizer implementation.</p> <p>Classes:</p> Name Description <code>BasicTokenizer</code> <p>Unicode-aware basic tokenizer for social media text.</p> <code>TokenizerConfig</code> <p>Configuration for tokenizer behavior.</p> <p>Functions:</p> Name Description <code>create_basic_tokenizer</code> <p>Create a BasicTokenizer with optional configuration.</p> <code>get_patterns</code> <p>Get global TokenizerPatterns instance.</p> <code>tokenize_text</code> <p>Simple convenience function for basic text tokenization.</p>"},{"location":"reference/tokenizer/#services.tokenizer.basic.BasicTokenizer","title":"<code>BasicTokenizer</code>","text":"<p>               Bases: <code>AbstractTokenizer</code></p> <p>Unicode-aware basic tokenizer for social media text.</p> <p>This tokenizer handles mixed-script content, preserves social media entities (@mentions, #hashtags, URLs), and applies appropriate tokenization strategies for different script families.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize BasicTokenizer with configuration.</p> <code>tokenize</code> <p>Tokenize input text into a list of tokens.</p> Source code in <code>services/tokenizer/basic/tokenizer.py</code> <pre><code>class BasicTokenizer(AbstractTokenizer):\n    \"\"\"\n    Unicode-aware basic tokenizer for social media text.\n\n    This tokenizer handles mixed-script content, preserves social media entities\n    (@mentions, #hashtags, URLs), and applies appropriate tokenization strategies\n    for different script families.\n    \"\"\"\n\n    def __init__(self, config: Optional[TokenizerConfig] = None):\n        \"\"\"\n        Initialize BasicTokenizer with configuration.\n\n        Args:\n            config: Tokenizer configuration. If None, default config will be used.\n        \"\"\"\n        super().__init__(config)\n        self._patterns = get_patterns()\n\n        # Compile regex pattern for character-level script detection (performance optimization)\n        self._CHAR_LEVEL_PATTERN = re.compile(\n            r\"[\\u4e00-\\u9fff\"  # CJK Unified Ideographs\n            r\"\\u3400-\\u4dbf\"  # CJK Extension A\n            r\"\\u3040-\\u309f\"  # Hiragana\n            r\"\\u30a0-\\u30ff\"  # Katakana\n            r\"\\u0e00-\\u0e7f\"  # Thai\n            r\"\\u0e80-\\u0eff\"  # Lao\n            r\"\\u1000-\\u109f\"  # Myanmar\n            r\"\\u1780-\\u17ff]\"  # Khmer\n        )\n\n    def tokenize(self, text: str) -&gt; TokenList:\n        \"\"\"\n        Tokenize input text into a list of tokens.\n\n        Applies appropriate tokenization strategies for mixed-script content\n        while preserving social media entities and handling Unicode correctly.\n\n        Args:\n            text: Input text to tokenize\n\n        Returns:\n            List of tokens extracted from the input text in document order\n        \"\"\"\n        if not text:\n            return []\n\n        # Apply preprocessing\n        processed_text = self._preprocess_text(text)\n        if not processed_text:\n            return []\n\n        # Extract tokens using comprehensive regex pattern\n        tokens = self._extract_tokens(processed_text)\n\n        # Apply post-processing\n        return self._postprocess_tokens(tokens)\n\n    def _extract_tokens(self, text: str) -&gt; TokenList:\n        \"\"\"\n        Extract tokens using comprehensive regex patterns.\n        Preserves the original order of tokens as they appear in the input text.\n\n        Args:\n            text: Preprocessed text to tokenize\n\n        Returns:\n            List of extracted tokens in their original order\n        \"\"\"\n        return self._extract_tokens_ordered(text, LanguageFamily.MIXED)\n\n    def _is_char_level_script(self, char: str) -&gt; bool:\n        \"\"\"Check if character belongs to a character-level script.\"\"\"\n        return bool(self._CHAR_LEVEL_PATTERN.match(char))\n\n    def _get_char_script(self, char: str) -&gt; str:\n        \"\"\"\n        Get the script family for a character.\n\n        Args:\n            char: Character to analyze\n\n        Returns:\n            Script family name\n        \"\"\"\n        code_point = ord(char)\n\n        # Latin script\n        if (\n            (0x0041 &lt;= code_point &lt;= 0x007A)\n            or (0x00C0 &lt;= code_point &lt;= 0x024F)\n            or (0x1E00 &lt;= code_point &lt;= 0x1EFF)\n        ):\n            return \"latin\"\n\n        # Korean Hangul (space-separated, NOT character-level!)\n        elif 0xAC00 &lt;= code_point &lt;= 0xD7AF:\n            return \"korean\"\n\n        # Character-level scripts (CJK, Thai, etc.)\n        elif self._is_char_level_script(char):\n            return \"cjk\"\n\n        # Arabic script\n        elif (\n            (0x0600 &lt;= code_point &lt;= 0x06FF)\n            or (0x0750 &lt;= code_point &lt;= 0x077F)\n            or (0x08A0 &lt;= code_point &lt;= 0x08FF)\n        ):\n            return \"arabic\"\n\n        else:\n            return \"other\"\n\n    def _extract_tokens_ordered(\n        self, text: str, language_family: LanguageFamily\n    ) -&gt; TokenList:\n        \"\"\"\n        Extract tokens preserving their original order in the text.\n\n        Uses a single comprehensive regex pattern to find ALL tokens in document order,\n        eliminating the need for complex segmentation and reassembly logic.\n        This is the Phase 2 optimization that removes O(n\u00d7segments) complexity.\n\n        Args:\n            text: Preprocessed text to tokenize\n            language_family: Detected language family for the full text\n\n        Returns:\n            List of extracted tokens in their original order\n        \"\"\"\n        if not text.strip():\n            return []\n\n        # Remove excluded entities (URLs/emails) from text if they are disabled\n        # This prevents them from being tokenized into component words\n        exclusion_pattern = self._patterns.get_exclusion_pattern(self._config)\n        if exclusion_pattern:\n            # Replace excluded entities with spaces to maintain word boundaries\n            text = exclusion_pattern.sub(\" \", text)\n            # Clean up multiple spaces\n            text = \" \".join(text.split())\n\n        if not text.strip():\n            return []\n\n        # Get comprehensive pattern based on configuration\n        # This single pattern finds ALL tokens in document order\n        comprehensive_pattern = self._patterns.get_comprehensive_pattern(self._config)\n\n        # Single regex call gets all tokens in order - this is the key optimization!\n        raw_tokens = comprehensive_pattern.findall(text)\n\n        # If no tokens were found but input has content, use fallback for edge cases\n        if not raw_tokens and text.strip():\n            # For pure punctuation or unrecognized content, return as single token\n            # This maintains compatibility with old tokenizer behavior for edge cases\n            return [text.strip()]\n\n        # Apply postprocessing for language-specific behavior and configuration filtering\n        tokens = []\n        for token in raw_tokens:\n            if not token.strip():\n                continue\n\n            # Clean URLs by removing trailing punctuation\n            if self._is_url_like(token):\n                token = self._clean_url_token(token)\n\n            # For character-level scripts, break down multi-character tokens into individual characters\n            # This maintains compatibility with existing test expectations\n            if (\n                language_family == LanguageFamily.CJK\n                and self._contains_char_level_chars(token)\n            ):\n                # Only break down pure character-level tokens, not mixed tokens\n                if self._is_pure_char_level_token(token):\n                    tokens.extend(list(token))\n                else:\n                    # Mixed token - keep as is but process character-level parts\n                    tokens.append(token)\n            elif language_family == LanguageFamily.MIXED:\n                # For mixed script, break down character-level script parts but keep Latin parts whole\n                processed_tokens = self._process_mixed_script_token(token)\n                tokens.extend(processed_tokens)\n            else:\n                tokens.append(token)\n\n        return [token for token in tokens if token.strip()]\n\n    def _is_punctuation_only(self, token: str) -&gt; bool:\n        \"\"\"Check if token contains only punctuation.\"\"\"\n        punctuation_chars = \".!?;:,()[]{}\\\"'-~`@#$%^&amp;*+=&lt;&gt;/|\\\\\"\n        return all(c in punctuation_chars for c in token)\n\n    def _is_numeric_only(self, token: str) -&gt; bool:\n        \"\"\"Check if token is purely numeric.\"\"\"\n        return (\n            token.replace(\".\", \"\")\n            .replace(\",\", \"\")\n            .replace(\"%\", \"\")\n            .replace(\"$\", \"\")\n            .isdigit()\n        )\n\n    def _is_url_like(self, token: str) -&gt; bool:\n        \"\"\"Check if token looks like a URL.\"\"\"\n        # Don't classify emails as URLs\n        if self._is_email_like(token):\n            return False\n\n        # Explicit URL indicators (http://, https://, www., or protocol markers)\n        if token.startswith((\"http://\", \"https://\", \"www.\")) or \"://\" in token:\n            return True\n\n        # Domain-like patterns (e.g., \"example.com\")\n        # But NOT abbreviations (e.g., \"U.S.\", \"c.e.o.s\")\n        # Heuristic: URLs have at least one period NOT followed by a single uppercase/lowercase letter\n        # This allows \"example.com\" but excludes \"U.S.\" and \"c.e.o.s\"\n        if (\n            token.count(\".\") &gt;= 1\n            and any(c.isalpha() for c in token)\n            and \"@\" not in token\n        ):\n            # Check if this looks like an abbreviation (single letters between periods)\n            # Pattern: letter(s).letter(s).letter(s) where segments are 1-3 chars\n            abbreviation_pattern = r\"^[a-z]{1,3}(?:\\.[a-z]{1,3})+\\.?$\"\n\n            if re.match(abbreviation_pattern, token, re.IGNORECASE):\n                return False  # This is an abbreviation, not a URL\n            # If it has a period and looks like a domain, it's URL-like\n            return True\n\n        return False\n\n    def _is_email_like(self, token: str) -&gt; bool:\n        \"\"\"Check if token looks like an email address.\"\"\"\n        return \"@\" in token and \".\" in token and not token.startswith(\"@\")\n\n    def _clean_url_token(self, url_token: str) -&gt; str:\n        \"\"\"Remove trailing punctuation from URL tokens.\"\"\"\n        trailing_punctuation = \".!?;:,)]}\\\"'\"\n        return url_token.rstrip(trailing_punctuation)\n\n    def _contains_char_level_chars(self, token: str) -&gt; bool:\n        \"\"\"Check if token contains any character-level script characters.\"\"\"\n        return any(self._is_char_level_script(char) for char in token)\n\n    def _is_pure_char_level_token(self, token: str) -&gt; bool:\n        \"\"\"Check if token contains only character-level script characters.\"\"\"\n        return all(self._is_char_level_script(char) or char.isspace() for char in token)\n\n    def _process_mixed_script_token(self, token: str) -&gt; TokenList:\n        \"\"\"Process mixed script tokens by breaking down character-level script parts.\"\"\"\n        if not self._contains_char_level_chars(token):\n            return [token]\n\n        # Check if token mixes Latin with CJK\n        has_latin = any(c.isascii() and c.isalpha() for c in token)\n        has_cjk = any(self._is_char_level_script(c) for c in token)\n\n        # Don't apply mixed-script preservation to social media entities\n        is_social_entity = token.startswith((\"@\", \"#\", \"$\"))\n\n        if has_latin and has_cjk and not is_social_entity:\n            # Mixed script - keep intact (brand names, bot tricks)\n            return [token]\n\n        # Rest of existing logic for pure CJK tokens...\n        result = []\n        current_token = \"\"\n        current_is_cjk = None\n\n        for char in token:\n            char_is_cjk = self._is_char_level_script(char)\n\n            if current_is_cjk is None:\n                current_is_cjk = char_is_cjk\n                current_token = char\n            elif char_is_cjk == current_is_cjk:\n                current_token += char\n            else:\n                # Script change\n                if current_token.strip():\n                    if current_is_cjk and len(current_token) &gt; 1:\n                        # Break CJK into individual characters\n                        result.extend(list(current_token))\n                    else:\n                        result.append(current_token)\n                current_token = char\n                current_is_cjk = char_is_cjk\n\n        # Handle final token\n        if current_token.strip():\n            if current_is_cjk and len(current_token) &gt; 1:\n                result.extend(list(current_token))\n            else:\n                result.append(current_token)\n\n        return result\n\n    def _postprocess_tokens(self, tokens: TokenList) -&gt; TokenList:\n        \"\"\"\n        Apply post-processing to extracted tokens.\n\n        Args:\n            tokens: List of raw tokens\n\n        Returns:\n            Processed token list\n        \"\"\"\n        if not tokens:\n            return tokens\n\n        # Apply base class post-processing (length filtering, whitespace stripping, etc.)\n        return super()._postprocess_tokens(tokens)\n</code></pre>"},{"location":"reference/tokenizer/#services.tokenizer.basic.BasicTokenizer.__init__","title":"<code>__init__(config=None)</code>","text":"<p>Initialize BasicTokenizer with configuration.</p> <p>Parameters:</p> Name Type Description Default <code>Optional[TokenizerConfig]</code> <p>Tokenizer configuration. If None, default config will be used.</p> <code>None</code> Source code in <code>services/tokenizer/basic/tokenizer.py</code> <pre><code>def __init__(self, config: Optional[TokenizerConfig] = None):\n    \"\"\"\n    Initialize BasicTokenizer with configuration.\n\n    Args:\n        config: Tokenizer configuration. If None, default config will be used.\n    \"\"\"\n    super().__init__(config)\n    self._patterns = get_patterns()\n\n    # Compile regex pattern for character-level script detection (performance optimization)\n    self._CHAR_LEVEL_PATTERN = re.compile(\n        r\"[\\u4e00-\\u9fff\"  # CJK Unified Ideographs\n        r\"\\u3400-\\u4dbf\"  # CJK Extension A\n        r\"\\u3040-\\u309f\"  # Hiragana\n        r\"\\u30a0-\\u30ff\"  # Katakana\n        r\"\\u0e00-\\u0e7f\"  # Thai\n        r\"\\u0e80-\\u0eff\"  # Lao\n        r\"\\u1000-\\u109f\"  # Myanmar\n        r\"\\u1780-\\u17ff]\"  # Khmer\n    )\n</code></pre>"},{"location":"reference/tokenizer/#services.tokenizer.basic.BasicTokenizer.__init__(config)","title":"<code>config</code>","text":""},{"location":"reference/tokenizer/#services.tokenizer.basic.BasicTokenizer.tokenize","title":"<code>tokenize(text)</code>","text":"<p>Tokenize input text into a list of tokens.</p> <p>Applies appropriate tokenization strategies for mixed-script content while preserving social media entities and handling Unicode correctly.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>Input text to tokenize</p> required <p>Returns:</p> Type Description <code>TokenList</code> <p>List of tokens extracted from the input text in document order</p> Source code in <code>services/tokenizer/basic/tokenizer.py</code> <pre><code>def tokenize(self, text: str) -&gt; TokenList:\n    \"\"\"\n    Tokenize input text into a list of tokens.\n\n    Applies appropriate tokenization strategies for mixed-script content\n    while preserving social media entities and handling Unicode correctly.\n\n    Args:\n        text: Input text to tokenize\n\n    Returns:\n        List of tokens extracted from the input text in document order\n    \"\"\"\n    if not text:\n        return []\n\n    # Apply preprocessing\n    processed_text = self._preprocess_text(text)\n    if not processed_text:\n        return []\n\n    # Extract tokens using comprehensive regex pattern\n    tokens = self._extract_tokens(processed_text)\n\n    # Apply post-processing\n    return self._postprocess_tokens(tokens)\n</code></pre>"},{"location":"reference/tokenizer/#services.tokenizer.basic.BasicTokenizer.tokenize(text)","title":"<code>text</code>","text":""},{"location":"reference/tokenizer/#services.tokenizer.basic.TokenizerConfig","title":"<code>TokenizerConfig</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for tokenizer behavior.</p> <p>Controls all aspects of text tokenization including script handling, social media entity processing, and output formatting.</p> <p>Social Media Entity Behavior: - extract_hashtags/extract_mentions: When False, splits into component words - include_urls/include_emails: When False, completely excludes (no fragmentation)</p> <p>Fields:</p> <ul> <li> <code>fallback_language_family</code>                 (<code>LanguageFamily</code>)             </li> <li> <code>include_punctuation</code>                 (<code>bool</code>)             </li> <li> <code>include_numeric</code>                 (<code>bool</code>)             </li> <li> <code>include_emoji</code>                 (<code>bool</code>)             </li> <li> <code>case_handling</code>                 (<code>CaseHandling</code>)             </li> <li> <code>normalize_unicode</code>                 (<code>bool</code>)             </li> <li> <code>extract_hashtags</code>                 (<code>bool</code>)             </li> <li> <code>extract_mentions</code>                 (<code>bool</code>)             </li> <li> <code>extract_cashtags</code>                 (<code>bool</code>)             </li> <li> <code>include_urls</code>                 (<code>bool</code>)             </li> <li> <code>include_emails</code>                 (<code>bool</code>)             </li> <li> <code>min_token_length</code>                 (<code>int</code>)             </li> <li> <code>max_token_length</code>                 (<code>Optional[int]</code>)             </li> <li> <code>strip_whitespace</code>                 (<code>bool</code>)             </li> </ul> Source code in <code>services/tokenizer/core/types.py</code> <pre><code>class TokenizerConfig(BaseModel):\n    \"\"\"Configuration for tokenizer behavior.\n\n    Controls all aspects of text tokenization including script handling,\n    social media entity processing, and output formatting.\n\n    Social Media Entity Behavior:\n    - extract_hashtags/extract_mentions: When False, splits into component words\n    - include_urls/include_emails: When False, completely excludes (no fragmentation)\n    \"\"\"\n\n    # Language detection settings\n    fallback_language_family: LanguageFamily = LanguageFamily.MIXED\n    \"\"\"Default language family when detection fails or mixed content is found.\"\"\"\n\n    # Token type filtering\n    include_punctuation: bool = False\n    \"\"\"Whether to include punctuation marks as separate tokens.\"\"\"\n\n    include_numeric: bool = True\n    \"\"\"Whether to include numeric tokens (integers, decimals, etc.).\"\"\"\n\n    include_emoji: bool = False\n    \"\"\"Whether to include emoji characters as tokens.\"\"\"\n\n    # Text preprocessing\n    case_handling: CaseHandling = CaseHandling.LOWERCASE\n    \"\"\"How to handle character case during tokenization.\"\"\"\n\n    normalize_unicode: bool = False\n    \"\"\"Whether to apply Unicode NFKC normalization. Default False to preserve unicode tricks for bot detection.\"\"\"\n\n    # Social media features\n    extract_hashtags: bool = True\n    \"\"\"Whether to preserve hashtags as single tokens. If False, splits into component words.\"\"\"\n\n    extract_mentions: bool = True\n    \"\"\"Whether to preserve @mentions as single tokens. If False, splits into component words.\"\"\"\n\n    extract_cashtags: bool = True\n    \"\"\"Whether to preserve cashtags ($AAPL) as single tokens. If False, splits into components.\"\"\"\n\n    include_urls: bool = True\n    \"\"\"Whether to include URLs as tokens. If False, URLs are completely excluded (not fragmented).\"\"\"\n\n    include_emails: bool = True\n    \"\"\"Whether to include email addresses as tokens. If False, emails are completely excluded (not fragmented).\"\"\"\n\n    # Output formatting\n    min_token_length: int = 1\n    \"\"\"Minimum length for tokens to be included in output.\"\"\"\n\n    max_token_length: Optional[int] = None\n    \"\"\"Maximum length for tokens. If None, no length limit is applied.\"\"\"\n\n    strip_whitespace: bool = True\n    \"\"\"Whether to strip leading/trailing whitespace from tokens.\"\"\"\n</code></pre>"},{"location":"reference/tokenizer/#services.tokenizer.basic.TokenizerConfig.case_handling","title":"<code>case_handling = CaseHandling.LOWERCASE</code>  <code>pydantic-field</code>","text":"<p>How to handle character case during tokenization.</p>"},{"location":"reference/tokenizer/#services.tokenizer.basic.TokenizerConfig.extract_cashtags","title":"<code>extract_cashtags = True</code>  <code>pydantic-field</code>","text":"<p>Whether to preserve cashtags ($AAPL) as single tokens. If False, splits into components.</p>"},{"location":"reference/tokenizer/#services.tokenizer.basic.TokenizerConfig.extract_hashtags","title":"<code>extract_hashtags = True</code>  <code>pydantic-field</code>","text":"<p>Whether to preserve hashtags as single tokens. If False, splits into component words.</p>"},{"location":"reference/tokenizer/#services.tokenizer.basic.TokenizerConfig.extract_mentions","title":"<code>extract_mentions = True</code>  <code>pydantic-field</code>","text":"<p>Whether to preserve @mentions as single tokens. If False, splits into component words.</p>"},{"location":"reference/tokenizer/#services.tokenizer.basic.TokenizerConfig.fallback_language_family","title":"<code>fallback_language_family = LanguageFamily.MIXED</code>  <code>pydantic-field</code>","text":"<p>Default language family when detection fails or mixed content is found.</p>"},{"location":"reference/tokenizer/#services.tokenizer.basic.TokenizerConfig.include_emails","title":"<code>include_emails = True</code>  <code>pydantic-field</code>","text":"<p>Whether to include email addresses as tokens. If False, emails are completely excluded (not fragmented).</p>"},{"location":"reference/tokenizer/#services.tokenizer.basic.TokenizerConfig.include_emoji","title":"<code>include_emoji = False</code>  <code>pydantic-field</code>","text":"<p>Whether to include emoji characters as tokens.</p>"},{"location":"reference/tokenizer/#services.tokenizer.basic.TokenizerConfig.include_numeric","title":"<code>include_numeric = True</code>  <code>pydantic-field</code>","text":"<p>Whether to include numeric tokens (integers, decimals, etc.).</p>"},{"location":"reference/tokenizer/#services.tokenizer.basic.TokenizerConfig.include_punctuation","title":"<code>include_punctuation = False</code>  <code>pydantic-field</code>","text":"<p>Whether to include punctuation marks as separate tokens.</p>"},{"location":"reference/tokenizer/#services.tokenizer.basic.TokenizerConfig.include_urls","title":"<code>include_urls = True</code>  <code>pydantic-field</code>","text":"<p>Whether to include URLs as tokens. If False, URLs are completely excluded (not fragmented).</p>"},{"location":"reference/tokenizer/#services.tokenizer.basic.TokenizerConfig.max_token_length","title":"<code>max_token_length = None</code>  <code>pydantic-field</code>","text":"<p>Maximum length for tokens. If None, no length limit is applied.</p>"},{"location":"reference/tokenizer/#services.tokenizer.basic.TokenizerConfig.min_token_length","title":"<code>min_token_length = 1</code>  <code>pydantic-field</code>","text":"<p>Minimum length for tokens to be included in output.</p>"},{"location":"reference/tokenizer/#services.tokenizer.basic.TokenizerConfig.normalize_unicode","title":"<code>normalize_unicode = False</code>  <code>pydantic-field</code>","text":"<p>Whether to apply Unicode NFKC normalization. Default False to preserve unicode tricks for bot detection.</p>"},{"location":"reference/tokenizer/#services.tokenizer.basic.TokenizerConfig.strip_whitespace","title":"<code>strip_whitespace = True</code>  <code>pydantic-field</code>","text":"<p>Whether to strip leading/trailing whitespace from tokens.</p>"},{"location":"reference/tokenizer/#services.tokenizer.basic.create_basic_tokenizer","title":"<code>create_basic_tokenizer(config=None)</code>","text":"<p>Create a BasicTokenizer with optional configuration.</p> Source code in <code>services/tokenizer/basic/__init__.py</code> <pre><code>def create_basic_tokenizer(config: TokenizerConfig | None = None) -&gt; BasicTokenizer:\n    \"\"\"Create a BasicTokenizer with optional configuration.\"\"\"\n    if config is None:\n        config = TokenizerConfig()\n    return BasicTokenizer(config)\n</code></pre>"},{"location":"reference/tokenizer/#services.tokenizer.basic.get_patterns","title":"<code>get_patterns()</code>","text":"<p>Get global TokenizerPatterns instance.</p> <p>Returns:</p> Type Description <code>TokenizerPatterns</code> <p>Singleton TokenizerPatterns instance</p> Source code in <code>services/tokenizer/basic/patterns.py</code> <pre><code>def get_patterns() -&gt; TokenizerPatterns:\n    \"\"\"\n    Get global TokenizerPatterns instance.\n\n    Returns:\n        Singleton TokenizerPatterns instance\n    \"\"\"\n    global _global_patterns\n    if _global_patterns is None:\n        _global_patterns = TokenizerPatterns()\n    return _global_patterns\n</code></pre>"},{"location":"reference/tokenizer/#services.tokenizer.basic.patterns","title":"<code>patterns</code>","text":"<p>Regex patterns for text tokenization.</p> <p>This module contains compiled regular expressions for extracting different types of tokens from social media text, with fallback support for both regex and re modules.</p> <p>Classes:</p> Name Description <code>TokenizerPatterns</code> <p>Compiled regex patterns for tokenization.</p> <p>Functions:</p> Name Description <code>get_patterns</code> <p>Get global TokenizerPatterns instance.</p>"},{"location":"reference/tokenizer/#services.tokenizer.basic.patterns.TokenizerPatterns","title":"<code>TokenizerPatterns</code>","text":"<p>Compiled regex patterns for tokenization.</p> <p>Organizes patterns logically and provides efficient compiled regex objects for different token types found in social media text.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize and compile all tokenization patterns.</p> <code>get_comprehensive_pattern</code> <p>Build comprehensive tokenization pattern based on configuration.</p> <code>get_exclusion_pattern</code> <p>Build pattern to identify and skip excluded entities in text.</p> <code>get_pattern</code> <p>Get compiled pattern by name.</p> <code>list_patterns</code> <p>Get list of available pattern names.</p> Source code in <code>services/tokenizer/basic/patterns.py</code> <pre><code>class TokenizerPatterns:\n    \"\"\"\n    Compiled regex patterns for tokenization.\n\n    Organizes patterns logically and provides efficient compiled regex objects\n    for different token types found in social media text.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize and compile all tokenization patterns.\"\"\"\n        self._patterns: Dict[str, Any] = {}\n        self._compile_patterns()\n\n    def get_pattern(self, pattern_name: str) -&gt; Any:\n        \"\"\"\n        Get compiled pattern by name.\n\n        Args:\n            pattern_name: Name of the pattern to retrieve\n\n        Returns:\n            Compiled regex pattern\n\n        Raises:\n            KeyError: If pattern name is not found\n        \"\"\"\n        if pattern_name not in self._patterns:\n            raise KeyError(f\"Pattern '{pattern_name}' not found\")\n        return self._patterns[pattern_name]\n\n    def get_comprehensive_pattern(self, config) -&gt; Any:\n        \"\"\"\n        Build comprehensive tokenization pattern based on configuration.\n\n        This creates a single regex pattern that finds ALL tokens in document order,\n        eliminating the need for segmentation and reassembly. URLs and emails are\n        conditionally included in the regex itself based on configuration, avoiding\n        the need for post-processing filtering.\n\n        Args:\n            config: TokenizerConfig specifying which token types to include\n\n        Returns:\n            Compiled regex pattern that matches all desired token types in priority order\n        \"\"\"\n        # Check cache first\n        cache_key = hash(frozenset(config.model_dump().items()))\n        if cache_key in _comprehensive_pattern_cache:\n            return _comprehensive_pattern_cache[cache_key]\n\n        pattern_parts = []\n\n        # Conditionally add URL and email patterns based on configuration\n        # This eliminates the need for post-processing filtering\n        if config.include_urls:\n            pattern_parts.append(self.get_pattern(\"url\").pattern)\n\n        if config.include_emails:\n            pattern_parts.append(self.get_pattern(\"email\").pattern)\n\n        if config.extract_mentions:\n            pattern_parts.append(self.get_pattern(\"mention\").pattern)\n\n        if config.extract_hashtags:\n            pattern_parts.append(self.get_pattern(\"hashtag\").pattern)\n\n        # Cashtag pattern BEFORE numeric pattern (priority matters)\n        if config.extract_cashtags:\n            pattern_parts.append(self.get_pattern(\"cashtag\").pattern)\n\n        if config.include_emoji:\n            pattern_parts.append(self.get_pattern(\"emoji\").pattern)\n\n        if config.include_numeric:\n            pattern_parts.append(self.get_pattern(\"numeric\").pattern)\n\n        # Always include word pattern (this is the core tokenization)\n        pattern_parts.append(self.get_pattern(\"word\").pattern)\n\n        if config.include_punctuation:\n            pattern_parts.append(self.get_pattern(\"punctuation\").pattern)\n\n        # Don't add the greedy fallback - let configuration control what gets captured\n\n        # Combine patterns with alternation (| operator)\n        comprehensive_pattern = \"(?:\" + \"|\".join(pattern_parts) + \")\"\n\n        try:\n            compiled_pattern = REGEX_MODULE.compile(\n                comprehensive_pattern, REGEX_MODULE.IGNORECASE\n            )\n        except Exception:\n            # Fallback to standard re module\n            if REGEX_AVAILABLE and REGEX_MODULE is not re:\n                try:\n                    compiled_pattern = re.compile(comprehensive_pattern, re.IGNORECASE)\n                except Exception:\n                    # Ultimate fallback - just match words\n                    compiled_pattern = re.compile(r\"\\S+\", re.IGNORECASE)\n            else:\n                compiled_pattern = re.compile(r\"\\S+\", re.IGNORECASE)\n\n        # Store in cache\n        _comprehensive_pattern_cache[cache_key] = compiled_pattern\n        return compiled_pattern\n\n    def get_exclusion_pattern(self, config) -&gt; Any:\n        \"\"\"\n        Build pattern to identify and skip excluded entities in text.\n\n        This creates a pattern that matches URLs and emails that should be excluded,\n        allowing the tokenizer to skip over them entirely instead of breaking them\n        into component words.\n\n        Args:\n            config: TokenizerConfig specifying which token types to exclude\n\n        Returns:\n            Compiled regex pattern that matches excluded entities, or None if no exclusions\n        \"\"\"\n        # Check cache first\n        cache_key = hash(frozenset(config.model_dump().items()))\n        if cache_key in _exclusion_pattern_cache:\n            return _exclusion_pattern_cache[cache_key]\n\n        exclusion_parts = []\n\n        if not config.include_urls:\n            exclusion_parts.append(self.get_pattern(\"url\").pattern)\n\n        if not config.include_emails:\n            exclusion_parts.append(self.get_pattern(\"email\").pattern)\n\n        if not config.include_numeric:\n            exclusion_parts.append(self.get_pattern(\"numeric\").pattern)\n\n        if not exclusion_parts:\n            # Cache None result\n            _exclusion_pattern_cache[cache_key] = None\n            return None\n\n        # Combine exclusion patterns\n        exclusion_pattern = \"(?:\" + \"|\".join(exclusion_parts) + \")\"\n\n        try:\n            result = REGEX_MODULE.compile(exclusion_pattern, REGEX_MODULE.IGNORECASE)\n        except Exception:\n            # Fallback to standard re module\n            if REGEX_AVAILABLE and REGEX_MODULE is not re:\n                try:\n                    result = re.compile(exclusion_pattern, re.IGNORECASE)\n                except Exception:\n                    result = None\n            else:\n                result = None\n\n        # Store in cache\n        _exclusion_pattern_cache[cache_key] = result\n        return result\n\n    def list_patterns(self) -&gt; List[str]:\n        \"\"\"Get list of available pattern names.\"\"\"\n        return list(self._patterns.keys())\n\n    def _compile_patterns(self):\n        \"\"\"Compile all regex patterns with fallback support.\"\"\"\n\n        # Compile patterns with fallback handling\n        patterns_to_compile = {\n            \"url\": URL_PATTERN,\n            \"email\": EMAIL_PATTERN,\n            \"mention\": MENTION_PATTERN,\n            \"hashtag\": HASHTAG_PATTERN,\n            \"cashtag\": CASHTAG_PATTERN,\n            \"emoji\": EMOJI_PATTERN,\n            \"numeric\": NUMERIC_PATTERN,\n            \"word\": WORD_PATTERN,\n            \"latin_word\": LATIN_WORD_PATTERN,\n            \"cjk_chars\": CJK_PATTERN,\n            \"arabic_chars\": ARABIC_PATTERN,\n            \"punctuation\": PUNCTUATION_PATTERN,\n            \"social_media\": SOCIAL_MEDIA_PATTERN,\n            \"word_boundary\": WORD_BOUNDARY_PATTERN,\n            \"combined_social_entities\": COMBINED_SOCIAL_ENTITIES_PATTERN,\n        }\n\n        for name, pattern in patterns_to_compile.items():\n            try:\n                self._patterns[name] = REGEX_MODULE.compile(\n                    pattern, REGEX_MODULE.IGNORECASE\n                )\n            except Exception:\n                # If compilation fails with regex module, fall back to re\n                if REGEX_AVAILABLE and REGEX_MODULE is not re:\n                    try:\n                        self._patterns[name] = re.compile(pattern, re.IGNORECASE)\n                    except Exception:\n                        # If both fail, create a simple fallback\n                        self._patterns[name] = re.compile(r\"\\S+\", re.IGNORECASE)\n                else:\n                    # Already using re module, create simple fallback\n                    self._patterns[name] = re.compile(r\"\\S+\", re.IGNORECASE)\n</code></pre>"},{"location":"reference/tokenizer/#services.tokenizer.basic.patterns.TokenizerPatterns.__init__","title":"<code>__init__()</code>","text":"<p>Initialize and compile all tokenization patterns.</p> Source code in <code>services/tokenizer/basic/patterns.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize and compile all tokenization patterns.\"\"\"\n    self._patterns: Dict[str, Any] = {}\n    self._compile_patterns()\n</code></pre>"},{"location":"reference/tokenizer/#services.tokenizer.basic.patterns.TokenizerPatterns.get_comprehensive_pattern","title":"<code>get_comprehensive_pattern(config)</code>","text":"<p>Build comprehensive tokenization pattern based on configuration.</p> <p>This creates a single regex pattern that finds ALL tokens in document order, eliminating the need for segmentation and reassembly. URLs and emails are conditionally included in the regex itself based on configuration, avoiding the need for post-processing filtering.</p> <p>Parameters:</p> Name Type Description Default <p>TokenizerConfig specifying which token types to include</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Compiled regex pattern that matches all desired token types in priority order</p> Source code in <code>services/tokenizer/basic/patterns.py</code> <pre><code>def get_comprehensive_pattern(self, config) -&gt; Any:\n    \"\"\"\n    Build comprehensive tokenization pattern based on configuration.\n\n    This creates a single regex pattern that finds ALL tokens in document order,\n    eliminating the need for segmentation and reassembly. URLs and emails are\n    conditionally included in the regex itself based on configuration, avoiding\n    the need for post-processing filtering.\n\n    Args:\n        config: TokenizerConfig specifying which token types to include\n\n    Returns:\n        Compiled regex pattern that matches all desired token types in priority order\n    \"\"\"\n    # Check cache first\n    cache_key = hash(frozenset(config.model_dump().items()))\n    if cache_key in _comprehensive_pattern_cache:\n        return _comprehensive_pattern_cache[cache_key]\n\n    pattern_parts = []\n\n    # Conditionally add URL and email patterns based on configuration\n    # This eliminates the need for post-processing filtering\n    if config.include_urls:\n        pattern_parts.append(self.get_pattern(\"url\").pattern)\n\n    if config.include_emails:\n        pattern_parts.append(self.get_pattern(\"email\").pattern)\n\n    if config.extract_mentions:\n        pattern_parts.append(self.get_pattern(\"mention\").pattern)\n\n    if config.extract_hashtags:\n        pattern_parts.append(self.get_pattern(\"hashtag\").pattern)\n\n    # Cashtag pattern BEFORE numeric pattern (priority matters)\n    if config.extract_cashtags:\n        pattern_parts.append(self.get_pattern(\"cashtag\").pattern)\n\n    if config.include_emoji:\n        pattern_parts.append(self.get_pattern(\"emoji\").pattern)\n\n    if config.include_numeric:\n        pattern_parts.append(self.get_pattern(\"numeric\").pattern)\n\n    # Always include word pattern (this is the core tokenization)\n    pattern_parts.append(self.get_pattern(\"word\").pattern)\n\n    if config.include_punctuation:\n        pattern_parts.append(self.get_pattern(\"punctuation\").pattern)\n\n    # Don't add the greedy fallback - let configuration control what gets captured\n\n    # Combine patterns with alternation (| operator)\n    comprehensive_pattern = \"(?:\" + \"|\".join(pattern_parts) + \")\"\n\n    try:\n        compiled_pattern = REGEX_MODULE.compile(\n            comprehensive_pattern, REGEX_MODULE.IGNORECASE\n        )\n    except Exception:\n        # Fallback to standard re module\n        if REGEX_AVAILABLE and REGEX_MODULE is not re:\n            try:\n                compiled_pattern = re.compile(comprehensive_pattern, re.IGNORECASE)\n            except Exception:\n                # Ultimate fallback - just match words\n                compiled_pattern = re.compile(r\"\\S+\", re.IGNORECASE)\n        else:\n            compiled_pattern = re.compile(r\"\\S+\", re.IGNORECASE)\n\n    # Store in cache\n    _comprehensive_pattern_cache[cache_key] = compiled_pattern\n    return compiled_pattern\n</code></pre>"},{"location":"reference/tokenizer/#services.tokenizer.basic.patterns.TokenizerPatterns.get_comprehensive_pattern(config)","title":"<code>config</code>","text":""},{"location":"reference/tokenizer/#services.tokenizer.basic.patterns.TokenizerPatterns.get_exclusion_pattern","title":"<code>get_exclusion_pattern(config)</code>","text":"<p>Build pattern to identify and skip excluded entities in text.</p> <p>This creates a pattern that matches URLs and emails that should be excluded, allowing the tokenizer to skip over them entirely instead of breaking them into component words.</p> <p>Parameters:</p> Name Type Description Default <p>TokenizerConfig specifying which token types to exclude</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Compiled regex pattern that matches excluded entities, or None if no exclusions</p> Source code in <code>services/tokenizer/basic/patterns.py</code> <pre><code>def get_exclusion_pattern(self, config) -&gt; Any:\n    \"\"\"\n    Build pattern to identify and skip excluded entities in text.\n\n    This creates a pattern that matches URLs and emails that should be excluded,\n    allowing the tokenizer to skip over them entirely instead of breaking them\n    into component words.\n\n    Args:\n        config: TokenizerConfig specifying which token types to exclude\n\n    Returns:\n        Compiled regex pattern that matches excluded entities, or None if no exclusions\n    \"\"\"\n    # Check cache first\n    cache_key = hash(frozenset(config.model_dump().items()))\n    if cache_key in _exclusion_pattern_cache:\n        return _exclusion_pattern_cache[cache_key]\n\n    exclusion_parts = []\n\n    if not config.include_urls:\n        exclusion_parts.append(self.get_pattern(\"url\").pattern)\n\n    if not config.include_emails:\n        exclusion_parts.append(self.get_pattern(\"email\").pattern)\n\n    if not config.include_numeric:\n        exclusion_parts.append(self.get_pattern(\"numeric\").pattern)\n\n    if not exclusion_parts:\n        # Cache None result\n        _exclusion_pattern_cache[cache_key] = None\n        return None\n\n    # Combine exclusion patterns\n    exclusion_pattern = \"(?:\" + \"|\".join(exclusion_parts) + \")\"\n\n    try:\n        result = REGEX_MODULE.compile(exclusion_pattern, REGEX_MODULE.IGNORECASE)\n    except Exception:\n        # Fallback to standard re module\n        if REGEX_AVAILABLE and REGEX_MODULE is not re:\n            try:\n                result = re.compile(exclusion_pattern, re.IGNORECASE)\n            except Exception:\n                result = None\n        else:\n            result = None\n\n    # Store in cache\n    _exclusion_pattern_cache[cache_key] = result\n    return result\n</code></pre>"},{"location":"reference/tokenizer/#services.tokenizer.basic.patterns.TokenizerPatterns.get_exclusion_pattern(config)","title":"<code>config</code>","text":""},{"location":"reference/tokenizer/#services.tokenizer.basic.patterns.TokenizerPatterns.get_pattern","title":"<code>get_pattern(pattern_name)</code>","text":"<p>Get compiled pattern by name.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>Name of the pattern to retrieve</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Compiled regex pattern</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If pattern name is not found</p> Source code in <code>services/tokenizer/basic/patterns.py</code> <pre><code>def get_pattern(self, pattern_name: str) -&gt; Any:\n    \"\"\"\n    Get compiled pattern by name.\n\n    Args:\n        pattern_name: Name of the pattern to retrieve\n\n    Returns:\n        Compiled regex pattern\n\n    Raises:\n        KeyError: If pattern name is not found\n    \"\"\"\n    if pattern_name not in self._patterns:\n        raise KeyError(f\"Pattern '{pattern_name}' not found\")\n    return self._patterns[pattern_name]\n</code></pre>"},{"location":"reference/tokenizer/#services.tokenizer.basic.patterns.TokenizerPatterns.get_pattern(pattern_name)","title":"<code>pattern_name</code>","text":""},{"location":"reference/tokenizer/#services.tokenizer.basic.patterns.TokenizerPatterns.list_patterns","title":"<code>list_patterns()</code>","text":"<p>Get list of available pattern names.</p> Source code in <code>services/tokenizer/basic/patterns.py</code> <pre><code>def list_patterns(self) -&gt; List[str]:\n    \"\"\"Get list of available pattern names.\"\"\"\n    return list(self._patterns.keys())\n</code></pre>"},{"location":"reference/tokenizer/#services.tokenizer.basic.patterns.get_patterns","title":"<code>get_patterns()</code>","text":"<p>Get global TokenizerPatterns instance.</p> <p>Returns:</p> Type Description <code>TokenizerPatterns</code> <p>Singleton TokenizerPatterns instance</p> Source code in <code>services/tokenizer/basic/patterns.py</code> <pre><code>def get_patterns() -&gt; TokenizerPatterns:\n    \"\"\"\n    Get global TokenizerPatterns instance.\n\n    Returns:\n        Singleton TokenizerPatterns instance\n    \"\"\"\n    global _global_patterns\n    if _global_patterns is None:\n        _global_patterns = TokenizerPatterns()\n    return _global_patterns\n</code></pre>"},{"location":"reference/tokenizer/#services.tokenizer.basic.tokenize_text","title":"<code>tokenize_text(text, config=None)</code>","text":"<p>Simple convenience function for basic text tokenization.</p> Source code in <code>services/tokenizer/basic/__init__.py</code> <pre><code>def tokenize_text(text: str, config: TokenizerConfig | None = None) -&gt; list[str]:\n    \"\"\"Simple convenience function for basic text tokenization.\"\"\"\n    tokenizer = create_basic_tokenizer(config)\n    return tokenizer.tokenize(text)\n</code></pre>"},{"location":"reference/tokenizer/#services.tokenizer.basic.tokenizer","title":"<code>tokenizer</code>","text":"<p>BasicTokenizer implementation.</p> <p>This module contains the main BasicTokenizer class that implements Unicode-aware tokenization for social media text with entity preservation.</p> <p>Classes:</p> Name Description <code>BasicTokenizer</code> <p>Unicode-aware basic tokenizer for social media text.</p>"},{"location":"reference/tokenizer/#services.tokenizer.basic.tokenizer.BasicTokenizer","title":"<code>BasicTokenizer</code>","text":"<p>               Bases: <code>AbstractTokenizer</code></p> <p>Unicode-aware basic tokenizer for social media text.</p> <p>This tokenizer handles mixed-script content, preserves social media entities (@mentions, #hashtags, URLs), and applies appropriate tokenization strategies for different script families.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize BasicTokenizer with configuration.</p> <code>tokenize</code> <p>Tokenize input text into a list of tokens.</p> Source code in <code>services/tokenizer/basic/tokenizer.py</code> <pre><code>class BasicTokenizer(AbstractTokenizer):\n    \"\"\"\n    Unicode-aware basic tokenizer for social media text.\n\n    This tokenizer handles mixed-script content, preserves social media entities\n    (@mentions, #hashtags, URLs), and applies appropriate tokenization strategies\n    for different script families.\n    \"\"\"\n\n    def __init__(self, config: Optional[TokenizerConfig] = None):\n        \"\"\"\n        Initialize BasicTokenizer with configuration.\n\n        Args:\n            config: Tokenizer configuration. If None, default config will be used.\n        \"\"\"\n        super().__init__(config)\n        self._patterns = get_patterns()\n\n        # Compile regex pattern for character-level script detection (performance optimization)\n        self._CHAR_LEVEL_PATTERN = re.compile(\n            r\"[\\u4e00-\\u9fff\"  # CJK Unified Ideographs\n            r\"\\u3400-\\u4dbf\"  # CJK Extension A\n            r\"\\u3040-\\u309f\"  # Hiragana\n            r\"\\u30a0-\\u30ff\"  # Katakana\n            r\"\\u0e00-\\u0e7f\"  # Thai\n            r\"\\u0e80-\\u0eff\"  # Lao\n            r\"\\u1000-\\u109f\"  # Myanmar\n            r\"\\u1780-\\u17ff]\"  # Khmer\n        )\n\n    def tokenize(self, text: str) -&gt; TokenList:\n        \"\"\"\n        Tokenize input text into a list of tokens.\n\n        Applies appropriate tokenization strategies for mixed-script content\n        while preserving social media entities and handling Unicode correctly.\n\n        Args:\n            text: Input text to tokenize\n\n        Returns:\n            List of tokens extracted from the input text in document order\n        \"\"\"\n        if not text:\n            return []\n\n        # Apply preprocessing\n        processed_text = self._preprocess_text(text)\n        if not processed_text:\n            return []\n\n        # Extract tokens using comprehensive regex pattern\n        tokens = self._extract_tokens(processed_text)\n\n        # Apply post-processing\n        return self._postprocess_tokens(tokens)\n\n    def _extract_tokens(self, text: str) -&gt; TokenList:\n        \"\"\"\n        Extract tokens using comprehensive regex patterns.\n        Preserves the original order of tokens as they appear in the input text.\n\n        Args:\n            text: Preprocessed text to tokenize\n\n        Returns:\n            List of extracted tokens in their original order\n        \"\"\"\n        return self._extract_tokens_ordered(text, LanguageFamily.MIXED)\n\n    def _is_char_level_script(self, char: str) -&gt; bool:\n        \"\"\"Check if character belongs to a character-level script.\"\"\"\n        return bool(self._CHAR_LEVEL_PATTERN.match(char))\n\n    def _get_char_script(self, char: str) -&gt; str:\n        \"\"\"\n        Get the script family for a character.\n\n        Args:\n            char: Character to analyze\n\n        Returns:\n            Script family name\n        \"\"\"\n        code_point = ord(char)\n\n        # Latin script\n        if (\n            (0x0041 &lt;= code_point &lt;= 0x007A)\n            or (0x00C0 &lt;= code_point &lt;= 0x024F)\n            or (0x1E00 &lt;= code_point &lt;= 0x1EFF)\n        ):\n            return \"latin\"\n\n        # Korean Hangul (space-separated, NOT character-level!)\n        elif 0xAC00 &lt;= code_point &lt;= 0xD7AF:\n            return \"korean\"\n\n        # Character-level scripts (CJK, Thai, etc.)\n        elif self._is_char_level_script(char):\n            return \"cjk\"\n\n        # Arabic script\n        elif (\n            (0x0600 &lt;= code_point &lt;= 0x06FF)\n            or (0x0750 &lt;= code_point &lt;= 0x077F)\n            or (0x08A0 &lt;= code_point &lt;= 0x08FF)\n        ):\n            return \"arabic\"\n\n        else:\n            return \"other\"\n\n    def _extract_tokens_ordered(\n        self, text: str, language_family: LanguageFamily\n    ) -&gt; TokenList:\n        \"\"\"\n        Extract tokens preserving their original order in the text.\n\n        Uses a single comprehensive regex pattern to find ALL tokens in document order,\n        eliminating the need for complex segmentation and reassembly logic.\n        This is the Phase 2 optimization that removes O(n\u00d7segments) complexity.\n\n        Args:\n            text: Preprocessed text to tokenize\n            language_family: Detected language family for the full text\n\n        Returns:\n            List of extracted tokens in their original order\n        \"\"\"\n        if not text.strip():\n            return []\n\n        # Remove excluded entities (URLs/emails) from text if they are disabled\n        # This prevents them from being tokenized into component words\n        exclusion_pattern = self._patterns.get_exclusion_pattern(self._config)\n        if exclusion_pattern:\n            # Replace excluded entities with spaces to maintain word boundaries\n            text = exclusion_pattern.sub(\" \", text)\n            # Clean up multiple spaces\n            text = \" \".join(text.split())\n\n        if not text.strip():\n            return []\n\n        # Get comprehensive pattern based on configuration\n        # This single pattern finds ALL tokens in document order\n        comprehensive_pattern = self._patterns.get_comprehensive_pattern(self._config)\n\n        # Single regex call gets all tokens in order - this is the key optimization!\n        raw_tokens = comprehensive_pattern.findall(text)\n\n        # If no tokens were found but input has content, use fallback for edge cases\n        if not raw_tokens and text.strip():\n            # For pure punctuation or unrecognized content, return as single token\n            # This maintains compatibility with old tokenizer behavior for edge cases\n            return [text.strip()]\n\n        # Apply postprocessing for language-specific behavior and configuration filtering\n        tokens = []\n        for token in raw_tokens:\n            if not token.strip():\n                continue\n\n            # Clean URLs by removing trailing punctuation\n            if self._is_url_like(token):\n                token = self._clean_url_token(token)\n\n            # For character-level scripts, break down multi-character tokens into individual characters\n            # This maintains compatibility with existing test expectations\n            if (\n                language_family == LanguageFamily.CJK\n                and self._contains_char_level_chars(token)\n            ):\n                # Only break down pure character-level tokens, not mixed tokens\n                if self._is_pure_char_level_token(token):\n                    tokens.extend(list(token))\n                else:\n                    # Mixed token - keep as is but process character-level parts\n                    tokens.append(token)\n            elif language_family == LanguageFamily.MIXED:\n                # For mixed script, break down character-level script parts but keep Latin parts whole\n                processed_tokens = self._process_mixed_script_token(token)\n                tokens.extend(processed_tokens)\n            else:\n                tokens.append(token)\n\n        return [token for token in tokens if token.strip()]\n\n    def _is_punctuation_only(self, token: str) -&gt; bool:\n        \"\"\"Check if token contains only punctuation.\"\"\"\n        punctuation_chars = \".!?;:,()[]{}\\\"'-~`@#$%^&amp;*+=&lt;&gt;/|\\\\\"\n        return all(c in punctuation_chars for c in token)\n\n    def _is_numeric_only(self, token: str) -&gt; bool:\n        \"\"\"Check if token is purely numeric.\"\"\"\n        return (\n            token.replace(\".\", \"\")\n            .replace(\",\", \"\")\n            .replace(\"%\", \"\")\n            .replace(\"$\", \"\")\n            .isdigit()\n        )\n\n    def _is_url_like(self, token: str) -&gt; bool:\n        \"\"\"Check if token looks like a URL.\"\"\"\n        # Don't classify emails as URLs\n        if self._is_email_like(token):\n            return False\n\n        # Explicit URL indicators (http://, https://, www., or protocol markers)\n        if token.startswith((\"http://\", \"https://\", \"www.\")) or \"://\" in token:\n            return True\n\n        # Domain-like patterns (e.g., \"example.com\")\n        # But NOT abbreviations (e.g., \"U.S.\", \"c.e.o.s\")\n        # Heuristic: URLs have at least one period NOT followed by a single uppercase/lowercase letter\n        # This allows \"example.com\" but excludes \"U.S.\" and \"c.e.o.s\"\n        if (\n            token.count(\".\") &gt;= 1\n            and any(c.isalpha() for c in token)\n            and \"@\" not in token\n        ):\n            # Check if this looks like an abbreviation (single letters between periods)\n            # Pattern: letter(s).letter(s).letter(s) where segments are 1-3 chars\n            abbreviation_pattern = r\"^[a-z]{1,3}(?:\\.[a-z]{1,3})+\\.?$\"\n\n            if re.match(abbreviation_pattern, token, re.IGNORECASE):\n                return False  # This is an abbreviation, not a URL\n            # If it has a period and looks like a domain, it's URL-like\n            return True\n\n        return False\n\n    def _is_email_like(self, token: str) -&gt; bool:\n        \"\"\"Check if token looks like an email address.\"\"\"\n        return \"@\" in token and \".\" in token and not token.startswith(\"@\")\n\n    def _clean_url_token(self, url_token: str) -&gt; str:\n        \"\"\"Remove trailing punctuation from URL tokens.\"\"\"\n        trailing_punctuation = \".!?;:,)]}\\\"'\"\n        return url_token.rstrip(trailing_punctuation)\n\n    def _contains_char_level_chars(self, token: str) -&gt; bool:\n        \"\"\"Check if token contains any character-level script characters.\"\"\"\n        return any(self._is_char_level_script(char) for char in token)\n\n    def _is_pure_char_level_token(self, token: str) -&gt; bool:\n        \"\"\"Check if token contains only character-level script characters.\"\"\"\n        return all(self._is_char_level_script(char) or char.isspace() for char in token)\n\n    def _process_mixed_script_token(self, token: str) -&gt; TokenList:\n        \"\"\"Process mixed script tokens by breaking down character-level script parts.\"\"\"\n        if not self._contains_char_level_chars(token):\n            return [token]\n\n        # Check if token mixes Latin with CJK\n        has_latin = any(c.isascii() and c.isalpha() for c in token)\n        has_cjk = any(self._is_char_level_script(c) for c in token)\n\n        # Don't apply mixed-script preservation to social media entities\n        is_social_entity = token.startswith((\"@\", \"#\", \"$\"))\n\n        if has_latin and has_cjk and not is_social_entity:\n            # Mixed script - keep intact (brand names, bot tricks)\n            return [token]\n\n        # Rest of existing logic for pure CJK tokens...\n        result = []\n        current_token = \"\"\n        current_is_cjk = None\n\n        for char in token:\n            char_is_cjk = self._is_char_level_script(char)\n\n            if current_is_cjk is None:\n                current_is_cjk = char_is_cjk\n                current_token = char\n            elif char_is_cjk == current_is_cjk:\n                current_token += char\n            else:\n                # Script change\n                if current_token.strip():\n                    if current_is_cjk and len(current_token) &gt; 1:\n                        # Break CJK into individual characters\n                        result.extend(list(current_token))\n                    else:\n                        result.append(current_token)\n                current_token = char\n                current_is_cjk = char_is_cjk\n\n        # Handle final token\n        if current_token.strip():\n            if current_is_cjk and len(current_token) &gt; 1:\n                result.extend(list(current_token))\n            else:\n                result.append(current_token)\n\n        return result\n\n    def _postprocess_tokens(self, tokens: TokenList) -&gt; TokenList:\n        \"\"\"\n        Apply post-processing to extracted tokens.\n\n        Args:\n            tokens: List of raw tokens\n\n        Returns:\n            Processed token list\n        \"\"\"\n        if not tokens:\n            return tokens\n\n        # Apply base class post-processing (length filtering, whitespace stripping, etc.)\n        return super()._postprocess_tokens(tokens)\n</code></pre>"},{"location":"reference/tokenizer/#services.tokenizer.basic.tokenizer.BasicTokenizer.__init__","title":"<code>__init__(config=None)</code>","text":"<p>Initialize BasicTokenizer with configuration.</p> <p>Parameters:</p> Name Type Description Default <code>Optional[TokenizerConfig]</code> <p>Tokenizer configuration. If None, default config will be used.</p> <code>None</code> Source code in <code>services/tokenizer/basic/tokenizer.py</code> <pre><code>def __init__(self, config: Optional[TokenizerConfig] = None):\n    \"\"\"\n    Initialize BasicTokenizer with configuration.\n\n    Args:\n        config: Tokenizer configuration. If None, default config will be used.\n    \"\"\"\n    super().__init__(config)\n    self._patterns = get_patterns()\n\n    # Compile regex pattern for character-level script detection (performance optimization)\n    self._CHAR_LEVEL_PATTERN = re.compile(\n        r\"[\\u4e00-\\u9fff\"  # CJK Unified Ideographs\n        r\"\\u3400-\\u4dbf\"  # CJK Extension A\n        r\"\\u3040-\\u309f\"  # Hiragana\n        r\"\\u30a0-\\u30ff\"  # Katakana\n        r\"\\u0e00-\\u0e7f\"  # Thai\n        r\"\\u0e80-\\u0eff\"  # Lao\n        r\"\\u1000-\\u109f\"  # Myanmar\n        r\"\\u1780-\\u17ff]\"  # Khmer\n    )\n</code></pre>"},{"location":"reference/tokenizer/#services.tokenizer.basic.tokenizer.BasicTokenizer.__init__(config)","title":"<code>config</code>","text":""},{"location":"reference/tokenizer/#services.tokenizer.basic.tokenizer.BasicTokenizer.tokenize","title":"<code>tokenize(text)</code>","text":"<p>Tokenize input text into a list of tokens.</p> <p>Applies appropriate tokenization strategies for mixed-script content while preserving social media entities and handling Unicode correctly.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>Input text to tokenize</p> required <p>Returns:</p> Type Description <code>TokenList</code> <p>List of tokens extracted from the input text in document order</p> Source code in <code>services/tokenizer/basic/tokenizer.py</code> <pre><code>def tokenize(self, text: str) -&gt; TokenList:\n    \"\"\"\n    Tokenize input text into a list of tokens.\n\n    Applies appropriate tokenization strategies for mixed-script content\n    while preserving social media entities and handling Unicode correctly.\n\n    Args:\n        text: Input text to tokenize\n\n    Returns:\n        List of tokens extracted from the input text in document order\n    \"\"\"\n    if not text:\n        return []\n\n    # Apply preprocessing\n    processed_text = self._preprocess_text(text)\n    if not processed_text:\n        return []\n\n    # Extract tokens using comprehensive regex pattern\n    tokens = self._extract_tokens(processed_text)\n\n    # Apply post-processing\n    return self._postprocess_tokens(tokens)\n</code></pre>"},{"location":"reference/tokenizer/#services.tokenizer.basic.tokenizer.BasicTokenizer.tokenize(text)","title":"<code>text</code>","text":""}]}